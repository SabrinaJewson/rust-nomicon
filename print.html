<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Rustonomicon</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="The Dark Arts of Advanced and Unsafe Rust Programming">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/nomicon.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">Introduction</a></li><li class="chapter-item expanded "><a href="meet-safe-and-unsafe.html"><strong aria-hidden="true">1.</strong> Meet Safe and Unsafe</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="safe-unsafe-meaning.html"><strong aria-hidden="true">1.1.</strong> How Safe and Unsafe Interact</a></li><li class="chapter-item expanded "><a href="what-unsafe-does.html"><strong aria-hidden="true">1.2.</strong> What Unsafe Can Do</a></li><li class="chapter-item expanded "><a href="working-with-unsafe.html"><strong aria-hidden="true">1.3.</strong> Working with Unsafe</a></li></ol></li><li class="chapter-item expanded "><a href="data.html"><strong aria-hidden="true">2.</strong> Data Layout</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="repr-rust.html"><strong aria-hidden="true">2.1.</strong> repr(Rust)</a></li><li class="chapter-item expanded "><a href="exotic-sizes.html"><strong aria-hidden="true">2.2.</strong> Exotically Sized Types</a></li><li class="chapter-item expanded "><a href="other-reprs.html"><strong aria-hidden="true">2.3.</strong> Other reprs</a></li></ol></li><li class="chapter-item expanded "><a href="ownership.html"><strong aria-hidden="true">3.</strong> Ownership</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="references.html"><strong aria-hidden="true">3.1.</strong> References</a></li><li class="chapter-item expanded "><a href="aliasing.html"><strong aria-hidden="true">3.2.</strong> Aliasing</a></li><li class="chapter-item expanded "><a href="lifetimes.html"><strong aria-hidden="true">3.3.</strong> Lifetimes</a></li><li class="chapter-item expanded "><a href="lifetime-mismatch.html"><strong aria-hidden="true">3.4.</strong> Limits of Lifetimes</a></li><li class="chapter-item expanded "><a href="lifetime-elision.html"><strong aria-hidden="true">3.5.</strong> Lifetime Elision</a></li><li class="chapter-item expanded "><a href="unbounded-lifetimes.html"><strong aria-hidden="true">3.6.</strong> Unbounded Lifetimes</a></li><li class="chapter-item expanded "><a href="hrtb.html"><strong aria-hidden="true">3.7.</strong> Higher-Rank Trait Bounds</a></li><li class="chapter-item expanded "><a href="subtyping.html"><strong aria-hidden="true">3.8.</strong> Subtyping and Variance</a></li><li class="chapter-item expanded "><a href="dropck.html"><strong aria-hidden="true">3.9.</strong> Drop Check</a></li><li class="chapter-item expanded "><a href="phantom-data.html"><strong aria-hidden="true">3.10.</strong> PhantomData</a></li><li class="chapter-item expanded "><a href="borrow-splitting.html"><strong aria-hidden="true">3.11.</strong> Splitting Borrows</a></li></ol></li><li class="chapter-item expanded "><a href="conversions.html"><strong aria-hidden="true">4.</strong> Type Conversions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="coercions.html"><strong aria-hidden="true">4.1.</strong> Coercions</a></li><li class="chapter-item expanded "><a href="dot-operator.html"><strong aria-hidden="true">4.2.</strong> The Dot Operator</a></li><li class="chapter-item expanded "><a href="casts.html"><strong aria-hidden="true">4.3.</strong> Casts</a></li><li class="chapter-item expanded "><a href="transmutes.html"><strong aria-hidden="true">4.4.</strong> Transmutes</a></li></ol></li><li class="chapter-item expanded "><a href="uninitialized.html"><strong aria-hidden="true">5.</strong> Uninitialized Memory</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="checked-uninit.html"><strong aria-hidden="true">5.1.</strong> Checked</a></li><li class="chapter-item expanded "><a href="drop-flags.html"><strong aria-hidden="true">5.2.</strong> Drop Flags</a></li><li class="chapter-item expanded "><a href="unchecked-uninit.html"><strong aria-hidden="true">5.3.</strong> Unchecked</a></li></ol></li><li class="chapter-item expanded "><a href="obrm.html"><strong aria-hidden="true">6.</strong> Ownership Based Resource Management</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="constructors.html"><strong aria-hidden="true">6.1.</strong> Constructors</a></li><li class="chapter-item expanded "><a href="destructors.html"><strong aria-hidden="true">6.2.</strong> Destructors</a></li><li class="chapter-item expanded "><a href="leaking.html"><strong aria-hidden="true">6.3.</strong> Leaking</a></li></ol></li><li class="chapter-item expanded "><a href="unwinding.html"><strong aria-hidden="true">7.</strong> Unwinding</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="exception-safety.html"><strong aria-hidden="true">7.1.</strong> Exception Safety</a></li><li class="chapter-item expanded "><a href="poisoning.html"><strong aria-hidden="true">7.2.</strong> Poisoning</a></li></ol></li><li class="chapter-item expanded "><a href="concurrency.html"><strong aria-hidden="true">8.</strong> Concurrency</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="races.html"><strong aria-hidden="true">8.1.</strong> Races</a></li><li class="chapter-item expanded "><a href="send-and-sync.html"><strong aria-hidden="true">8.2.</strong> Send and Sync</a></li><li class="chapter-item expanded "><a href="atomics/atomics.html"><strong aria-hidden="true">8.3.</strong> Atomics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="atomics/multithread.html"><strong aria-hidden="true">8.3.1.</strong> Multithreaded Execution</a></li><li class="chapter-item expanded "><a href="atomics/relaxed.html"><strong aria-hidden="true">8.3.2.</strong> Relaxed</a></li><li class="chapter-item expanded "><a href="atomics/acquire-release.html"><strong aria-hidden="true">8.3.3.</strong> Acquire and Release</a></li><li class="chapter-item expanded "><a href="atomics/seqcst.html"><strong aria-hidden="true">8.3.4.</strong> SeqCst</a></li><li class="chapter-item expanded "><a href="atomics/fences.html"><strong aria-hidden="true">8.3.5.</strong> Fences</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="vec/vec.html"><strong aria-hidden="true">9.</strong> Implementing Vec</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="vec/vec-layout.html"><strong aria-hidden="true">9.1.</strong> Layout</a></li><li class="chapter-item expanded "><a href="vec/vec-alloc.html"><strong aria-hidden="true">9.2.</strong> Allocating</a></li><li class="chapter-item expanded "><a href="vec/vec-push-pop.html"><strong aria-hidden="true">9.3.</strong> Push and Pop</a></li><li class="chapter-item expanded "><a href="vec/vec-dealloc.html"><strong aria-hidden="true">9.4.</strong> Deallocating</a></li><li class="chapter-item expanded "><a href="vec/vec-deref.html"><strong aria-hidden="true">9.5.</strong> Deref</a></li><li class="chapter-item expanded "><a href="vec/vec-insert-remove.html"><strong aria-hidden="true">9.6.</strong> Insert and Remove</a></li><li class="chapter-item expanded "><a href="vec/vec-into-iter.html"><strong aria-hidden="true">9.7.</strong> IntoIter</a></li><li class="chapter-item expanded "><a href="vec/vec-raw.html"><strong aria-hidden="true">9.8.</strong> RawVec</a></li><li class="chapter-item expanded "><a href="vec/vec-drain.html"><strong aria-hidden="true">9.9.</strong> Drain</a></li><li class="chapter-item expanded "><a href="vec/vec-zsts.html"><strong aria-hidden="true">9.10.</strong> Handling Zero-Sized Types</a></li><li class="chapter-item expanded "><a href="vec/vec-final.html"><strong aria-hidden="true">9.11.</strong> Final Code</a></li></ol></li><li class="chapter-item expanded "><a href="arc-mutex/arc-and-mutex.html"><strong aria-hidden="true">10.</strong> Implementing Arc and Mutex</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="arc-mutex/arc.html"><strong aria-hidden="true">10.1.</strong> Arc</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="arc-mutex/arc-layout.html"><strong aria-hidden="true">10.1.1.</strong> Layout</a></li><li class="chapter-item expanded "><a href="arc-mutex/arc-base.html"><strong aria-hidden="true">10.1.2.</strong> Base Code</a></li><li class="chapter-item expanded "><a href="arc-mutex/arc-clone.html"><strong aria-hidden="true">10.1.3.</strong> Cloning</a></li><li class="chapter-item expanded "><a href="arc-mutex/arc-drop.html"><strong aria-hidden="true">10.1.4.</strong> Dropping</a></li><li class="chapter-item expanded "><a href="arc-mutex/arc-final.html"><strong aria-hidden="true">10.1.5.</strong> Final Code</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="ffi.html"><strong aria-hidden="true">11.</strong> FFI</a></li><li class="chapter-item expanded "><a href="beneath-std.html"><strong aria-hidden="true">12.</strong> Beneath std</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panic-handler.html"><strong aria-hidden="true">12.1.</strong> #[panic_handler]</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Rustonomicon</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/rust-lang/nomicon" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="the-rustonomicon"><a class="header" href="#the-rustonomicon">The Rustonomicon</a></h1>
<div class="warning">
<p>Warning:
This book is incomplete.
Documenting everything and rewriting outdated parts take a while.
See the <a href="https://github.com/rust-lang/nomicon/issues">issue tracker</a> to check what's missing/outdated, and if there are any mistakes or ideas that haven't been reported, feel free to open a new issue there.</p>
</div>
<h2 id="the-dark-arts-of-unsafe-rust"><a class="header" href="#the-dark-arts-of-unsafe-rust">The Dark Arts of Unsafe Rust</a></h2>
<blockquote>
<p>THE KNOWLEDGE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF UNLEASHING INDESCRIBABLE HORRORS THAT SHATTER YOUR PSYCHE AND SET YOUR MIND ADRIFT IN THE UNKNOWABLY INFINITE COSMOS.</p>
</blockquote>
<p>The Rustonomicon digs into all the awful details that you need to understand when writing Unsafe Rust programs.</p>
<p>Should you wish a long and happy career of writing Rust programs, you should turn back now and forget you ever saw this book.
It is not necessary.
However if you intend to write unsafe code — or just want to dig into the guts of the language — this book contains lots of useful information.</p>
<p>Unlike <em><a href="../book/index.html">The Rust Programming Language</a></em>, we will be assuming considerable prior knowledge.
In particular, you should be comfortable with basic systems programming and Rust.
If you don't feel comfortable with these topics, you should consider reading <a href="../book/index.html">The Book</a> first.
That said, we won't assume you have read it, and we will take care to occasionally give a refresher on the basics where appropriate.
You can skip straight to this book if you want; just know that we won't be explaining everything from the ground up.</p>
<p>This book exists primarily as a high-level companion to <a href="../reference/index.html">The Reference</a>.
Where The Reference exists to detail the syntax and semantics of every part of the language, The Rustonomicon exists to describe how to use those pieces together, and the issues that you will have in doing so.</p>
<p>The Reference will tell you the syntax and semantics of references, destructors, and unwinding, but it won't tell you how combining them can lead to exception-safety issues, or how to deal with those issues.</p>
<p>It should be noted that we haven't synced The Rustnomicon and The Reference well, so they may have duplicate content.
In general, if the two documents disagree, The Reference should be assumed to be correct (it isn't yet considered normative, it's just better maintained).</p>
<p>Topics that are within the scope of this book include: the meaning of (un)safety, unsafe primitives provided by the language and standard library, techniques for creating safe abstractions with those unsafe primitives, subtyping and variance, exception-safety (panic/unwind-safety), working with uninitialized memory, type punning, concurrency, interoperating with other languages (FFI), optimization tricks, how constructs lower to compiler/OS/hardware primitives, how to <strong>not</strong> make the memory model people angry, how you're <strong>going</strong> to make the memory model people angry, and more.</p>
<p>The Rustonomicon is not a place to exhaustively describe the semantics and guarantees of every single API in the standard library, nor is it a place to exhaustively describe every feature of Rust.</p>
<p>Unless otherwise noted, Rust code in this book uses the Rust 2021 edition.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="meet-safe-and-unsafe"><a class="header" href="#meet-safe-and-unsafe">Meet Safe and Unsafe</a></h1>
<p><img src="img/safeandunsafe.svg" alt="safe and unsafe" /></p>
<p>It would be great to not have to worry about low-level implementation details.
Who could possibly care how much space the empty tuple occupies? Sadly, it
sometimes matters and we need to worry about it. The most common reason
developers start to care about implementation details is performance, but more
importantly, these details can become a matter of correctness when interfacing
directly with hardware, operating systems, or other languages.</p>
<p>When implementation details start to matter in a safe programming language,
programmers usually have three options:</p>
<ul>
<li>fiddle with the code to encourage the compiler/runtime to perform an optimization</li>
<li>adopt a more unidiomatic or cumbersome design to get the desired implementation</li>
<li>rewrite the implementation in a language that lets you deal with those details</li>
</ul>
<p>For that last option, the language programmers tend to use is <em>C</em>. This is often
necessary to interface with systems that only declare a C interface.</p>
<p>Unfortunately, C is incredibly unsafe to use (sometimes for good reason),
and this unsafety is magnified when trying to interoperate with another
language. Care must be taken to ensure C and the other language agree on
what's happening, and that they don't step on each other's toes.</p>
<p>So what does this have to do with Rust?</p>
<p>Well, unlike C, Rust is a safe programming language.</p>
<p>But, like C, Rust is an unsafe programming language.</p>
<p>More accurately, Rust <em>contains</em> both a safe and unsafe programming language.</p>
<p>Rust can be thought of as a combination of two programming languages: <em>Safe
Rust</em> and <em>Unsafe Rust</em>. Conveniently, these names mean exactly what they say:
Safe Rust is Safe. Unsafe Rust is, well, not. In fact, Unsafe Rust lets us
do some <em>really</em> unsafe things. Things the Rust authors will implore you not to
do, but we'll do anyway.</p>
<p>Safe Rust is the <em>true</em> Rust programming language. If all you do is write Safe
Rust, you will never have to worry about type-safety or memory-safety. You will
never endure a dangling pointer, a use-after-free, or any other kind of
Undefined Behavior (a.k.a. UB).</p>
<p>The standard library also gives you enough utilities out of the box that you'll
be able to write high-performance applications and libraries in pure idiomatic
Safe Rust.</p>
<p>But maybe you want to talk to another language. Maybe you're writing a
low-level abstraction not exposed by the standard library. Maybe you're
<em>writing</em> the standard library (which is written entirely in Rust). Maybe you
need to do something the type-system doesn't understand and just <em>frob some dang
bits</em>. Maybe you need Unsafe Rust.</p>
<p>Unsafe Rust is exactly like Safe Rust with all the same rules and semantics.
It just lets you do some <em>extra</em> things that are Definitely Not Safe
(which we will define in the next section).</p>
<p>The value of this separation is that we gain the benefits of using an unsafe
language like C — low level control over implementation details — without most
of the problems that come with trying to integrate it with a completely
different safe language.</p>
<p>There are still some problems — most notably, we must become aware of properties
that the type system assumes and audit them in any code that interacts with
Unsafe Rust. That's the purpose of this book: to teach you about these assumptions
and how to manage them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-safe-and-unsafe-interact"><a class="header" href="#how-safe-and-unsafe-interact">How Safe and Unsafe Interact</a></h1>
<p>What's the relationship between Safe Rust and Unsafe Rust? How do they
interact?</p>
<p>The separation between Safe Rust and Unsafe Rust is controlled with the
<code>unsafe</code> keyword, which acts as an interface from one to the other. This is
why we can say Safe Rust is a safe language: all the unsafe parts are kept
exclusively behind the <code>unsafe</code> boundary. If you wish, you can even toss
<code>#![forbid(unsafe_code)]</code> into your code base to statically guarantee that
you're only writing Safe Rust.</p>
<p>The <code>unsafe</code> keyword has two uses: to declare the existence of contracts the
compiler can't check, and to declare that a programmer has checked that these
contracts have been upheld.</p>
<p>You can use <code>unsafe</code> to indicate the existence of unchecked contracts on
<em>functions</em> and <em>trait declarations</em>. On functions, <code>unsafe</code> means that
users of the function must check that function's documentation to ensure
they are using it in a way that maintains the contracts the function
requires. On trait declarations, <code>unsafe</code> means that implementors of the
trait must check the trait documentation to ensure their implementation
maintains the contracts the trait requires.</p>
<p>You can use <code>unsafe</code> on a block to declare that all unsafe actions performed
within are verified to uphold the contracts of those operations. For instance,
the index passed to <a href="../std/primitive.slice.html#method.get_unchecked"><code>slice::get_unchecked</code></a> is in-bounds.</p>
<p>You can use <code>unsafe</code> on a trait implementation to declare that the implementation
upholds the trait's contract. For instance, that a type implementing <a href="../std/marker/trait.Send.html"><code>Send</code></a> is
really safe to move to another thread.</p>
<p>The standard library has a number of unsafe functions, including:</p>
<ul>
<li><a href="../std/primitive.slice.html#method.get_unchecked"><code>slice::get_unchecked</code></a>, which performs unchecked indexing,
allowing memory safety to be freely violated.</li>
<li><a href="../std/mem/fn.transmute.html"><code>mem::transmute</code></a> reinterprets some value as having a given type,
bypassing type safety in arbitrary ways (see <a href="conversions.html">conversions</a> for details).</li>
<li>Every raw pointer to a sized type has an <a href="../std/primitive.pointer.html#method.offset"><code>offset</code></a> method that
invokes Undefined Behavior if the passed offset is not <a href="../std/primitive.pointer.html#method.offset">"in bounds"</a>.</li>
<li>All FFI (Foreign Function Interface) functions are <code>unsafe</code> to call because the
other language can do arbitrary operations that the Rust compiler can't check.</li>
</ul>
<p>As of Rust 1.29.2 the standard library defines the following unsafe traits
(there are others, but they are not stabilized yet and some of them may never
be):</p>
<ul>
<li><a href="../std/marker/trait.Send.html"><code>Send</code></a> is a marker trait (a trait with no API) that promises implementors
are safe to send (move) to another thread.</li>
<li><a href="../std/marker/trait.Sync.html"><code>Sync</code></a> is a marker trait that promises threads can safely share implementors
through a shared reference.</li>
<li><a href="../std/alloc/trait.GlobalAlloc.html"><code>GlobalAlloc</code></a> allows customizing the memory allocator of the whole program.</li>
</ul>
<p>Much of the Rust standard library also uses Unsafe Rust internally. These
implementations have generally been rigorously manually checked, so the Safe Rust
interfaces built on top of these implementations can be assumed to be safe.</p>
<p>The need for all of this separation boils down a single fundamental property
of Safe Rust, the <em>soundness property</em>:</p>
<p><strong>No matter what, Safe Rust can't cause Undefined Behavior.</strong></p>
<p>The design of the safe/unsafe split means that there is an asymmetric trust
relationship between Safe and Unsafe Rust. Safe Rust inherently has to
trust that any Unsafe Rust it touches has been written correctly.
On the other hand, Unsafe Rust cannot trust Safe Rust without care.</p>
<p>As an example, Rust has the <a href="../std/cmp/trait.PartialOrd.html"><code>PartialOrd</code></a> and <a href="../std/cmp/trait.Ord.html"><code>Ord</code></a> traits to differentiate
between types which can "just" be compared, and those that provide a "total"
ordering (which basically means that comparison behaves reasonably).</p>
<p><a href="../std/collections/struct.BTreeMap.html"><code>BTreeMap</code></a> doesn't really make sense for partially-ordered types, and so it
requires that its keys implement <code>Ord</code>. However, <code>BTreeMap</code> has Unsafe Rust code
inside of its implementation. Because it would be unacceptable for a sloppy <code>Ord</code>
implementation (which is Safe to write) to cause Undefined Behavior, the Unsafe
code in BTreeMap must be written to be robust against <code>Ord</code> implementations which
aren't actually total — even though that's the whole point of requiring <code>Ord</code>.</p>
<p>The Unsafe Rust code just can't trust the Safe Rust code to be written correctly.
That said, <code>BTreeMap</code> will still behave completely erratically if you feed in
values that don't have a total ordering. It just won't ever cause Undefined
Behavior.</p>
<p>One may wonder, if <code>BTreeMap</code> cannot trust <code>Ord</code> because it's Safe, why can it
trust <em>any</em> Safe code? For instance <code>BTreeMap</code> relies on integers and slices to
be implemented correctly. Those are safe too, right?</p>
<p>The difference is one of scope. When <code>BTreeMap</code> relies on integers and slices,
it's relying on one very specific implementation. This is a measured risk that
can be weighed against the benefit. In this case there's basically zero risk;
if integers and slices are broken, <em>everyone</em> is broken. Also, they're maintained
by the same people who maintain <code>BTreeMap</code>, so it's easy to keep tabs on them.</p>
<p>On the other hand, <code>BTreeMap</code>'s key type is generic. Trusting its <code>Ord</code> implementation
means trusting every <code>Ord</code> implementation in the past, present, and future.
Here the risk is high: someone somewhere is going to make a mistake and mess up
their <code>Ord</code> implementation, or even just straight up lie about providing a total
ordering because "it seems to work". When that happens, <code>BTreeMap</code> needs to be
prepared.</p>
<p>The same logic applies to trusting a closure that's passed to you to behave
correctly.</p>
<p>This problem of unbounded generic trust is the problem that <code>unsafe</code> traits
exist to resolve. The <code>BTreeMap</code> type could theoretically require that keys
implement a new trait called <code>UnsafeOrd</code>, rather than <code>Ord</code>, that might look
like this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::cmp::Ordering;

unsafe trait UnsafeOrd {
    fn cmp(&amp;self, other: &amp;Self) -&gt; Ordering;
}
<span class="boring">}</span></code></pre></pre>
<p>Then, a type would use <code>unsafe</code> to implement <code>UnsafeOrd</code>, indicating that
they've ensured their implementation maintains whatever contracts the
trait expects. In this situation, the Unsafe Rust in the internals of
<code>BTreeMap</code> would be justified in trusting that the key type's <code>UnsafeOrd</code>
implementation is correct. If it isn't, it's the fault of the unsafe trait
implementation, which is consistent with Rust's safety guarantees.</p>
<p>The decision of whether to mark a trait <code>unsafe</code> is an API design choice. A
safe trait is easier to implement, but any unsafe code that relies on it must
defend against incorrect behavior. Marking a trait <code>unsafe</code> shifts this
responsibility to the implementor. Rust has traditionally avoided marking
traits <code>unsafe</code> because it makes Unsafe Rust pervasive, which isn't desirable.</p>
<p><code>Send</code> and <code>Sync</code> are marked unsafe because thread safety is a <em>fundamental
property</em> that unsafe code can't possibly hope to defend against in the way it
could defend against a buggy <code>Ord</code> implementation. Similarly, <code>GlobalAllocator</code>
is keeping accounts of all the memory in the program and other things like
<code>Box</code> or <code>Vec</code> build on top of it. If it does something weird (giving the same
chunk of memory to another request when it is still in use), there's no chance
to detect that and do anything about it.</p>
<p>The decision of whether to mark your own traits <code>unsafe</code> depends on the same
sort of consideration. If <code>unsafe</code> code can't reasonably expect to defend
against a broken implementation of the trait, then marking the trait <code>unsafe</code> is
a reasonable choice.</p>
<p>As an aside, while <code>Send</code> and <code>Sync</code> are <code>unsafe</code> traits, they are <em>also</em>
automatically implemented for types when such derivations are provably safe
to do. <code>Send</code> is automatically derived for all types composed only of values
whose types also implement <code>Send</code>. <code>Sync</code> is automatically derived for all
types composed only of values whose types also implement <code>Sync</code>. This minimizes
the pervasive unsafety of making these two traits <code>unsafe</code>. And not many people
are going to <em>implement</em> memory allocators (or use them directly, for that
matter).</p>
<p>This is the balance between Safe and Unsafe Rust. The separation is designed to
make using Safe Rust as ergonomic as possible, but requires extra effort and
care when writing Unsafe Rust. The rest of this book is largely a discussion
of the sort of care that must be taken, and what contracts Unsafe Rust must uphold.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-unsafe-rust-can-do"><a class="header" href="#what-unsafe-rust-can-do">What Unsafe Rust Can Do</a></h1>
<p>The only things that are different in Unsafe Rust are that you can:</p>
<ul>
<li>Dereference raw pointers</li>
<li>Call <code>unsafe</code> functions (including C functions, compiler intrinsics, and the raw allocator)</li>
<li>Implement <code>unsafe</code> traits</li>
<li>Mutate statics</li>
<li>Access fields of <code>union</code>s</li>
</ul>
<p>That's it. The reason these operations are relegated to Unsafe is that misusing
any of these things will cause the ever dreaded Undefined Behavior. Invoking
Undefined Behavior gives the compiler full rights to do arbitrarily bad things
to your program. You definitely <em>should not</em> invoke Undefined Behavior.</p>
<p>Unlike C, Undefined Behavior is pretty limited in scope in Rust. All the core
language cares about is preventing the following things:</p>
<ul>
<li>Dereferencing (using the <code>*</code> operator on) dangling or unaligned pointers (see below)</li>
<li>Breaking the <a href="references.html">pointer aliasing rules</a></li>
<li>Calling a function with the wrong call ABI or unwinding from a function with the wrong unwind ABI.</li>
<li>Causing a <a href="races.html">data race</a></li>
<li>Executing code compiled with <a href="../reference/attributes/codegen.html#the-target_feature-attribute">target features</a> that the current thread of execution does
not support</li>
<li>Producing invalid values (either alone or as a field of a compound type such
as <code>enum</code>/<code>struct</code>/array/tuple):
<ul>
<li>a <code>bool</code> that isn't 0 or 1</li>
<li>an <code>enum</code> with an invalid discriminant</li>
<li>a null <code>fn</code> pointer</li>
<li>a <code>char</code> outside the ranges [0x0, 0xD7FF] and [0xE000, 0x10FFFF]</li>
<li>a <code>!</code> (all values are invalid for this type)</li>
<li>an integer (<code>i*</code>/<code>u*</code>), floating point value (<code>f*</code>), or raw pointer read from
<a href="uninitialized.html">uninitialized memory</a>, or uninitialized memory in a <code>str</code>.</li>
<li>a reference/<code>Box</code> that is dangling, unaligned, or points to an invalid value.</li>
<li>a wide reference, <code>Box</code>, or raw pointer that has invalid metadata:
<ul>
<li><code>dyn Trait</code> metadata is invalid if it is not a pointer to a vtable for
<code>Trait</code> that matches the actual dynamic trait the pointer or reference points to</li>
<li>slice metadata is invalid if the length is not a valid <code>usize</code>
(i.e., it must not be read from uninitialized memory)</li>
</ul>
</li>
<li>a type with custom invalid values that is one of those values, such as a
<a href="../std/ptr/struct.NonNull.html"><code>NonNull</code></a> that is null. (Requesting custom invalid values is an unstable
feature, but some stable libstd types, like <code>NonNull</code>, make use of it.)</li>
</ul>
</li>
</ul>
<p>"Producing" a value happens any time a value is assigned, passed to a
function/primitive operation or returned from a function/primitive operation.</p>
<p>A reference/pointer is "dangling" if it is null or not all of the bytes it
points to are part of the same allocation (so in particular they all have to be
part of <em>some</em> allocation). The span of bytes it points to is determined by the
pointer value and the size of the pointee type. As a consequence, if the span is
empty, "dangling" is the same as "null". Note that slices and strings point
to their entire range, so it's important that the length metadata is never too
large (in particular, allocations and therefore slices and strings cannot be
bigger than <code>isize::MAX</code> bytes). If for some reason this is too cumbersome,
consider using raw pointers.</p>
<p>That's it. That's all the causes of Undefined Behavior baked into Rust. Of
course, unsafe functions and traits are free to declare arbitrary other
constraints that a program must maintain to avoid Undefined Behavior. For
instance, the allocator APIs declare that deallocating unallocated memory is
Undefined Behavior.</p>
<p>However, violations of these constraints generally will just transitively lead to one of
the above problems. Some additional constraints may also derive from compiler
intrinsics that make special assumptions about how code can be optimized. For instance,
Vec and Box make use of intrinsics that require their pointers to be non-null at all times.</p>
<p>Rust is otherwise quite permissive with respect to other dubious operations.
Rust considers it "safe" to:</p>
<ul>
<li>Deadlock</li>
<li>Have a <a href="races.html">race condition</a></li>
<li>Leak memory</li>
<li>Overflow integers (with the built-in operators such as <code>+</code> etc.)</li>
<li>Abort the program</li>
<li>Delete the production database</li>
</ul>
<p>However any program that actually manages to do such a thing is <em>probably</em>
incorrect. Rust provides lots of tools to make these things rare, but
these problems are considered impractical to categorically prevent.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="working-with-unsafe"><a class="header" href="#working-with-unsafe">Working with Unsafe</a></h1>
<p>Rust generally only gives us the tools to talk about Unsafe Rust in a scoped and
binary manner. Unfortunately, reality is significantly more complicated than
that. For instance, consider the following toy function:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn index(idx: usize, arr: &amp;[u8]) -&gt; Option&lt;u8&gt; {
    if idx &lt; arr.len() {
        unsafe {
            Some(*arr.get_unchecked(idx))
        }
    } else {
        None
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This function is safe and correct. We check that the index is in bounds, and if
it is, index into the array in an unchecked manner. We say that such a correct
unsafely implemented function is <em>sound</em>, meaning that safe code cannot cause
Undefined Behavior through it (which, remember, is the single fundamental
property of Safe Rust).</p>
<p>But even in such a trivial function, the scope of the unsafe block is
questionable. Consider changing the <code>&lt;</code> to a <code>&lt;=</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn index(idx: usize, arr: &amp;[u8]) -&gt; Option&lt;u8&gt; {
    if idx &lt;= arr.len() {
        unsafe {
            Some(*arr.get_unchecked(idx))
        }
    } else {
        None
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This program is now <em>unsound</em>, Safe Rust can cause Undefined Behavior, and yet
<em>we only modified safe code</em>. This is the fundamental problem of safety: it's
non-local. The soundness of our unsafe operations necessarily depends on the
state established by otherwise "safe" operations.</p>
<p>Safety is modular in the sense that opting into unsafety doesn't require you
to consider arbitrary other kinds of badness. For instance, doing an unchecked
index into a slice doesn't mean you suddenly need to worry about the slice being
null or containing uninitialized memory. Nothing fundamentally changes. However
safety <em>isn't</em> modular in the sense that programs are inherently stateful and
your unsafe operations may depend on arbitrary other state.</p>
<p>This non-locality gets much worse when we incorporate actual persistent state.
Consider a simple implementation of <code>Vec</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use std::ptr;

// Note: This definition is naive. See the chapter on implementing Vec.
pub struct Vec&lt;T&gt; {
    ptr: *mut T,
    len: usize,
    cap: usize,
}

// Note this implementation does not correctly handle zero-sized types.
// See the chapter on implementing Vec.
impl&lt;T&gt; Vec&lt;T&gt; {
    pub fn push(&amp;mut self, elem: T) {
        if self.len == self.cap {
            // not important for this example
            self.reallocate();
        }
        unsafe {
            ptr::write(self.ptr.add(self.len), elem);
            self.len += 1;
        }
    }
<span class="boring">    fn reallocate(&amp;mut self) { }
</span>}

<span class="boring">fn main() {}</span></code></pre></pre>
<p>This code is simple enough to reasonably audit and informally verify. Now consider
adding the following method:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn make_room(&amp;mut self) {
    // grow the capacity
    self.cap += 1;
}</code></pre>
<p>This code is 100% Safe Rust but it is also completely unsound. Changing the
capacity violates the invariants of Vec (that <code>cap</code> reflects the allocated space
in the Vec). This is not something the rest of Vec can guard against. It <em>has</em>
to trust the capacity field because there's no way to verify it.</p>
<p>Because it relies on invariants of a struct field, this <code>unsafe</code> code
does more than pollute a whole function: it pollutes a whole <em>module</em>.
Generally, the only bullet-proof way to limit the scope of unsafe code is at the
module boundary with privacy.</p>
<p>However this works <em>perfectly</em>. The existence of <code>make_room</code> is <em>not</em> a
problem for the soundness of Vec because we didn't mark it as public. Only the
module that defines this function can call it. Also, <code>make_room</code> directly
accesses the private fields of Vec, so it can only be written in the same module
as Vec.</p>
<p>It is therefore possible for us to write a completely safe abstraction that
relies on complex invariants. This is <em>critical</em> to the relationship between
Safe Rust and Unsafe Rust.</p>
<p>We have already seen that Unsafe code must trust <em>some</em> Safe code, but shouldn't
trust <em>generic</em> Safe code. Privacy is important to unsafe code for similar reasons:
it prevents us from having to trust all the safe code in the universe from messing
with our trusted state.</p>
<p>Safety lives!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-representation-in-rust"><a class="header" href="#data-representation-in-rust">Data Representation in Rust</a></h1>
<p>Low-level programming cares a lot about data layout. It's a big deal. It also
pervasively influences the rest of the language, so we're going to start by
digging into how data is represented in Rust.</p>
<p>This chapter is ideally in agreement with, and rendered redundant by,
the <a href="../reference/type-layout.html">Type Layout section of the Reference</a>. When this
book was first written, the reference was in complete disrepair, and the
Rustonomicon was attempting to serve as a partial replacement for the reference.
This is no longer the case, so this whole chapter can ideally be deleted.</p>
<p>We'll keep this chapter around for a bit longer, but ideally you should be
contributing any new facts or improvements to the Reference instead.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reprrust"><a class="header" href="#reprrust">repr(Rust)</a></h1>
<p>First and foremost, all types have an alignment specified in bytes. The
alignment of a type specifies what addresses are valid to store the value at. A
value with alignment <code>n</code> must only be stored at an address that is a multiple of
<code>n</code>. So alignment 2 means you must be stored at an even address, and 1 means
that you can be stored anywhere. Alignment is at least 1, and always a power
of 2.</p>
<p>Primitives are usually aligned to their size, although this is
platform-specific behavior. For example, on x86 <code>u64</code> and <code>f64</code> are often
aligned to 4 bytes (32 bits).</p>
<p>A type's size must always be a multiple of its alignment (Zero being a valid size
for any alignment). This ensures that an array of that type may always be indexed
by offsetting by a multiple of its size. Note that the size and alignment of a
type may not be known statically in the case of <a href="exotic-sizes.html#dynamically-sized-types-dsts">dynamically sized types</a>.</p>
<p>Rust gives you the following ways to lay out composite data:</p>
<ul>
<li>structs (named product types)</li>
<li>tuples (anonymous product types)</li>
<li>arrays (homogeneous product types)</li>
<li>enums (named sum types -- tagged unions)</li>
<li>unions (untagged unions)</li>
</ul>
<p>An enum is said to be <em>field-less</em> if none of its variants have associated data.</p>
<p>By default, composite structures have an alignment equal to the maximum
of their fields' alignments. Rust will consequently insert padding where
necessary to ensure that all fields are properly aligned and that the overall
type's size is a multiple of its alignment. For instance:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct A {
    a: u8,
    b: u32,
    c: u16,
}
<span class="boring">}</span></code></pre></pre>
<p>will be 32-bit aligned on a target that aligns these primitives to their
respective sizes. The whole struct will therefore have a size that is a multiple
of 32-bits. It may become:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct A {
    a: u8,
    _pad1: [u8; 3], // to align `b`
    b: u32,
    c: u16,
    _pad2: [u8; 2], // to make overall size multiple of 4
}
<span class="boring">}</span></code></pre></pre>
<p>or maybe:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct A {
    b: u32,
    c: u16,
    a: u8,
    _pad: u8,
}
<span class="boring">}</span></code></pre></pre>
<p>There is <em>no indirection</em> for these types; all data is stored within the struct,
as you would expect in C. However with the exception of arrays (which are
densely packed and in-order), the layout of data is not specified by default.
Given the two following struct definitions:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct A {
    a: i32,
    b: u64,
}

struct B {
    a: i32,
    b: u64,
}
<span class="boring">}</span></code></pre></pre>
<p>Rust <em>does</em> guarantee that two instances of A have their data laid out in
exactly the same way. However Rust <em>does not</em> currently guarantee that an
instance of A has the same field ordering or padding as an instance of B.</p>
<p>With A and B as written, this point would seem to be pedantic, but several other
features of Rust make it desirable for the language to play with data layout in
complex ways.</p>
<p>For instance, consider this struct:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Foo&lt;T, U&gt; {
    count: u16,
    data1: T,
    data2: U,
}
<span class="boring">}</span></code></pre></pre>
<p>Now consider the monomorphizations of <code>Foo&lt;u32, u16&gt;</code> and <code>Foo&lt;u16, u32&gt;</code>. If
Rust lays out the fields in the order specified, we expect it to pad the
values in the struct to satisfy their alignment requirements. So if Rust
didn't reorder fields, we would expect it to produce the following:</p>
<!-- ignore: explanation code -->
<pre><code class="language-rust ignore">struct Foo&lt;u16, u32&gt; {
    count: u16,
    data1: u16,
    data2: u32,
}

struct Foo&lt;u32, u16&gt; {
    count: u16,
    _pad1: u16,
    data1: u32,
    data2: u16,
    _pad2: u16,
}</code></pre>
<p>The latter case quite simply wastes space. An optimal use of space
requires different monomorphizations to have <em>different field orderings</em>.</p>
<p>Enums make this consideration even more complicated. Naively, an enum such as:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum Foo {
    A(u32),
    B(u64),
    C(u8),
}
<span class="boring">}</span></code></pre></pre>
<p>might be laid out as:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct FooRepr {
    data: u64, // this is either a u64, u32, or u8 based on `tag`
    tag: u8,   // 0 = A, 1 = B, 2 = C
}
<span class="boring">}</span></code></pre></pre>
<p>And indeed this is approximately how it would be laid out (modulo the
size and position of <code>tag</code>).</p>
<p>However there are several cases where such a representation is inefficient. The
classic case of this is Rust's "null pointer optimization": an enum consisting
of a single outer unit variant (e.g. <code>None</code>) and a (potentially nested) non-
nullable pointer variant (e.g. <code>Some(&amp;T)</code>) makes the tag unnecessary. A null
pointer can safely be interpreted as the unit (<code>None</code>) variant. The net
result is that, for example, <code>size_of::&lt;Option&lt;&amp;T&gt;&gt;() == size_of::&lt;&amp;T&gt;()</code>.</p>
<p>There are many types in Rust that are, or contain, non-nullable pointers such as
<code>Box&lt;T&gt;</code>, <code>Vec&lt;T&gt;</code>, <code>String</code>, <code>&amp;T</code>, and <code>&amp;mut T</code>. Similarly, one can imagine
nested enums pooling their tags into a single discriminant, as they are by
definition known to have a limited range of valid values. In principle enums could
use fairly elaborate algorithms to store bits throughout nested types with
forbidden values. As such it is <em>especially</em> desirable that
we leave enum layout unspecified today.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exotically-sized-types"><a class="header" href="#exotically-sized-types">Exotically Sized Types</a></h1>
<p>Most of the time, we expect types to have a statically known and positive size.
This isn't always the case in Rust.</p>
<h2 id="dynamically-sized-types-dsts"><a class="header" href="#dynamically-sized-types-dsts">Dynamically Sized Types (DSTs)</a></h2>
<p>Rust supports Dynamically Sized Types (DSTs): types without a statically
known size or alignment. On the surface, this is a bit nonsensical: Rust <em>must</em>
know the size and alignment of something in order to correctly work with it! In
this regard, DSTs are not normal types. Because they lack a statically known
size, these types can only exist behind a pointer. Any pointer to a
DST consequently becomes a <em>wide</em> pointer consisting of the pointer and the
information that "completes" them (more on this below).</p>
<p>There are two major DSTs exposed by the language:</p>
<ul>
<li>trait objects: <code>dyn MyTrait</code></li>
<li>slices: <a href="../std/primitive.slice.html"><code>[T]</code></a>, <a href="../std/primitive.str.html"><code>str</code></a>, and others</li>
</ul>
<p>A trait object represents some type that implements the traits it specifies.
The exact original type is <em>erased</em> in favor of runtime reflection
with a vtable containing all the information necessary to use the type.
The information that completes a trait object pointer is the vtable pointer.
The runtime size of the pointee can be dynamically requested from the vtable.</p>
<p>A slice is simply a view into some contiguous storage -- typically an array or
<code>Vec</code>. The information that completes a slice pointer is just the number of elements
it points to. The runtime size of the pointee is just the statically known size
of an element multiplied by the number of elements.</p>
<p>Structs can actually store a single DST directly as their last field, but this
makes them a DST as well:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Can't be stored on the stack directly
struct MySuperSlice {
    info: u32,
    data: [u8],
}
<span class="boring">}</span></code></pre></pre>
<p>Although such a type is largely useless without a way to construct it. Currently the
only properly supported way to create a custom DST is by making your type generic
and performing an <em>unsizing coercion</em>:</p>
<pre><pre class="playground"><code class="language-rust edition2021">struct MySuperSliceable&lt;T: ?Sized&gt; {
    info: u32,
    data: T,
}

fn main() {
    let sized: MySuperSliceable&lt;[u8; 8]&gt; = MySuperSliceable {
        info: 17,
        data: [0; 8],
    };

    let dynamic: &amp;MySuperSliceable&lt;[u8]&gt; = &amp;sized;

    // prints: "17 [0, 0, 0, 0, 0, 0, 0, 0]"
    println!("{} {:?}", dynamic.info, &amp;dynamic.data);
}</code></pre></pre>
<p>(Yes, custom DSTs are a largely half-baked feature for now.)</p>
<h2 id="zero-sized-types-zsts"><a class="header" href="#zero-sized-types-zsts">Zero Sized Types (ZSTs)</a></h2>
<p>Rust also allows types to be specified that occupy no space:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Nothing; // No fields = no size

// All fields have no size = no size
struct LotsOfNothing {
    foo: Nothing,
    qux: (),      // empty tuple has no size
    baz: [u8; 0], // empty array has no size
}
<span class="boring">}</span></code></pre></pre>
<p>On their own, Zero Sized Types (ZSTs) are, for obvious reasons, pretty useless.
However as with many curious layout choices in Rust, their potential is realized
in a generic context: Rust largely understands that any operation that produces
or stores a ZST can be reduced to a no-op. First off, storing it doesn't even
make sense -- it doesn't occupy any space. Also there's only one value of that
type, so anything that loads it can just produce it from the aether -- which is
also a no-op since it doesn't occupy any space.</p>
<p>One of the most extreme examples of this is Sets and Maps. Given a
<code>Map&lt;Key, Value&gt;</code>, it is common to implement a <code>Set&lt;Key&gt;</code> as just a thin wrapper
around <code>Map&lt;Key, UselessJunk&gt;</code>. In many languages, this would necessitate
allocating space for UselessJunk and doing work to store and load UselessJunk
only to discard it. Proving this unnecessary would be a difficult analysis for
the compiler.</p>
<p>However in Rust, we can just say that  <code>Set&lt;Key&gt; = Map&lt;Key, ()&gt;</code>. Now Rust
statically knows that every load and store is useless, and no allocation has any
size. The result is that the monomorphized code is basically a custom
implementation of a HashSet with none of the overhead that HashMap would have to
support values.</p>
<p>Safe code need not worry about ZSTs, but <em>unsafe</em> code must be careful about the
consequence of types with no size. In particular, pointer offsets are no-ops,
and allocators typically <a href="../std/alloc/trait.GlobalAlloc.html#tymethod.alloc">require a non-zero size</a>.</p>
<p>Note that references to ZSTs (including empty slices), just like all other
references, must be non-null and suitably aligned. Dereferencing a null or
unaligned pointer to a ZST is <a href="what-unsafe-does.html">undefined behavior</a>, just like for any other
type.</p>
<h2 id="empty-types"><a class="header" href="#empty-types">Empty Types</a></h2>
<p>Rust also enables types to be declared that <em>cannot even be instantiated</em>. These
types can only be talked about at the type level, and never at the value level.
Empty types can be declared by specifying an enum with no variants:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum Void {} // No variants = EMPTY
<span class="boring">}</span></code></pre></pre>
<p>Empty types are even more marginal than ZSTs. The primary motivating example for
an empty type is type-level unreachability. For instance, suppose an API needs to
return a Result in general, but a specific case actually is infallible. It's
actually possible to communicate this at the type level by returning a
<code>Result&lt;T, Void&gt;</code>. Consumers of the API can confidently unwrap such a Result
knowing that it's <em>statically impossible</em> for this value to be an <code>Err</code>, as
this would require providing a value of type <code>Void</code>.</p>
<p>In principle, Rust can do some interesting analyses and optimizations based
on this fact. For instance, <code>Result&lt;T, Void&gt;</code> is represented as just <code>T</code>,
because the <code>Err</code> case doesn't actually exist (strictly speaking, this is only
an optimization that is not guaranteed, so for example transmuting one into the
other is still Undefined Behavior).</p>
<p>The following <em>could</em> also compile:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum Void {}

let res: Result&lt;u32, Void&gt; = Ok(0);

// Err doesn't exist anymore, so Ok is actually irrefutable.
let Ok(num) = res;
<span class="boring">}</span></code></pre></pre>
<p>But this trick doesn't work yet.</p>
<p>One final subtle detail about empty types is that raw pointers to them are
actually valid to construct, but dereferencing them is Undefined Behavior
because that wouldn't make sense.</p>
<p>We recommend against modelling C's <code>void*</code> type with <code>*const Void</code>.
A lot of people started doing that but quickly ran into trouble because
Rust doesn't really have any safety guards against trying to instantiate
empty types with unsafe code, and if you do it, it's Undefined Behavior.
This was especially problematic because developers had a habit of converting
raw pointers to references and <code>&amp;Void</code> is <em>also</em> Undefined Behavior to
construct.</p>
<p><code>*const ()</code> (or equivalent) works reasonably well for <code>void*</code>, and can be made
into a reference without any safety problems. It still doesn't prevent you from
trying to read or write values, but at least it compiles to a no-op instead
of Undefined Behavior.</p>
<h2 id="extern-types"><a class="header" href="#extern-types">Extern Types</a></h2>
<p>There is <a href="https://github.com/rust-lang/rfcs/blob/master/text/1861-extern-types.md">an accepted RFC</a> to add proper types with an unknown size,
called <em>extern types</em>, which would let Rust developers model things like C's <code>void*</code>
and other "declared but never defined" types more accurately. However as of
Rust 2018, <a href="https://github.com/rust-lang/rust/issues/43467">the feature is stuck in limbo over how <code>size_of_val::&lt;MyExternType&gt;()</code>
should behave</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="alternative-representations"><a class="header" href="#alternative-representations">Alternative representations</a></h1>
<p>Rust allows you to specify alternative data layout strategies from the default.
There's also the <a href="https://rust-lang.github.io/unsafe-code-guidelines/layout.html">unsafe code guidelines</a> (note that it's <strong>NOT</strong> normative).</p>
<h2 id="reprc"><a class="header" href="#reprc">repr(C)</a></h2>
<p>This is the most important <code>repr</code>. It has fairly simple intent: do what C does.
The order, size, and alignment of fields is exactly what you would expect from C
or C++. Any type you expect to pass through an FFI boundary should have
<code>repr(C)</code>, as C is the lingua-franca of the programming world. This is also
necessary to soundly do more elaborate tricks with data layout such as
reinterpreting values as a different type.</p>
<p>We strongly recommend using <a href="https://rust-lang.github.io/rust-bindgen/">rust-bindgen</a> and/or <a href="https://github.com/eqrion/cbindgen">cbindgen</a> to manage your FFI
boundaries for you. The Rust team works closely with those projects to ensure
that they work robustly and are compatible with current and future guarantees
about type layouts and <code>repr</code>s.</p>
<p>The interaction of <code>repr(C)</code> with Rust's more exotic data layout features must be
kept in mind. Due to its dual purpose as "for FFI" and "for layout control",
<code>repr(C)</code> can be applied to types that will be nonsensical or problematic if
passed through the FFI boundary.</p>
<ul>
<li>
<p>ZSTs are still zero-sized, even though this is not a standard behavior in
C, and is explicitly contrary to the behavior of an empty type in C++, which
says they should still consume a byte of space.</p>
</li>
<li>
<p>DST pointers (wide pointers) and tuples are not a concept
in C, and as such are never FFI-safe.</p>
</li>
<li>
<p>Enums with fields also aren't a concept in C or C++, but a valid bridging
of the types <a href="https://github.com/rust-lang/rfcs/blob/master/text/2195-really-tagged-unions.md">is defined</a>.</p>
</li>
<li>
<p>If <code>T</code> is an <a href="ffi.html#the-nullable-pointer-optimization">FFI-safe non-nullable pointer
type</a>,
<code>Option&lt;T&gt;</code> is guaranteed to have the same layout and ABI as <code>T</code> and is
therefore also FFI-safe. As of this writing, this covers <code>&amp;</code>, <code>&amp;mut</code>,
and function pointers, all of which can never be null.</p>
</li>
<li>
<p>Tuple structs are like structs with regards to <code>repr(C)</code>, as the only
difference from a struct is that the fields aren’t named.</p>
</li>
<li>
<p><code>repr(C)</code> is equivalent to one of <code>repr(u*)</code> (see the next section) for
fieldless enums. The chosen size is the default enum size for the target platform's C
application binary interface (ABI). Note that enum representation in C is implementation
defined, so this is really a "best guess". In particular, this may be incorrect
when the C code of interest is compiled with certain flags.</p>
</li>
<li>
<p>Fieldless enums with <code>repr(C)</code> or <code>repr(u*)</code> still may not be set to an
integer value without a corresponding variant, even though this is
permitted behavior in C or C++. It is undefined behavior to (unsafely)
construct an instance of an enum that does not match one of its
variants. (This allows exhaustive matches to continue to be written and
compiled as normal.)</p>
</li>
</ul>
<h2 id="reprtransparent"><a class="header" href="#reprtransparent">repr(transparent)</a></h2>
<p><code>#[repr(transparent)]</code> can only be used on a struct or single-variant enum that has a single non-zero-sized field (there may be additional zero-sized fields).
The effect is that the layout and ABI of the whole struct/enum is guaranteed to be the same as that one field.</p>
<blockquote>
<p>NOTE: There's a <code>transparent_unions</code> nightly feature to apply <code>repr(transparent)</code> to unions,
but it hasn't been stabilized due to design concerns. See the <a href="https://github.com/rust-lang/rust/issues/60405">tracking issue</a> for more details.</p>
</blockquote>
<p>The goal is to make it possible to transmute between the single field and the
struct/enum. An example of that is <a href="../std/cell/struct.UnsafeCell.html"><code>UnsafeCell</code></a>, which can be transmuted into
the type it wraps (<a href="../std/cell/struct.UnsafeCell.html"><code>UnsafeCell</code></a> also uses the unstable <a href="https://github.com/rust-lang/rust/pull/68491">no_niche</a>,
so its ABI is not actually guaranteed to be the same when nested in other types).</p>
<p>Also, passing the struct/enum through FFI where the inner field type is expected on
the other side is guaranteed to work. In particular, this is necessary for
<code>struct Foo(f32)</code> or <code>enum Foo { Bar(f32) }</code> to always have the same ABI as <code>f32</code>.</p>
<p>This repr is only considered part of the public ABI of a type if either the single
field is <code>pub</code>, or if its layout is documented in prose. Otherwise, the layout should
not be relied upon by other crates.</p>
<p>More details are in the <a href="https://github.com/rust-lang/rfcs/blob/master/text/1758-repr-transparent.md">RFC 1758</a> and the <a href="https://rust-lang.github.io/rfcs/2645-transparent-unions.html">RFC 2645</a>.</p>
<h2 id="repru-repri"><a class="header" href="#repru-repri">repr(u*), repr(i*)</a></h2>
<p>These specify the size to make a fieldless enum. If the discriminant overflows
the integer it has to fit in, it will produce a compile-time error. You can
manually ask Rust to allow this by setting the overflowing element to explicitly
be 0. However Rust will not allow you to create an enum where two variants have
the same discriminant.</p>
<p>The term "fieldless enum" only means that the enum doesn't have data in any
of its variants. A fieldless enum without a <code>repr(u*)</code> or <code>repr(C)</code> is
still a Rust native type, and does not have a stable ABI representation.
Adding a <code>repr</code> causes it to be treated exactly like the specified
integer size for ABI purposes.</p>
<p>If the enum has fields, the effect is similar to the effect of <code>repr(C)</code>
in that there is a defined layout of the type. This makes it possible to
pass the enum to C code, or access the type's raw representation and directly
manipulate its tag and fields. See <a href="https://github.com/rust-lang/rfcs/blob/master/text/2195-really-tagged-unions.md">the RFC</a> for details.</p>
<p>These <code>repr</code>s have no effect on a struct.</p>
<p>Adding an explicit <code>repr(u*)</code>, <code>repr(i*)</code>, or <code>repr(C)</code> to an enum with fields suppresses the null-pointer optimization, like:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::mem::size_of;
</span>enum MyOption&lt;T&gt; {
    Some(T),
    None,
}

#[repr(u8)]
enum MyReprOption&lt;T&gt; {
    Some(T),
    None,
}

assert_eq!(8, size_of::&lt;MyOption&lt;&amp;u16&gt;&gt;());
assert_eq!(16, size_of::&lt;MyReprOption&lt;&amp;u16&gt;&gt;());
<span class="boring">}</span></code></pre></pre>
<p>This optimization still applies to fieldless enums with an explicit <code>repr(u*)</code>, <code>repr(i*)</code>, or <code>repr(C)</code>.</p>
<h2 id="reprpacked"><a class="header" href="#reprpacked">repr(packed)</a></h2>
<p><code>repr(packed)</code> forces Rust to strip any padding, and only align the type to a
byte. This may improve the memory footprint, but will likely have other negative
side-effects.</p>
<p>In particular, most architectures <em>strongly</em> prefer values to be aligned. This
may mean the unaligned loads are penalized (x86), or even fault (some ARM
chips). For simple cases like directly loading or storing a packed field, the
compiler might be able to paper over alignment issues with shifts and masks.
However if you take a reference to a packed field, it's unlikely that the
compiler will be able to emit code to avoid an unaligned load.</p>
<p><a href="https://github.com/rust-lang/rust/issues/27060">As this can cause undefined behavior</a>, the lint has been implemented
and it will become a hard error.</p>
<p><code>repr(packed)</code> is not to be used lightly. Unless you have extreme requirements,
this should not be used.</p>
<p>This repr is a modifier on <code>repr(C)</code> and <code>repr(Rust)</code>.</p>
<h2 id="repralignn"><a class="header" href="#repralignn">repr(align(n))</a></h2>
<p><code>repr(align(n))</code> (where <code>n</code> is a power of two) forces the type to have an
alignment of <em>at least</em> n.</p>
<p>This enables several tricks, like making sure neighboring elements of an array
never share the same cache line with each other (which may speed up certain
kinds of concurrent code).</p>
<p>This is a modifier on <code>repr(C)</code> and <code>repr(Rust)</code>. It is incompatible with
<code>repr(packed)</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ownership-and-lifetimes"><a class="header" href="#ownership-and-lifetimes">Ownership and Lifetimes</a></h1>
<p>Ownership is the breakout feature of Rust. It allows Rust to be completely
memory-safe and efficient, while avoiding garbage collection. Before getting
into the ownership system in detail, we will consider the motivation of this
design.</p>
<p>We will assume that you accept that garbage collection (GC) is not always an
optimal solution, and that it is desirable to manually manage memory in some
contexts. If you do not accept this, might I interest you in a different
language?</p>
<p>Regardless of your feelings on GC, it is pretty clearly a <em>massive</em> boon to
making code safe. You never have to worry about things going away <em>too soon</em>
(although whether you still wanted to be pointing at that thing is a different
issue...). This is a pervasive problem that C and C++ programs need to deal
with. Consider this simple mistake that all of us who have used a non-GC'd
language have made at one point:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn as_str(data: &amp;u32) -&gt; &amp;str {
    // compute the string
    let s = format!("{}", data);

    // OH NO! We returned a reference to something that
    // exists only in this function!
    // Dangling pointer! Use after free! Alas!
    // (this does not compile in Rust)
    &amp;s
}
<span class="boring">}</span></code></pre></pre>
<p>This is exactly what Rust's ownership system was built to solve.
Rust knows the scope in which the <code>&amp;s</code> lives, and as such can prevent it from
escaping. However this is a simple case that even a C compiler could plausibly
catch. Things get more complicated as code gets bigger and pointers get fed through
various functions. Eventually, a C compiler will fall down and won't be able to
perform sufficient escape analysis to prove your code unsound. It will consequently
be forced to accept your program on the assumption that it is correct.</p>
<p>This will never happen to Rust. It's up to the programmer to prove to the
compiler that everything is sound.</p>
<p>Of course, Rust's story around ownership is much more complicated than just
verifying that references don't escape the scope of their referent. That's
because ensuring pointers are always valid is much more complicated than this.
For instance in this code,</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = vec![1, 2, 3];
// get an internal reference
let x = &amp;data[0];

// OH NO! `push` causes the backing storage of `data` to be reallocated.
// Dangling pointer! Use after free! Alas!
// (this does not compile in Rust)
data.push(4);

println!("{}", x);
<span class="boring">}</span></code></pre></pre>
<p>naive scope analysis would be insufficient to prevent this bug, because <code>data</code>
does in fact live as long as we needed. However it was <em>changed</em> while we had
a reference into it. This is why Rust requires any references to freeze the
referent and its owners.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<p>There are two kinds of reference:</p>
<ul>
<li>Shared reference: <code>&amp;</code></li>
<li>Mutable reference: <code>&amp;mut</code></li>
</ul>
<p>Which obey the following rules:</p>
<ul>
<li>A reference cannot outlive its referent</li>
<li>A mutable reference cannot be aliased</li>
</ul>
<p>That's it. That's the whole model references follow.</p>
<p>Of course, we should probably define what <em>aliased</em> means.</p>
<pre><code class="language-text">error[E0425]: cannot find value `aliased` in this scope
 --&gt; &lt;rust.rs&gt;:2:20
  |
2 |     println!("{}", aliased);
  |                    ^^^^^^^ not found in this scope

error: aborting due to previous error
</code></pre>
<p>Unfortunately, Rust hasn't actually defined its aliasing model. 🙀</p>
<p>While we wait for the Rust devs to specify the semantics of their language,
let's use the next section to discuss what aliasing is in general, and why it
matters.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aliasing"><a class="header" href="#aliasing">Aliasing</a></h1>
<p>First off, let's get some important caveats out of the way:</p>
<ul>
<li>
<p>We will be using the broadest possible definition of aliasing for the sake
of discussion. Rust's definition will probably be more restricted to factor
in mutations and liveness.</p>
</li>
<li>
<p>We will be assuming a single-threaded, interrupt-free, execution. We will also
be ignoring things like memory-mapped hardware. Rust assumes these things
don't happen unless you tell it otherwise. For more details, see the
<a href="concurrency.html">Concurrency Chapter</a>.</p>
</li>
</ul>
<p>With that said, here's our working definition: variables and pointers <em>alias</em>
if they refer to overlapping regions of memory.</p>
<h2 id="why-aliasing-matters"><a class="header" href="#why-aliasing-matters">Why Aliasing Matters</a></h2>
<p>So why should we care about aliasing?</p>
<p>Consider this simple function:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn compute(input: &amp;u32, output: &amp;mut u32) {
    if *input &gt; 10 {
        *output = 1;
    }
    if *input &gt; 5 {
        *output *= 2;
    }
    // remember that `output` will be `2` if `input &gt; 10`
}
<span class="boring">}</span></code></pre></pre>
<p>We would <em>like</em> to be able to optimize it to the following function:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn compute(input: &amp;u32, output: &amp;mut u32) {
    let cached_input = *input; // keep `*input` in a register
    if cached_input &gt; 10 {
        // If the input is greater than 10, the previous code would set the output to 1 and then double it,
        // resulting in an output of 2 (because `&gt;10` implies `&gt;5`).
        // Here, we avoid the double assignment and just set it directly to 2.
        *output = 2;
    } else if cached_input &gt; 5 {
        *output *= 2;
    }
}
<span class="boring">}</span></code></pre></pre>
<p>In Rust, this optimization should be sound. For almost any other language, it
wouldn't be (barring global analysis). This is because the optimization relies
on knowing that aliasing doesn't occur, which most languages are fairly liberal
with. Specifically, we need to worry about function arguments that make <code>input</code>
and <code>output</code> overlap, such as <code>compute(&amp;x, &amp;mut x)</code>.</p>
<p>With that input, we could get this execution:</p>
<!-- ignore: expanded code -->
<pre><code class="language-rust ignore">                    //  input ==  output == 0xabad1dea
                    // *input == *output == 20
if *input &gt; 10 {    // true  (*input == 20)
    *output = 1;    // also overwrites *input, because they are the same
}
if *input &gt; 5 {     // false (*input == 1)
    *output *= 2;
}
                    // *input == *output == 1</code></pre>
<p>Our optimized function would produce <code>*output == 2</code> for this input, so the
correctness of our optimization relies on this input being impossible.</p>
<p>In Rust we know this input should be impossible because <code>&amp;mut</code> isn't allowed to be
aliased. So we can safely reject its possibility and perform this optimization.
In most other languages, this input would be entirely possible, and must be considered.</p>
<p>This is why alias analysis is important: it lets the compiler perform useful
optimizations! Some examples:</p>
<ul>
<li>keeping values in registers by proving no pointers access the value's memory</li>
<li>eliminating reads by proving some memory hasn't been written to since last we read it</li>
<li>eliminating writes by proving some memory is never read before the next write to it</li>
<li>moving or reordering reads and writes by proving they don't depend on each other</li>
</ul>
<p>These optimizations also tend to prove the soundness of bigger optimizations
such as loop vectorization, constant propagation, and dead code elimination.</p>
<p>In the previous example, we used the fact that <code>&amp;mut u32</code> can't be aliased to prove
that writes to <code>*output</code> can't possibly affect <code>*input</code>. This lets us cache <code>*input</code>
in a register, eliminating a read.</p>
<p>By caching this read, we knew that the write in the <code>&gt; 10</code> branch couldn't
affect whether we take the <code>&gt; 5</code> branch, allowing us to also eliminate a
read-modify-write (doubling <code>*output</code>) when <code>*input &gt; 10</code>.</p>
<p>The key thing to remember about alias analysis is that writes are the primary
hazard for optimizations. That is, the only thing that prevents us
from moving a read to any other part of the program is the possibility of us
re-ordering it with a write to the same location.</p>
<p>For instance, we have no concern for aliasing in the following modified version
of our function, because we've moved the only write to <code>*output</code> to the very
end of our function. This allows us to freely reorder the reads of <code>*input</code> that
occur before it:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn compute(input: &amp;u32, output: &amp;mut u32) {
    let mut temp = *output;
    if *input &gt; 10 {
        temp = 1;
    }
    if *input &gt; 5 {
        temp *= 2;
    }
    *output = temp;
}
<span class="boring">}</span></code></pre></pre>
<p>We're still relying on alias analysis to assume that <code>input</code> doesn't alias
<code>temp</code>, but the proof is much simpler: the value of a local variable can't be
aliased by things that existed before it was declared. This is an assumption
every language freely makes, and so this version of the function could be
optimized the way we want in any language.</p>
<p>This is why the definition of "alias" that Rust will use likely involves some
notion of liveness and mutation: we don't actually care if aliasing occurs if
there aren't any actual writes to memory happening.</p>
<p>Of course, a full aliasing model for Rust must also take into consideration things like
function calls (which may mutate things we don't see), raw pointers (which have
no aliasing requirements on their own), and UnsafeCell (which lets the referent
of an <code>&amp;</code> be mutated).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lifetimes"><a class="header" href="#lifetimes">Lifetimes</a></h1>
<p>Rust enforces these rules through <em>lifetimes</em>. Lifetimes are named
regions of code that a reference must be valid for. Those regions
may be fairly complex, as they correspond to paths of execution
in the program. There may even be holes in these paths of execution,
as it's possible to invalidate a reference as long as it's reinitialized
before it's used again. Types which contain references (or pretend to)
may also be tagged with lifetimes so that Rust can prevent them from
being invalidated as well.</p>
<p>In most of our examples, the lifetimes will coincide with scopes. This is
because our examples are simple. The more complex cases where they don't
coincide are described below.</p>
<p>Within a function body, Rust generally doesn't let you explicitly name the
lifetimes involved. This is because it's generally not really necessary
to talk about lifetimes in a local context; Rust has all the information and
can work out everything as optimally as possible. Many anonymous scopes and
temporaries that you would otherwise have to write are often introduced to
make your code Just Work.</p>
<p>However once you cross the function boundary, you need to start talking about
lifetimes. Lifetimes are denoted with an apostrophe: <code>'a</code>, <code>'static</code>. To dip
our toes with lifetimes, we're going to pretend that we're actually allowed
to label scopes with lifetimes, and desugar the examples from the start of
this chapter.</p>
<p>Originally, our examples made use of <em>aggressive</em> sugar -- high fructose corn
syrup even -- around scopes and lifetimes, because writing everything out
explicitly is <em>extremely noisy</em>. All Rust code relies on aggressive inference
and elision of "obvious" things.</p>
<p>One particularly interesting piece of sugar is that each <code>let</code> statement
implicitly introduces a scope. For the most part, this doesn't really matter.
However it does matter for variables that refer to each other. As a simple
example, let's completely desugar this simple piece of Rust code:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = 0;
let y = &amp;x;
let z = &amp;y;
<span class="boring">}</span></code></pre></pre>
<p>The borrow checker always tries to minimize the extent of a lifetime, so it will
likely desugar to the following:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">// NOTE: `'a: {` and `&amp;'b x` is not valid syntax!
'a: {
    let x: i32 = 0;
    'b: {
        // lifetime used is 'b because that's good enough.
        let y: &amp;'b i32 = &amp;'b x;
        'c: {
            // ditto on 'c
            let z: &amp;'c &amp;'b i32 = &amp;'c y; // "a reference to a reference to an i32" (with lifetimes annotated)
        }
    }
}</code></pre>
<p>Wow. That's... awful. Let's all take a moment to thank Rust for making this easier.</p>
<p>Actually passing references to outer scopes will cause Rust to infer
a larger lifetime:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = 0;
let z;
let y = &amp;x;
z = y;
<span class="boring">}</span></code></pre></pre>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">'a: {
    let x: i32 = 0;
    'b: {
        let z: &amp;'b i32;
        'c: {
            // Must use 'b here because the reference to x is
            // being passed to the scope 'b.
            let y: &amp;'b i32 = &amp;'b x;
            z = y;
        }
    }
}</code></pre>
<h2 id="example-references-that-outlive-referents"><a class="header" href="#example-references-that-outlive-referents">Example: references that outlive referents</a></h2>
<p>Alright, let's look at some of those examples from before:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn as_str(data: &amp;u32) -&gt; &amp;str {
    let s = format!("{}", data);
    &amp;s
}
<span class="boring">}</span></code></pre></pre>
<p>desugars to:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">fn as_str&lt;'a&gt;(data: &amp;'a u32) -&gt; &amp;'a str {
    'b: {
        let s = format!("{}", data);
        return &amp;'a s;
    }
}</code></pre>
<p>This signature of <code>as_str</code> takes a reference to a u32 with <em>some</em> lifetime, and
promises that it can produce a reference to a str that can live <em>just as long</em>.
Already we can see why this signature might be trouble. That basically implies
that we're going to find a str somewhere in the scope the reference
to the u32 originated in, or somewhere <em>even earlier</em>. That's a bit of a tall
order.</p>
<p>We then proceed to compute the string <code>s</code>, and return a reference to it. Since
the contract of our function says the reference must outlive <code>'a</code>, that's the
lifetime we infer for the reference. Unfortunately, <code>s</code> was defined in the
scope <code>'b</code>, so the only way this is sound is if <code>'b</code> contains <code>'a</code> -- which is
clearly false since <code>'a</code> must contain the function call itself. We have therefore
created a reference whose lifetime outlives its referent, which is <em>literally</em>
the first thing we said that references can't do. The compiler rightfully blows
up in our face.</p>
<p>To make this more clear, we can expand the example:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">fn as_str&lt;'a&gt;(data: &amp;'a u32) -&gt; &amp;'a str {
    'b: {
        let s = format!("{}", data);
        return &amp;'a s
    }
}

fn main() {
    'c: {
        let x: u32 = 0;
        'd: {
            // An anonymous scope is introduced because the borrow does not
            // need to last for the whole scope x is valid for. The return
            // of as_str must find a str somewhere before this function
            // call. Obviously not happening.
            println!("{}", as_str::&lt;'d&gt;(&amp;'d x));
        }
    }
}</code></pre>
<p>Shoot!</p>
<p>Of course, the right way to write this function is as follows:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn to_string(data: &amp;u32) -&gt; String {
    format!("{}", data)
}
<span class="boring">}</span></code></pre></pre>
<p>We must produce an owned value inside the function to return it! The only way
we could have returned an <code>&amp;'a str</code> would have been if it was in a field of the
<code>&amp;'a u32</code>, which is obviously not the case.</p>
<p>(Actually we could have also just returned a string literal, which as a global
can be considered to reside at the bottom of the stack; though this limits
our implementation <em>just a bit</em>.)</p>
<h2 id="example-aliasing-a-mutable-reference"><a class="header" href="#example-aliasing-a-mutable-reference">Example: aliasing a mutable reference</a></h2>
<p>How about the other example:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = vec![1, 2, 3];
let x = &amp;data[0];
data.push(4);
println!("{}", x);
<span class="boring">}</span></code></pre></pre>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">'a: {
    let mut data: Vec&lt;i32&gt; = vec![1, 2, 3];
    'b: {
        // 'b is as big as we need this borrow to be
        // (just need to get to `println!`)
        let x: &amp;'b i32 = Index::index::&lt;'b&gt;(&amp;'b data, 0);
        'c: {
            // Temporary scope because we don't need the
            // &amp;mut to last any longer.
            Vec::push(&amp;'c mut data, 4);
        }
        println!("{}", x);
    }
}</code></pre>
<p>The problem here is a bit more subtle and interesting. We want Rust to
reject this program for the following reason: We have a live shared reference <code>x</code>
to a descendant of <code>data</code> when we try to take a mutable reference to <code>data</code>
to <code>push</code>. This would create an aliased mutable reference, which would
violate the <em>second</em> rule of references.</p>
<p>However this is <em>not at all</em> how Rust reasons that this program is bad. Rust
doesn't understand that <code>x</code> is a reference to a subpath of <code>data</code>. It doesn't
understand <code>Vec</code> at all. What it <em>does</em> see is that <code>x</code> has to live for <code>'b</code> in
order to be printed. The signature of <code>Index::index</code> subsequently demands that
the reference we take to <code>data</code> has to survive for <code>'b</code>. When we try to call
<code>push</code>, it then sees us try to make an <code>&amp;'c mut data</code>. Rust knows that <code>'c</code> is
contained within <code>'b</code>, and rejects our program because the <code>&amp;'b data</code> must still
be alive!</p>
<p>Here we see that the lifetime system is much more coarse than the reference
semantics we're actually interested in preserving. For the most part, <em>that's
totally ok</em>, because it keeps us from spending all day explaining our program
to the compiler. However it does mean that several programs that are totally
correct with respect to Rust's <em>true</em> semantics are rejected because lifetimes
are too dumb.</p>
<h2 id="the-area-covered-by-a-lifetime"><a class="header" href="#the-area-covered-by-a-lifetime">The area covered by a lifetime</a></h2>
<p>A reference (sometimes called a <em>borrow</em>) is <em>alive</em> from the place it is
created to its last use. The borrowed value needs to outlive only borrows that
are alive. This looks simple, but there are a few subtleties.</p>
<p>The following snippet compiles, because after printing <code>x</code>, it is no longer
needed, so it doesn't matter if it is dangling or aliased (even though the
variable <code>x</code> <em>technically</em> exists to the very end of the scope).</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = vec![1, 2, 3];
let x = &amp;data[0];
println!("{}", x);
// This is OK, x is no longer needed
data.push(4);
<span class="boring">}</span></code></pre></pre>
<p>However, if the value has a destructor, the destructor is run at the end of the
scope. And running the destructor is considered a use ‒ obviously the last one.
So, this will <em>not</em> compile.</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug)]
struct X&lt;'a&gt;(&amp;'a i32);

impl Drop for X&lt;'_&gt; {
    fn drop(&amp;mut self) {}
}

let mut data = vec![1, 2, 3];
let x = X(&amp;data[0]);
println!("{:?}", x);
data.push(4);
// Here, the destructor is run and therefore this'll fail to compile.
<span class="boring">}</span></code></pre></pre>
<p>One way to convince the compiler that <code>x</code> is no longer valid is by using <code>drop(x)</code> before <code>data.push(4)</code>.</p>
<p>Furthermore, there might be multiple possible last uses of the borrow, for
example in each branch of a condition.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">fn some_condition() -&gt; bool { true }
</span>let mut data = vec![1, 2, 3];
let x = &amp;data[0];

if some_condition() {
    println!("{}", x); // This is the last use of `x` in this branch
    data.push(4);      // So we can push here
} else {
    // There's no use of `x` in here, so effectively the last use is the
    // creation of x at the top of the example.
    data.push(5);
}
<span class="boring">}</span></code></pre></pre>
<p>And a lifetime can have a pause in it. Or you might look at it as two distinct
borrows just being tied to the same local variable. This often happens around
loops (writing a new value of a variable at the end of the loop and using it for
the last time at the top of the next iteration).</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = vec![1, 2, 3];
// This mut allows us to change where the reference points to
let mut x = &amp;data[0];

println!("{}", x); // Last use of this borrow
data.push(4);
x = &amp;data[3]; // We start a new borrow here
println!("{}", x);
<span class="boring">}</span></code></pre></pre>
<p>Historically, Rust kept the borrow alive until the end of scope, so these
examples might fail to compile with older compilers. Also, there are still some
corner cases where Rust fails to properly shorten the live part of the borrow
and fails to compile even when it looks like it should. These'll be solved over
time.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="limits-of-lifetimes"><a class="header" href="#limits-of-lifetimes">Limits of Lifetimes</a></h1>
<p>Given the following code:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">#[derive(Debug)]
struct Foo;

impl Foo {
    fn mutate_and_share(&amp;mut self) -&gt; &amp;Self { &amp;*self }
    fn share(&amp;self) {}
}

fn main() {
    let mut foo = Foo;
    let loan = foo.mutate_and_share();
    foo.share();
    println!("{:?}", loan);
}</code></pre></pre>
<p>One might expect it to compile. We call <code>mutate_and_share</code>, which mutably
borrows <code>foo</code> temporarily, but then returns only a shared reference. Therefore
we would expect <code>foo.share()</code> to succeed as <code>foo</code> shouldn't be mutably borrowed.</p>
<p>However when we try to compile it:</p>
<pre><code class="language-text">error[E0502]: cannot borrow `foo` as immutable because it is also borrowed as mutable
  --&gt; src/main.rs:12:5
   |
11 |     let loan = foo.mutate_and_share();
   |                --- mutable borrow occurs here
12 |     foo.share();
   |     ^^^ immutable borrow occurs here
13 |     println!("{:?}", loan);
</code></pre>
<p>What happened? Well, we got the exact same reasoning as we did for
<a href="lifetimes.html#example-aliasing-a-mutable-reference">Example 2 in the previous section</a>. We desugar the program and we get
the following:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">struct Foo;

impl Foo {
    fn mutate_and_share&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a Self { &amp;'a *self }
    fn share&lt;'a&gt;(&amp;'a self) {}
}

fn main() {
    'b: {
        let mut foo: Foo = Foo;
        'c: {
            let loan: &amp;'c Foo = Foo::mutate_and_share::&lt;'c&gt;(&amp;'c mut foo);
            'd: {
                Foo::share::&lt;'d&gt;(&amp;'d foo);
            }
            println!("{:?}", loan);
        }
    }
}</code></pre>
<p>The lifetime system is forced to extend the <code>&amp;mut foo</code> to have lifetime <code>'c</code>,
due to the lifetime of <code>loan</code> and <code>mutate_and_share</code>'s signature. Then when we
try to call <code>share</code>, it sees we're trying to alias that <code>&amp;'c mut foo</code> and
blows up in our face!</p>
<p>This program is clearly correct according to the reference semantics we actually
care about, but the lifetime system is too coarse-grained to handle that.</p>
<h2 id="improperly-reduced-borrows"><a class="header" href="#improperly-reduced-borrows">Improperly reduced borrows</a></h2>
<p>The following code fails to compile, because Rust sees that a variable, <code>map</code>,
is borrowed twice, and can not infer that the first borrow ceases to be needed
before the second one occurs. This is caused by Rust conservatively falling back
to using a whole scope for the first borrow. This will eventually get fixed.</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::collections::HashMap;
</span><span class="boring">use std::hash::Hash;
</span>fn get_default&lt;'m, K, V&gt;(map: &amp;'m mut HashMap&lt;K, V&gt;, key: K) -&gt; &amp;'m mut V
where
    K: Clone + Eq + Hash,
    V: Default,
{
    match map.get_mut(&amp;key) {
        Some(value) =&gt; value,
        None =&gt; {
            map.insert(key.clone(), V::default());
            map.get_mut(&amp;key).unwrap()
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Because of the lifetime restrictions imposed, <code>&amp;mut map</code>'s lifetime
overlaps other mutable borrows, resulting in a compile error:</p>
<pre><code class="language-text">error[E0499]: cannot borrow `*map` as mutable more than once at a time
  --&gt; src/main.rs:12:13
   |
4  |   fn get_default&lt;'m, K, V&gt;(map: &amp;'m mut HashMap&lt;K, V&gt;, key: K) -&gt; &amp;'m mut V
   |                  -- lifetime `'m` defined here
...
9  |       match map.get_mut(&amp;key) {
   |       -     --- first mutable borrow occurs here
   |  _____|
   | |
10 | |         Some(value) =&gt; value,
11 | |         None =&gt; {
12 | |             map.insert(key.clone(), V::default());
   | |             ^^^ second mutable borrow occurs here
13 | |             map.get_mut(&amp;key).unwrap()
14 | |         }
15 | |     }
   | |_____- returning this value requires that `*map` is borrowed for `'m`
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lifetime-elision"><a class="header" href="#lifetime-elision">Lifetime Elision</a></h1>
<p>In order to make common patterns more ergonomic, Rust allows lifetimes to be
<em>elided</em> in function signatures.</p>
<p>A <em>lifetime position</em> is anywhere you can write a lifetime in a type:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">&amp;'a T
&amp;'a mut T
T&lt;'a&gt;</code></pre>
<p>Lifetime positions can appear as either "input" or "output":</p>
<ul>
<li>
<p>For <code>fn</code> definitions, <code>fn</code> types, and the traits <code>Fn</code>, <code>FnMut</code>, and <code>FnOnce</code>,
input refers to the types of the formal arguments, while output refers to
result types. So <code>fn foo(s: &amp;str) -&gt; (&amp;str, &amp;str)</code> has elided one lifetime in
input position and two lifetimes in output position. Note that the input
positions of a <code>fn</code> method definition do not include the lifetimes that occur
in the method's <code>impl</code> header (nor lifetimes that occur in the trait header,
for a default method).</p>
</li>
<li>
<p>For <code>impl</code> headers, all types are input. So <code>impl Trait&lt;&amp;T&gt; for Struct&lt;&amp;T&gt;</code>
has elided two lifetimes in input position, while <code>impl Struct&lt;&amp;T&gt;</code> has elided
one.</p>
</li>
</ul>
<p>Elision rules are as follows:</p>
<ul>
<li>
<p>Each elided lifetime in input position becomes a distinct lifetime
parameter.</p>
</li>
<li>
<p>If there is exactly one input lifetime position (elided or not), that lifetime
is assigned to <em>all</em> elided output lifetimes.</p>
</li>
<li>
<p>If there are multiple input lifetime positions, but one of them is <code>&amp;self</code> or
<code>&amp;mut self</code>, the lifetime of <code>self</code> is assigned to <em>all</em> elided output lifetimes.</p>
</li>
<li>
<p>Otherwise, it is an error to elide an output lifetime.</p>
</li>
</ul>
<p>Examples:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn print(s: &amp;str);                                      // elided
fn print&lt;'a&gt;(s: &amp;'a str);                               // expanded

fn debug(lvl: usize, s: &amp;str);                          // elided
fn debug&lt;'a&gt;(lvl: usize, s: &amp;'a str);                   // expanded

fn substr(s: &amp;str, until: usize) -&gt; &amp;str;               // elided
fn substr&lt;'a&gt;(s: &amp;'a str, until: usize) -&gt; &amp;'a str;     // expanded

fn get_str() -&gt; &amp;str;                                   // ILLEGAL

fn frob(s: &amp;str, t: &amp;str) -&gt; &amp;str;                      // ILLEGAL

fn get_mut(&amp;mut self) -&gt; &amp;mut T;                        // elided
fn get_mut&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T;              // expanded

fn args&lt;T: ToCStr&gt;(&amp;mut self, args: &amp;[T]) -&gt; &amp;mut Command                  // elided
fn args&lt;'a, 'b, T: ToCStr&gt;(&amp;'a mut self, args: &amp;'b [T]) -&gt; &amp;'a mut Command // expanded

fn new(buf: &amp;mut [u8]) -&gt; BufWriter;                    // elided
fn new(buf: &amp;mut [u8]) -&gt; BufWriter&lt;'_&gt;;                // elided (with `rust_2018_idioms`)
fn new&lt;'a&gt;(buf: &amp;'a mut [u8]) -&gt; BufWriter&lt;'a&gt;          // expanded</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unbounded-lifetimes"><a class="header" href="#unbounded-lifetimes">Unbounded Lifetimes</a></h1>
<p>Unsafe code can often end up producing references or lifetimes out of thin air.
Such lifetimes come into the world as <em>unbounded</em>. The most common source of
this is taking a reference to a dereferenced raw pointer, which produces a
reference with an unbounded lifetime. Such a lifetime becomes as big as context
demands. This is in fact more powerful than simply becoming <code>'static</code>, because
for instance <code>&amp;'static &amp;'a T</code> will fail to typecheck, but the unbound lifetime
will perfectly mold into <code>&amp;'a &amp;'a T</code> as needed. However for most intents and
purposes, such an unbounded lifetime can be regarded as <code>'static</code>.</p>
<p>Almost no reference is <code>'static</code>, so this is probably wrong. <code>transmute</code> and
<code>transmute_copy</code> are the two other primary offenders. One should endeavor to
bound an unbounded lifetime as quickly as possible, especially across function
boundaries.</p>
<p>Given a function, any output lifetimes that don't derive from inputs are
unbounded. For instance:</p>
<!-- no_run: This example exhibits undefined behavior. -->
<pre><pre class="playground"><code class="language-rust no_run edition2021">fn get_str&lt;'a&gt;(s: *const String) -&gt; &amp;'a str {
    unsafe { &amp;*s }
}

fn main() {
    let soon_dropped = String::from("hello");
    let dangling = get_str(&amp;soon_dropped);
    drop(soon_dropped);
    println!("Invalid str: {}", dangling); // Invalid str: gӚ_`
}</code></pre></pre>
<p>The easiest way to avoid unbounded lifetimes is to use lifetime elision at the
function boundary. If an output lifetime is elided, then it <em>must</em> be bounded by
an input lifetime. Of course it might be bounded by the <em>wrong</em> lifetime, but
this will usually just cause a compiler error, rather than allow memory safety
to be trivially violated.</p>
<p>Within a function, bounding lifetimes is more error-prone. The safest and easiest
way to bound a lifetime is to return it from a function with a bound lifetime.
However if this is unacceptable, the reference can be placed in a location with
a specific lifetime. Unfortunately it's impossible to name all lifetimes involved
in a function.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="higher-rank-trait-bounds-hrtbs"><a class="header" href="#higher-rank-trait-bounds-hrtbs">Higher-Rank Trait Bounds (HRTBs)</a></h1>
<p>Rust's <code>Fn</code> traits are a little bit magic. For instance, we can write the
following code:</p>
<pre><pre class="playground"><code class="language-rust edition2021">struct Closure&lt;F&gt; {
    data: (u8, u16),
    func: F,
}

impl&lt;F&gt; Closure&lt;F&gt;
    where F: Fn(&amp;(u8, u16)) -&gt; &amp;u8,
{
    fn call(&amp;self) -&gt; &amp;u8 {
        (self.func)(&amp;self.data)
    }
}

fn do_it(data: &amp;(u8, u16)) -&gt; &amp;u8 { &amp;data.0 }

fn main() {
    let clo = Closure { data: (0, 1), func: do_it };
    println!("{}", clo.call());
}</code></pre></pre>
<p>If we try to naively desugar this code in the same way that we did in the
<a href="lifetimes.html">lifetimes section</a>, we run into some trouble:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">// NOTE: `&amp;'b data.0` and `'x: {` is not valid syntax!
struct Closure&lt;F&gt; {
    data: (u8, u16),
    func: F,
}

impl&lt;F&gt; Closure&lt;F&gt;
    // where F: Fn(&amp;'??? (u8, u16)) -&gt; &amp;'??? u8,
{
    fn call&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a u8 {
        (self.func)(&amp;self.data)
    }
}

fn do_it&lt;'b&gt;(data: &amp;'b (u8, u16)) -&gt; &amp;'b u8 { &amp;'b data.0 }

fn main() {
    'x: {
        let clo = Closure { data: (0, 1), func: do_it };
        println!("{}", clo.call());
    }
}</code></pre>
<p>How on earth are we supposed to express the lifetimes on <code>F</code>'s trait bound? We
need to provide some lifetime there, but the lifetime we care about can't be
named until we enter the body of <code>call</code>! Also, that isn't some fixed lifetime;
<code>call</code> works with <em>any</em> lifetime <code>&amp;self</code> happens to have at that point.</p>
<p>This job requires The Magic of Higher-Rank Trait Bounds (HRTBs). The way we
desugar this is as follows:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">where for&lt;'a&gt; F: Fn(&amp;'a (u8, u16)) -&gt; &amp;'a u8,</code></pre>
<p>Alternatively:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">where F: for&lt;'a&gt; Fn(&amp;'a (u8, u16)) -&gt; &amp;'a u8,</code></pre>
<p>(Where <code>Fn(a, b, c) -&gt; d</code> is itself just sugar for the unstable <em>real</em> <code>Fn</code>
trait)</p>
<p><code>for&lt;'a&gt;</code> can be read as "for all choices of <code>'a</code>", and basically produces an
<em>infinite list</em> of trait bounds that F must satisfy. Intense. There aren't many
places outside of the <code>Fn</code> traits where we encounter HRTBs, and even for
those we have a nice magic sugar for the common cases.</p>
<p>In summary, we can rewrite the original code more explicitly as:</p>
<pre><pre class="playground"><code class="language-rust edition2021">struct Closure&lt;F&gt; {
    data: (u8, u16),
    func: F,
}

impl&lt;F&gt; Closure&lt;F&gt;
    where for&lt;'a&gt; F: Fn(&amp;'a (u8, u16)) -&gt; &amp;'a u8,
{
    fn call(&amp;self) -&gt; &amp;u8 {
        (self.func)(&amp;self.data)
    }
}

fn do_it(data: &amp;(u8, u16)) -&gt; &amp;u8 { &amp;data.0 }

fn main() {
    let clo = Closure { data: (0, 1), func: do_it };
    println!("{}", clo.call());
}</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="subtyping-and-variance"><a class="header" href="#subtyping-and-variance">Subtyping and Variance</a></h1>
<p>Rust uses lifetimes to track the relationships between borrows and ownership.
However, a naive implementation of lifetimes would be either too restrictive,
or permit undefined behavior.</p>
<p>In order to allow flexible usage of lifetimes
while also preventing their misuse, Rust uses <strong>subtyping</strong> and <strong>variance</strong>.</p>
<p>Let's start with an example.</p>
<pre><pre class="playground"><code class="language-rust edition2021">// Note: debug expects two parameters with the *same* lifetime
fn debug&lt;'a&gt;(a: &amp;'a str, b: &amp;'a str) {
    println!("a = {a:?} b = {b:?}");
}

fn main() {
    let hello: &amp;'static str = "hello";
    {
        let world = String::from("world");
        let world = &amp;world; // 'world has a shorter lifetime than 'static
        debug(hello, world);
    }
}</code></pre></pre>
<p>In a conservative implementation of lifetimes, since <code>hello</code> and <code>world</code> have different lifetimes,
we might see the following error:</p>
<pre><code class="language-text">error[E0308]: mismatched types
 --&gt; src/main.rs:10:16
   |
10 |         debug(hello, world);
   |                      ^
   |                      |
   |                      expected `&amp;'static str`, found struct `&amp;'world str`
</code></pre>
<p>This would be rather unfortunate. In this case,
what we want is to accept any type that lives <em>at least as long</em> as <code>'world</code>.
Let's try using subtyping with our lifetimes.</p>
<h2 id="subtyping"><a class="header" href="#subtyping">Subtyping</a></h2>
<p>Subtyping is the idea that one type can be used in place of another.</p>
<p>Let's define that <code>Sub</code> is a subtype of <code>Super</code> (we'll be using the notation <code>Sub &lt;: Super</code> throughout this chapter).</p>
<p>What this is suggesting to us is that the set of <em>requirements</em> that <code>Super</code> defines
are completely satisfied by <code>Sub</code>. <code>Sub</code> may then have more requirements.</p>
<p>Now, in order to use subtyping with lifetimes, we need to define the requirement of a lifetime:</p>
<blockquote>
<p><code>'a</code> defines a region of code.</p>
</blockquote>
<p>Now that we have a defined set of requirements for lifetimes, we can define how they relate to each other:</p>
<blockquote>
<p><code>'long &lt;: 'short</code> if and only if <code>'long</code> defines a region of code that <strong>completely contains</strong> <code>'short</code>.</p>
</blockquote>
<p><code>'long</code> may define a region larger than <code>'short</code>, but that still fits our definition.</p>
<blockquote>
<p>As we will see throughout the rest of this chapter,
subtyping is a lot more complicated and subtle than this,
but this simple rule is a very good 99% intuition.
And unless you write unsafe code, the compiler will automatically handle all the corner cases for you.</p>
</blockquote>
<blockquote>
<p>But this is the Rustonomicon. We're writing unsafe code,
so we need to understand how this stuff really works, and how we can mess it up.</p>
</blockquote>
<p>Going back to our example above, we can say that <code>'static &lt;: 'world</code>.
For now, let's also accept the idea that subtypes of lifetimes can be passed through references
(more on this in <a href="subtyping.html#variance">Variance</a>),
<em>e.g.</em> <code>&amp;'static str</code> is a subtype of <code>&amp;'world str</code>, then we can "downgrade" <code>&amp;'static str</code> into a <code>&amp;'world str</code>.
With that, the example above will compile:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn debug&lt;'a&gt;(a: &amp;'a str, b: &amp;'a str) {
    println!("a = {a:?} b = {b:?}");
}

fn main() {
    let hello: &amp;'static str = "hello";
    {
        let world = String::from("world");
        let world = &amp;world; // 'world has a shorter lifetime than 'static
        debug(hello, world); // hello silently downgrades from `&amp;'static str` into `&amp;'world str`
    }
}</code></pre></pre>
<h2 id="variance"><a class="header" href="#variance">Variance</a></h2>
<p>Above, we glossed over the fact that <code>'static &lt;: 'b</code> implied that <code>&amp;'static T &lt;: &amp;'b T</code>. This uses a property known as <em>variance</em>.
It's not always as simple as this example, though. To understand that, let's try to extend this example a bit:</p>
<pre><pre class="playground"><code class="language-rust compile_fail E0597 edition2021">fn assign&lt;T&gt;(input: &amp;mut T, val: T) {
    *input = val;
}

fn main() {
    let mut hello: &amp;'static str = "hello";
    {
        let world = String::from("world");
        assign(&amp;mut hello, &amp;world);
    }
    println!("{hello}"); // use after free 😿
}</code></pre></pre>
<p>In <code>assign</code>, we are setting the <code>hello</code> reference to point to <code>world</code>.
But then <code>world</code> goes out of scope, before the later use of <code>hello</code> in the println!</p>
<p>This is a classic use-after-free bug!</p>
<p>Our first instinct might be to blame the <code>assign</code> impl, but there's really nothing wrong here.
It shouldn't be surprising that we might want to assign a <code>T</code> into a <code>T</code>.</p>
<p>The problem is that we cannot assume that <code>&amp;mut &amp;'static str</code> and <code>&amp;mut &amp;'b str</code> are compatible.
This means that <code>&amp;mut &amp;'static str</code> <strong>cannot</strong> be a <em>subtype</em> of <code>&amp;mut &amp;'b str</code>,
even if <code>'static</code> is a subtype of <code>'b</code>.</p>
<p>Variance is the concept that Rust borrows to define relationships about subtypes through their generic parameters.</p>
<blockquote>
<p>NOTE: For convenience we will define a generic type <code>F&lt;T&gt;</code> so
that we can easily talk about <code>T</code>. Hopefully this is clear in context.</p>
</blockquote>
<p>The type <code>F</code>'s <em>variance</em> is how the subtyping of its inputs affects the
subtyping of its outputs. There are three kinds of variance in Rust. Given two
types <code>Sub</code> and <code>Super</code>, where <code>Sub</code> is a subtype of <code>Super</code>:</p>
<ul>
<li><code>F</code> is <strong>covariant</strong> if <code>F&lt;Sub&gt;</code> is a subtype of <code>F&lt;Super&gt;</code> (the subtype property is passed through)</li>
<li><code>F</code> is <strong>contravariant</strong> if <code>F&lt;Super&gt;</code> is a subtype of <code>F&lt;Sub&gt;</code> (the subtype property is "inverted")</li>
<li><code>F</code> is <strong>invariant</strong> otherwise (no subtyping relationship exists)</li>
</ul>
<p>If we remember from the above examples,
it was ok for us to treat <code>&amp;'a T</code> as a subtype of <code>&amp;'b T</code> if <code>'a &lt;: 'b</code>,
therefore we can say that <code>&amp;'a T</code> is <em>covariant</em> over <code>'a</code>.</p>
<p>Also, we saw that it was not ok for us to treat <code>&amp;mut &amp;'a U</code> as a subtype of <code>&amp;mut &amp;'b U</code>,
therefore we can say that <code>&amp;mut T</code> is <em>invariant</em> over <code>T</code></p>
<p>Here is a table of some other generic types and their variances:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th style="text-align: center">'a</th><th style="text-align: center">T</th><th style="text-align: center">U</th></tr></thead><tbody>
<tr><td><code>&amp;'a T </code></td><td style="text-align: center">covariant</td><td style="text-align: center">covariant</td><td style="text-align: center"></td></tr>
<tr><td><code>&amp;'a mut T</code></td><td style="text-align: center">covariant</td><td style="text-align: center">invariant</td><td style="text-align: center"></td></tr>
<tr><td><code>Box&lt;T&gt;</code></td><td style="text-align: center"></td><td style="text-align: center">covariant</td><td style="text-align: center"></td></tr>
<tr><td><code>Vec&lt;T&gt;</code></td><td style="text-align: center"></td><td style="text-align: center">covariant</td><td style="text-align: center"></td></tr>
<tr><td><code>UnsafeCell&lt;T&gt;</code></td><td style="text-align: center"></td><td style="text-align: center">invariant</td><td style="text-align: center"></td></tr>
<tr><td><code>Cell&lt;T&gt;</code></td><td style="text-align: center"></td><td style="text-align: center">invariant</td><td style="text-align: center"></td></tr>
<tr><td><code>fn(T) -&gt; U</code></td><td style="text-align: center"></td><td style="text-align: center"><strong>contra</strong>variant</td><td style="text-align: center">covariant</td></tr>
<tr><td><code>*const T</code></td><td style="text-align: center"></td><td style="text-align: center">covariant</td><td style="text-align: center"></td></tr>
<tr><td><code>*mut T</code></td><td style="text-align: center"></td><td style="text-align: center">invariant</td><td style="text-align: center"></td></tr>
</tbody></table>
</div>
<p>Some of these can be explained simply in relation to the others:</p>
<ul>
<li><code>Vec&lt;T&gt;</code> and all other owning pointers and collections follow the same logic as <code>Box&lt;T&gt;</code></li>
<li><code>Cell&lt;T&gt;</code> and all other interior mutability types follow the same logic as <code>UnsafeCell&lt;T&gt;</code></li>
<li><code>UnsafeCell&lt;T&gt;</code> having interior mutability gives it the same variance properties as <code>&amp;mut T</code></li>
<li><code>*const T</code> follows the logic of <code>&amp;T</code></li>
<li><code>*mut T</code> follows the logic of <code>&amp;mut T</code> (or <code>UnsafeCell&lt;T&gt;</code>)</li>
</ul>
<p>For more types, see the <a href="../reference/subtyping.html#variance">"Variance" section</a> on the reference.</p>
<blockquote>
<p>NOTE: the <em>only</em> source of contravariance in the language is the arguments to
a function, which is why it really doesn't come up much in practice. Invoking
contravariance involves higher-order programming with function pointers that
take references with specific lifetimes (as opposed to the usual "any lifetime",
which gets into higher rank lifetimes, which work independently of subtyping).</p>
</blockquote>
<p>Now that we have some more formal understanding of variance,
let's go through some more examples in more detail.</p>
<pre><pre class="playground"><code class="language-rust compile_fail E0597 edition2021">fn assign&lt;T&gt;(input: &amp;mut T, val: T) {
    *input = val;
}

fn main() {
    let mut hello: &amp;'static str = "hello";
    {
        let world = String::from("world");
        assign(&amp;mut hello, &amp;world);
    }
    println!("{hello}");
}</code></pre></pre>
<p>And what do we get when we run this?</p>
<pre><code class="language-text">error[E0597]: `world` does not live long enough
  --&gt; src/main.rs:9:28
   |
6  |     let mut hello: &amp;'static str = "hello";
   |                    ------------ type annotation requires that `world` is borrowed for `'static`
...
9  |         assign(&amp;mut hello, &amp;world);
   |                            ^^^^^^ borrowed value does not live long enough
10 |     }
   |     - `world` dropped here while still borrowed
</code></pre>
<p>Good, it doesn't compile! Let's break down what's happening here in detail.</p>
<p>First let's look at the <code>assign</code> function:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn assign&lt;T&gt;(input: &amp;mut T, val: T) {
    *input = val;
}
<span class="boring">}</span></code></pre></pre>
<p>All it does is take a mutable reference and a value and overwrite the referent with it.
What's important about this function is that it creates a type equality constraint. It
clearly says in its signature the referent and the value must be the <em>exact same</em> type.</p>
<p>Meanwhile, in the caller we pass in <code>&amp;mut &amp;'static str</code> and <code>&amp;'world str</code>.</p>
<p>Because <code>&amp;mut T</code> is invariant over <code>T</code>, the compiler concludes it can't apply any subtyping
to the first argument, and so <code>T</code> must be exactly <code>&amp;'static str</code>.</p>
<p>This is counter to the <code>&amp;T</code> case:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn debug&lt;T: std::fmt::Debug&gt;(a: T, b: T) {
    println!("a = {a:?} b = {b:?}");
}
<span class="boring">}</span></code></pre></pre>
<p>where similarly <code>a</code> and <code>b</code> must have the same type <code>T</code>.
But since <code>&amp;'a T</code> <em>is</em> covariant over <code>'a</code>, we are allowed to perform subtyping.
So the compiler decides that <code>&amp;'static str</code> can become <code>&amp;'b str</code> if and only if
<code>&amp;'static str</code> is a subtype of <code>&amp;'b str</code>, which will hold if <code>'static &lt;: 'b</code>.
This is true, so the compiler is happy to continue compiling this code.</p>
<p>As it turns out, the argument for why it's ok for Box (and Vec, HashMap, etc.) to be covariant is pretty similar to the argument for why it's ok for lifetimes to be covariant: as soon as you try to stuff them in something like a mutable reference, they inherit invariance and you're prevented from doing anything bad.</p>
<p>However Box makes it easier to focus on the by-value aspect of references that we partially glossed over.</p>
<p>Unlike a lot of languages which allow values to be freely aliased at all times, Rust has a very strict rule: if you're allowed to mutate or move a value, you are guaranteed to be the only one with access to it.</p>
<p>Consider the following code:</p>
<pre><code class="language-rust ignore">let hello: Box&lt;&amp;'static str&gt; = Box::new("hello");

let mut world: Box&lt;&amp;'b str&gt;;
world = hello;</code></pre>
<p>There is no problem at all with the fact that we have forgotten that <code>hello</code> was alive for <code>'static</code>,
because as soon as we moved <code>hello</code> to a variable that only knew it was alive for <code>'b</code>,
<strong>we destroyed the only thing in the universe that remembered it lived for longer</strong>!</p>
<p>Only one thing left to explain: function pointers.</p>
<p>To see why <code>fn(T) -&gt; U</code> should be covariant over <code>U</code>, consider the following signature:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn get_str() -&gt; &amp;'a str;</code></pre>
<p>This function claims to produce a <code>str</code> bound by some liftime <code>'a</code>. As such, it is perfectly valid to
provide a function with the following signature instead:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn get_static() -&gt; &amp;'static str;</code></pre>
<p>So when the function is called, all it's expecting is a <code>&amp;str</code> which lives at least the lifetime of <code>'a</code>,
it doesn't matter if the value actually lives longer.</p>
<p>However, the same logic does not apply to <em>arguments</em>. Consider trying to satisfy:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn store_ref(&amp;'a str);</code></pre>
<p>with:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn store_static(&amp;'static str);</code></pre>
<p>The first function can accept any string reference as long as it lives at least for <code>'a</code>,
but the second cannot accept a string reference that lives for any duration less than <code>'static</code>,
which would cause a conflict.
Covariance doesn't work here. But if we flip it around, it actually <em>does</em>
work! If we need a function that can handle <code>&amp;'static str</code>, a function that can handle <em>any</em> reference lifetime
will surely work fine.</p>
<p>Let's see this in practice</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">use std::cell::RefCell;
</span>thread_local! {
    pub static StaticVecs: RefCell&lt;Vec&lt;&amp;'static str&gt;&gt; = RefCell::new(Vec::new());
}

/// saves the input given into a thread local `Vec&lt;&amp;'static str&gt;`
fn store(input: &amp;'static str) {
    StaticVecs.with_borrow_mut(|v| v.push(input));
}

/// Calls the function with it's input (must have the same lifetime!)
fn demo&lt;'a&gt;(input: &amp;'a str, f: fn(&amp;'a str)) {
    f(input);
}

fn main() {
    demo("hello", store); // "hello" is 'static. Can call `store` fine

    {
        let smuggle = String::from("smuggle");

        // `&amp;smuggle` is not static. If we were to call `store` with `&amp;smuggle`,
        // we would have pushed an invalid lifetime into the `StaticVecs`.
        // Therefore, `fn(&amp;'static str)` cannot be a subtype of `fn(&amp;'a str)`
        demo(&amp;smuggle, store);
    }

    // use after free 😿
    StaticVecs.with_borrow(|v| println!("{v:?}"));
}</code></pre></pre>
<p>And that's why function types, unlike anything else in the language, are
<strong>contra</strong>variant over their arguments.</p>
<p>Now, this is all well and good for the types the standard library provides, but
how is variance determined for types that <em>you</em> define? A struct, informally
speaking, inherits the variance of its fields. If a struct <code>MyType</code>
has a generic argument <code>A</code> that is used in a field <code>a</code>, then MyType's variance
over <code>A</code> is exactly <code>a</code>'s variance over <code>A</code>.</p>
<p>However if <code>A</code> is used in multiple fields:</p>
<ul>
<li>If all uses of <code>A</code> are covariant, then MyType is covariant over <code>A</code></li>
<li>If all uses of <code>A</code> are contravariant, then MyType is contravariant over <code>A</code></li>
<li>Otherwise, MyType is invariant over <code>A</code></li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::cell::Cell;

struct MyType&lt;'a, 'b, A: 'a, B: 'b, C, D, E, F, G, H, In, Out, Mixed&gt; {
    a: &amp;'a A,     // covariant over 'a and A
    b: &amp;'b mut B, // covariant over 'b and invariant over B

    c: *const C,  // covariant over C
    d: *mut D,    // invariant over D

    e: E,         // covariant over E
    f: Vec&lt;F&gt;,    // covariant over F
    g: Cell&lt;G&gt;,   // invariant over G

    h1: H,        // would also be covariant over H except...
    h2: Cell&lt;H&gt;,  // invariant over H, because invariance wins all conflicts

    i: fn(In) -&gt; Out,       // contravariant over In, covariant over Out

    k1: fn(Mixed) -&gt; usize, // would be contravariant over Mixed except..
    k2: Mixed,              // invariant over Mixed, because invariance wins all conflicts
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drop-check"><a class="header" href="#drop-check">Drop Check</a></h1>
<p>We have seen how lifetimes provide us some fairly simple rules for ensuring
that we never read dangling references. However up to this point we have only ever
interacted with the <em>outlives</em> relationship in an inclusive manner. That is,
when we talked about <code>'a: 'b</code>, it was ok for <code>'a</code> to live <em>exactly</em> as long as
<code>'b</code>. At first glance, this seems to be a meaningless distinction. Nothing ever
gets dropped at the same time as another, right? This is why we used the
following desugaring of <code>let</code> statements:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let x;
let y;</code></pre>
<p>desugaring to:</p>
<!-- ignore: desugared code -->
<pre><code class="language-rust ignore">{
    let x;
    {
        let y;
    }
}</code></pre>
<p>There are some more complex situations which are not possible to desugar using
scopes, but the order is still defined ‒ variables are dropped in the reverse
order of their definition, fields of structs and tuples in order of their
definition. There are some more details about order of drop in <a href="https://github.com/rust-lang/rfcs/blob/master/text/1857-stabilize-drop-order.md">RFC 1857</a>.</p>
<p>Let's do this:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let tuple = (vec![], vec![]);</code></pre>
<p>The left vector is dropped first. But does it mean the right one strictly
outlives it in the eyes of the borrow checker? The answer to this question is
<em>no</em>. The borrow checker could track fields of tuples separately, but it would
still be unable to decide what outlives what in case of vector elements, which
are dropped manually via pure-library code the borrow checker doesn't
understand.</p>
<p>So why do we care? We care because if the type system isn't careful, it could
accidentally make dangling pointers. Consider the following simple program:</p>
<pre><pre class="playground"><code class="language-rust edition2021">struct Inspector&lt;'a&gt;(&amp;'a u8);

struct World&lt;'a&gt; {
    inspector: Option&lt;Inspector&lt;'a&gt;&gt;,
    days: Box&lt;u8&gt;,
}

fn main() {
    let mut world = World {
        inspector: None,
        days: Box::new(1),
    };
    world.inspector = Some(Inspector(&amp;world.days));
}</code></pre></pre>
<p>This program is totally sound and compiles today. The fact that <code>days</code> does not
strictly outlive <code>inspector</code> doesn't matter. As long as the <code>inspector</code> is
alive, so is <code>days</code>.</p>
<p>However if we add a destructor, the program will no longer compile!</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">struct Inspector&lt;'a&gt;(&amp;'a u8);

impl&lt;'a&gt; Drop for Inspector&lt;'a&gt; {
    fn drop(&amp;mut self) {
        println!("I was only {} days from retirement!", self.0);
    }
}

struct World&lt;'a&gt; {
    inspector: Option&lt;Inspector&lt;'a&gt;&gt;,
    days: Box&lt;u8&gt;,
}

fn main() {
    let mut world = World {
        inspector: None,
        days: Box::new(1),
    };
    world.inspector = Some(Inspector(&amp;world.days));
    // Let's say `days` happens to get dropped first.
    // Then when Inspector is dropped, it will try to read free'd memory!
}</code></pre></pre>
<pre><code class="language-text">error[E0597]: `world.days` does not live long enough
  --&gt; src/main.rs:19:38
   |
19 |     world.inspector = Some(Inspector(&amp;world.days));
   |                                      ^^^^^^^^^^^ borrowed value does not live long enough
...
22 | }
   | -
   | |
   | `world.days` dropped here while still borrowed
   | borrow might be used here, when `world` is dropped and runs the destructor for type `World&lt;'_&gt;`
</code></pre>
<p>You can try changing the order of fields or use a tuple instead of the struct,
it'll still not compile.</p>
<p>Implementing <code>Drop</code> lets the <code>Inspector</code> execute some arbitrary code during its
death. This means it can potentially observe that types that are supposed to
live as long as it does actually were destroyed first.</p>
<p>Interestingly, only generic types need to worry about this. If they aren't
generic, then the only lifetimes they can harbor are <code>'static</code>, which will truly
live <em>forever</em>. This is why this problem is referred to as <em>sound generic drop</em>.
Sound generic drop is enforced by the <em>drop checker</em>. As of this writing, some
of the finer details of how the drop checker (also called dropck) validates
types is totally up in the air. However The Big Rule is the subtlety that we
have focused on this whole section:</p>
<p><strong>For a generic type to soundly implement drop, its generics arguments must
strictly outlive it.</strong></p>
<p>Obeying this rule is (usually) necessary to satisfy the borrow
checker; obeying it is sufficient but not necessary to be
sound. That is, if your type obeys this rule then it's definitely
sound to drop.</p>
<p>The reason that it is not always necessary to satisfy the above rule
is that some Drop implementations will not access borrowed data even
though their type gives them the capability for such access, or because we know
the specific drop order and the borrowed data is still fine even if the borrow
checker doesn't know that.</p>
<p>For example, this variant of the above <code>Inspector</code> example will never
access borrowed data:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">struct Inspector&lt;'a&gt;(&amp;'a u8, &amp;'static str);

impl&lt;'a&gt; Drop for Inspector&lt;'a&gt; {
    fn drop(&amp;mut self) {
        println!("Inspector(_, {}) knows when *not* to inspect.", self.1);
    }
}

struct World&lt;'a&gt; {
    inspector: Option&lt;Inspector&lt;'a&gt;&gt;,
    days: Box&lt;u8&gt;,
}

fn main() {
    let mut world = World {
        inspector: None,
        days: Box::new(1),
    };
    world.inspector = Some(Inspector(&amp;world.days, "gadget"));
    // Let's say `days` happens to get dropped first.
    // Even when Inspector is dropped, its destructor will not access the
    // borrowed `days`.
}</code></pre></pre>
<p>Likewise, this variant will also never access borrowed data:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">struct Inspector&lt;T&gt;(T, &amp;'static str);

impl&lt;T&gt; Drop for Inspector&lt;T&gt; {
    fn drop(&amp;mut self) {
        println!("Inspector(_, {}) knows when *not* to inspect.", self.1);
    }
}

struct World&lt;T&gt; {
    inspector: Option&lt;Inspector&lt;T&gt;&gt;,
    days: Box&lt;u8&gt;,
}

fn main() {
    let mut world = World {
        inspector: None,
        days: Box::new(1),
    };
    world.inspector = Some(Inspector(&amp;world.days, "gadget"));
    // Let's say `days` happens to get dropped first.
    // Even when Inspector is dropped, its destructor will not access the
    // borrowed `days`.
}</code></pre></pre>
<p>However, <em>both</em> of the above variants are rejected by the borrow
checker during the analysis of <code>fn main</code>, saying that <code>days</code> does not
live long enough.</p>
<p>The reason is that the borrow checking analysis of <code>main</code> does not
know about the internals of each <code>Inspector</code>'s <code>Drop</code> implementation. As
far as the borrow checker knows while it is analyzing <code>main</code>, the body
of an inspector's destructor might access that borrowed data.</p>
<p>Therefore, the drop checker forces all borrowed data in a value to
strictly outlive that value.</p>
<h2 id="an-escape-hatch"><a class="header" href="#an-escape-hatch">An Escape Hatch</a></h2>
<p>The precise rules that govern drop checking may be less restrictive in
the future.</p>
<p>The current analysis is deliberately conservative and trivial; it forces all
borrowed data in a value to outlive that value, which is certainly sound.</p>
<p>Future versions of the language may make the analysis more precise, to
reduce the number of cases where sound code is rejected as unsafe.
This would help address cases such as the two <code>Inspector</code>s above that
know not to inspect during destruction.</p>
<p>In the meantime, there is an unstable attribute that one can use to
assert (unsafely) that a generic type's destructor is <em>guaranteed</em> to
not access any expired data, even if its type gives it the capability
to do so.</p>
<p>That attribute is called <code>may_dangle</code> and was introduced in <a href="https://github.com/rust-lang/rfcs/blob/master/text/1327-dropck-param-eyepatch.md">RFC 1327</a>.
To deploy it on the <code>Inspector</code> from above, we would write:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(dropck_eyepatch)]

struct Inspector&lt;'a&gt;(&amp;'a u8, &amp;'static str);

unsafe impl&lt;#[may_dangle] 'a&gt; Drop for Inspector&lt;'a&gt; {
    fn drop(&amp;mut self) {
        println!("Inspector(_, {}) knows when *not* to inspect.", self.1);
    }
}

struct World&lt;'a&gt; {
    days: Box&lt;u8&gt;,
    inspector: Option&lt;Inspector&lt;'a&gt;&gt;,
}

fn main() {
    let mut world = World {
        inspector: None,
        days: Box::new(1),
    };
    world.inspector = Some(Inspector(&amp;world.days, "gadget"));
}</code></pre></pre>
<p>Use of this attribute requires the <code>Drop</code> impl to be marked <code>unsafe</code> because the
compiler is not checking the implicit assertion that no potentially expired data
(e.g. <code>self.0</code> above) is accessed.</p>
<p>The attribute can be applied to any number of lifetime and type parameters. In
the following example, we assert that we access no data behind a reference of
lifetime <code>'b</code> and that the only uses of <code>T</code> will be moves or drops, but omit
the attribute from <code>'a</code> and <code>U</code>, because we do access data with that lifetime
and that type:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span>#![feature(dropck_eyepatch)]
<span class="boring">fn main() {
</span>use std::fmt::Display;

struct Inspector&lt;'a, 'b, T, U: Display&gt;(&amp;'a u8, &amp;'b u8, T, U);

unsafe impl&lt;'a, #[may_dangle] 'b, #[may_dangle] T, U: Display&gt; Drop for Inspector&lt;'a, 'b, T, U&gt; {
    fn drop(&amp;mut self) {
        println!("Inspector({}, _, _, {})", self.0, self.3);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>It is sometimes obvious that no such access can occur, like the case above.
However, when dealing with a generic type parameter, such access can
occur indirectly. Examples of such indirect access are:</p>
<ul>
<li>invoking a callback,</li>
<li>via a trait method call.</li>
</ul>
<p>(Future changes to the language, such as impl specialization, may add
other avenues for such indirect access.)</p>
<p>Here is an example of invoking a callback:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Inspector&lt;T&gt;(T, &amp;'static str, Box&lt;for &lt;'r&gt; fn(&amp;'r T) -&gt; String&gt;);

impl&lt;T&gt; Drop for Inspector&lt;T&gt; {
    fn drop(&amp;mut self) {
        // The `self.2` call could access a borrow e.g. if `T` is `&amp;'a _`.
        println!("Inspector({}, {}) unwittingly inspects expired data.",
                 (self.2)(&amp;self.0), self.1);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Here is an example of a trait method call:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fmt;

struct Inspector&lt;T: fmt::Display&gt;(T, &amp;'static str);

impl&lt;T: fmt::Display&gt; Drop for Inspector&lt;T&gt; {
    fn drop(&amp;mut self) {
        // There is a hidden call to `&lt;T as Display&gt;::fmt` below, which
        // could access a borrow e.g. if `T` is `&amp;'a _`
        println!("Inspector({}, {}) unwittingly inspects expired data.",
                 self.0, self.1);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>And of course, all of these accesses could be further hidden within
some other method invoked by the destructor, rather than being written
directly within it.</p>
<p>In all of the above cases where the <code>&amp;'a u8</code> is accessed in the
destructor, adding the <code>#[may_dangle]</code>
attribute makes the type vulnerable to misuse that the borrow
checker will not catch, inviting havoc. It is better to avoid adding
the attribute.</p>
<h2 id="a-related-side-note-about-drop-order"><a class="header" href="#a-related-side-note-about-drop-order">A related side note about drop order</a></h2>
<p>While the drop order of fields inside a struct is defined, relying on it is
fragile and subtle. When the order matters, it is better to use the
<a href="../std/mem/struct.ManuallyDrop.html"><code>ManuallyDrop</code></a> wrapper.</p>
<h2 id="is-that-all-about-drop-checker"><a class="header" href="#is-that-all-about-drop-checker">Is that all about drop checker?</a></h2>
<p>It turns out that when writing unsafe code, we generally don't need to
worry at all about doing the right thing for the drop checker. However there
is one special case that you need to worry about, which we will look at in
the next section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="phantomdata"><a class="header" href="#phantomdata">PhantomData</a></h1>
<p>When working with unsafe code, we can often end up in a situation where
types or lifetimes are logically associated with a struct, but not actually
part of a field. This most commonly occurs with lifetimes. For instance, the
<code>Iter</code> for <code>&amp;'a [T]</code> is (approximately) defined as follows:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Iter&lt;'a, T: 'a&gt; {
    ptr: *const T,
    end: *const T,
}
<span class="boring">}</span></code></pre></pre>
<p>However because <code>'a</code> is unused within the struct's body, it's <em>unbounded</em>.
<a href="https://rust-lang.github.io/rfcs/0738-variance.html#the-corner-case-unused-parameters-and-parameters-that-are-only-used-unsafely">Because of the troubles this has historically caused</a>,
unbounded lifetimes and types are <em>forbidden</em> in struct definitions.
Therefore we must somehow refer to these types in the body.
Correctly doing this is necessary to have correct variance and drop checking.</p>
<p>We do this using <code>PhantomData</code>, which is a special marker type. <code>PhantomData</code>
consumes no space, but simulates a field of the given type for the purpose of
static analysis. This was deemed to be less error-prone than explicitly telling
the type-system the kind of variance that you want, while also providing other
useful things such as auto traits and the information needed by drop check.</p>
<p>Iter logically contains a bunch of <code>&amp;'a T</code>s, so this is exactly what we tell
the <code>PhantomData</code> to simulate:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::marker;

struct Iter&lt;'a, T: 'a&gt; {
    ptr: *const T,
    end: *const T,
    _marker: marker::PhantomData&lt;&amp;'a T&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p>and that's it. The lifetime will be bounded, and your iterator will be covariant
over <code>'a</code> and <code>T</code>. Everything Just Works.</p>
<h2 id="generic-parameters-and-drop-checking"><a class="header" href="#generic-parameters-and-drop-checking">Generic parameters and drop-checking</a></h2>
<p>In the past, there used to be another thing to take into consideration.</p>
<p>This very documentation used to say:</p>
<blockquote>
<p>Another important example is Vec, which is (approximately) defined as follows:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Vec&lt;T&gt; {
    data: *const T, // *const for variance!
    len: usize,
    cap: usize,
}
<span class="boring">}</span></code></pre></pre>
<p>Unlike the previous example, it <em>appears</em> that everything is exactly as we
want. Every generic argument to Vec shows up in at least one field.
Good to go!</p>
<p>Nope.</p>
<p>The drop checker will generously determine that <code>Vec&lt;T&gt;</code> does not own any values
of type T. This will in turn make it conclude that it doesn't need to worry
about Vec dropping any T's in its destructor for determining drop check
soundness. This will in turn allow people to create unsoundness using
Vec's destructor.</p>
<p>In order to tell the drop checker that we <em>do</em> own values of type T, and
therefore may drop some T's when <em>we</em> drop, we must add an extra <code>PhantomData</code>
saying exactly that:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::marker;

struct Vec&lt;T&gt; {
    data: *const T, // *const for variance!
    len: usize,
    cap: usize,
    _owns_T: marker::PhantomData&lt;T&gt;,
}
<span class="boring">}</span></code></pre></pre>
</blockquote>
<p>But ever since <a href="https://rust-lang.github.io/rfcs/1238-nonparametric-dropck.html">RFC 1238</a>,
<strong>this is no longer true nor necessary</strong>.</p>
<p>If you were to write:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Vec&lt;T&gt; {
    data: *const T, // `*const` for variance!
    len: usize,
    cap: usize,
}

<span class="boring">#[cfg(any())]
</span>impl&lt;T&gt; Drop for Vec&lt;T&gt; { /* … */ }
<span class="boring">}</span></code></pre></pre>
<p>then the existence of that <code>impl&lt;T&gt; Drop for Vec&lt;T&gt;</code> makes it so Rust will consider
that that <code>Vec&lt;T&gt;</code> <em>owns</em> values of type <code>T</code> (more precisely: may use values of type <code>T</code>
in its <code>Drop</code> implementation), and Rust will thus not allow them to <em>dangle</em> should a
<code>Vec&lt;T&gt;</code> be dropped.</p>
<p>When a type already has a <code>Drop impl</code>, <strong>adding an extra <code>_owns_T: PhantomData&lt;T&gt;</code> field
is thus <em>superfluous</em> and accomplishes nothing</strong>, dropck-wise (it still affects variance
and auto-traits).</p>
<ul>
<li>(advanced edge case: if the type containing the <code>PhantomData</code> has no <code>Drop</code> impl at all,
but still has drop glue (by having <em>another</em> field with drop glue), then the
dropck/<code>#[may_dangle]</code> considerations mentioned herein do apply as well: a <code>PhantomData&lt;T&gt;</code>
field will then require <code>T</code> to be droppable whenever the containing type goes out of scope).</li>
</ul>
<hr />
<p>But this situation can sometimes lead to overly restrictive code. That's why the
standard library uses an unstable and <code>unsafe</code> attribute to opt back into the old
"unchecked" drop-checking behavior, that this very documentation warned about: the
<code>#[may_dangle]</code> attribute.</p>
<h3 id="an-exception-the-special-case-of-the-standard-library-and-its-unstable-may_dangle"><a class="header" href="#an-exception-the-special-case-of-the-standard-library-and-its-unstable-may_dangle">An exception: the special case of the standard library and its unstable <code>#[may_dangle]</code></a></h3>
<p>This section can be skipped if you are only writing your own library code; but if you are
curious about what the standard library does with the actual <code>Vec</code> definition, you'll notice
that it still needs to use a <code>_owns_T: PhantomData&lt;T&gt;</code> field for soundness.</p>
<details><summary>Click here to see why</summary>
<p>Consider the following example:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn main() {
    let mut v: Vec&lt;&amp;str&gt; = Vec::new();
    let s: String = "Short-lived".into();
    v.push(&amp;s);
    drop(s);
} // &lt;- `v` is dropped here</code></pre></pre>
<p>with a classical <code>impl&lt;T&gt; Drop for Vec&lt;T&gt; {</code> definition, the above <a href="https://rust.godbolt.org/z/ans15Kqz3">is denied</a>.</p>
<p>Indeed, in this case we have a <code>Vec&lt;/* T = */ &amp;'s str&gt;</code> vector of <code>'s</code>-lived references
to <code>str</code>ings, but in the case of <code>let s: String</code>, it is dropped before the <code>Vec</code> is, and
thus <code>'s</code> <strong>is expired</strong> by the time the <code>Vec</code> is dropped, and the
<code>impl&lt;'s&gt; Drop for Vec&lt;&amp;'s str&gt; {</code> is used.</p>
<p>This means that if such <code>Drop</code> were to be used, it would be dealing with an <em>expired</em>, or
<em>dangling</em> lifetime <code>'s</code>. But this is contrary to Rust principles, where by default all
Rust references involved in a function signature are non-dangling and valid to dereference.</p>
<p>Hence why Rust has to conservatively deny this snippet.</p>
<p>And yet, in the case of the real <code>Vec</code>, the <code>Drop</code> impl does not care about <code>&amp;'s str</code>,
<em>since it has no drop glue of its own</em>: it only wants to deallocate the backing buffer.</p>
<p>In other words, it would be nice if the above snippet was somehow accepted, by special
casing <code>Vec</code>, or by relying on some special property of <code>Vec</code>: <code>Vec</code> could try to
<em>promise not to use the <code>&amp;'s str</code>s it holds when being dropped</em>.</p>
<p>This is the kind of <code>unsafe</code> promise that can be expressed with <code>#[may_dangle]</code>:</p>
<pre><code class="language-rust  ignore">unsafe impl&lt;#[may_dangle] 's&gt; Drop for Vec&lt;&amp;'s str&gt; { /* … */ }</code></pre>
<p>or, more generally:</p>
<pre><code class="language-rust  ignore">unsafe impl&lt;#[may_dangle] T&gt; Drop for Vec&lt;T&gt; { /* … */ }</code></pre>
<p>is the <code>unsafe</code> way to opt out of this conservative assumption that Rust's drop
checker makes about type parameters of a dropped instance not being allowed to dangle.</p>
<p>And when this is done, such as in the standard library, we need to be careful in the
case where <code>T</code> has drop glue of its own. In this instance, imagine replacing the
<code>&amp;'s str</code>s with a <code>struct PrintOnDrop&lt;'s&gt; /* = */ (&amp;'s str);</code> which would have a
<code>Drop</code> impl wherein the inner <code>&amp;'s str</code> would be dereferenced and printed to the screen.</p>
<p>Indeed, <code>Drop for Vec&lt;T&gt; {</code>, before deallocating the backing buffer, does have to transitively
drop each <code>T</code> item when it has drop glue; in the case of <code>PrintOnDrop&lt;'s&gt;</code>, it means that
<code>Drop for Vec&lt;PrintOnDrop&lt;'s&gt;&gt;</code> has to transitively drop the <code>PrintOnDrop&lt;'s&gt;</code>s elements before
deallocating the backing buffer.</p>
<p>So when we said that <code>'s</code> <code>#[may_dangle]</code>, it was an excessively loose statement. We'd rather want
to say: "<code>'s</code> may dangle provided it not be involved in some transitive drop glue". Or, more generally,
"<code>T</code> may dangle provided it not be involved in some transitive drop glue". This "exception to the
exception" is a pervasive situation whenever <strong>we own a <code>T</code></strong>. That's why Rust's <code>#[may_dangle]</code> is
smart enough to know of this opt-out, and will thus be disabled <em>when the generic parameter is held
in an owned fashion</em> by the fields of the struct.</p>
<p>Hence why the standard library ends up with:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">#[cfg(any())]
</span>// we pinky-swear not to use `T` when dropping a `Vec`…
unsafe impl&lt;#[may_dangle] T&gt; Drop for Vec&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            if mem::needs_drop::&lt;T&gt;() {
                /* … except here, that is, … */
                ptr::drop_in_place::&lt;[T]&gt;(/* … */);
            }
            // …
            dealloc(/* … */)
            // …
        }
    }
}

struct Vec&lt;T&gt; {
    // … except for the fact that a `Vec` owns `T` items and
    // may thus be dropping `T` items on drop!
    _owns_T: core::marker::PhantomData&lt;T&gt;,

    ptr: *const T, // `*const` for variance (but this does not express ownership of a `T` *per se*)
    len: usize,
    cap: usize,
}
<span class="boring">}</span></code></pre></pre>
</details>
<hr />
<p>Raw pointers that own an allocation is such a pervasive pattern that the
standard library made a utility for itself called <code>Unique&lt;T&gt;</code> which:</p>
<ul>
<li>wraps a <code>*const T</code> for variance</li>
<li>includes a <code>PhantomData&lt;T&gt;</code></li>
<li>auto-derives <code>Send</code>/<code>Sync</code> as if T was contained</li>
<li>marks the pointer as <code>NonZero</code> for the null-pointer optimization</li>
</ul>
<h2 id="table-of-phantomdata-patterns"><a class="header" href="#table-of-phantomdata-patterns">Table of <code>PhantomData</code> patterns</a></h2>
<p>Here’s a table of all the wonderful ways <code>PhantomData</code> could be used:</p>
<div class="table-wrapper"><table><thead><tr><th>Phantom type</th><th style="text-align: center">variance of <code>'a</code></th><th style="text-align: center">variance of <code>T</code></th><th style="text-align: center"><code>Send</code>/<code>Sync</code><br/>(or lack thereof)</th><th style="text-align: center">dangling <code>'a</code> or <code>T</code> in drop glue<br/>(<em>e.g.</em>, <code>#[may_dangle] Drop</code>)</th></tr></thead><tbody>
<tr><td><code>PhantomData&lt;T&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center">inherited</td><td style="text-align: center">disallowed ("owns <code>T</code>")</td></tr>
<tr><td><code>PhantomData&lt;&amp;'a T&gt;</code></td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center"><code>Send + Sync</code><br/>requires<br/><code>T : Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;&amp;'a mut T&gt;</code></td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center"><strong>inv</strong>ariant</td><td style="text-align: center">inherited</td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;*const T&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center"><code>!Send + !Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;*mut T&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>inv</strong>ariant</td><td style="text-align: center"><code>!Send + !Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;fn(T)&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>contra</strong>variant</td><td style="text-align: center"><code>Send + Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;fn() -&gt; T&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>cov</strong>ariant</td><td style="text-align: center"><code>Send + Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;fn(T) -&gt; T&gt;</code></td><td style="text-align: center">-</td><td style="text-align: center"><strong>inv</strong>ariant</td><td style="text-align: center"><code>Send + Sync</code></td><td style="text-align: center">allowed</td></tr>
<tr><td><code>PhantomData&lt;Cell&lt;&amp;'a ()&gt;&gt;</code></td><td style="text-align: center"><strong>inv</strong>ariant</td><td style="text-align: center">-</td><td style="text-align: center"><code>Send + !Sync</code></td><td style="text-align: center">allowed</td></tr>
</tbody></table>
</div>
<ul>
<li>Note: opting out of the <code>Unpin</code> auto-trait requires the dedicated <a href="../core/marker/struct.PhantomPinned.html"><code>PhantomPinned</code></a> type instead.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="splitting-borrows"><a class="header" href="#splitting-borrows">Splitting Borrows</a></h1>
<p>The mutual exclusion property of mutable references can be very limiting when
working with a composite structure. The borrow checker (a.k.a. borrowck)
understands some basic stuff, but will fall over pretty easily. It does
understand structs sufficiently to know that it's possible to borrow disjoint
fields of a struct simultaneously. So this works today:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Foo {
    a: i32,
    b: i32,
    c: i32,
}

let mut x = Foo {a: 0, b: 0, c: 0};
let a = &amp;mut x.a;
let b = &amp;mut x.b;
let c = &amp;x.c;
*b += 1;
let c2 = &amp;x.c;
*a += 10;
println!("{} {} {} {}", a, b, c, c2);
<span class="boring">}</span></code></pre></pre>
<p>However borrowck doesn't understand arrays or slices in any way, so this doesn't
work:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = [1, 2, 3];
let a = &amp;mut x[0];
let b = &amp;mut x[1];
println!("{} {}", a, b);
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-text">error[E0499]: cannot borrow `x[..]` as mutable more than once at a time
 --&gt; src/lib.rs:4:18
  |
3 |     let a = &amp;mut x[0];
  |                  ---- first mutable borrow occurs here
4 |     let b = &amp;mut x[1];
  |                  ^^^^ second mutable borrow occurs here
5 |     println!("{} {}", a, b);
6 | }
  | - first borrow ends here

error: aborting due to previous error
</code></pre>
<p>While it was plausible that borrowck could understand this simple case, it's
pretty clearly hopeless for borrowck to understand disjointness in general
container types like a tree, especially if distinct keys actually <em>do</em> map
to the same value.</p>
<p>In order to "teach" borrowck that what we're doing is ok, we need to drop down
to unsafe code. For instance, mutable slices expose a <code>split_at_mut</code> function
that consumes the slice and returns two mutable slices. One for everything to
the left of the index, and one for everything to the right. Intuitively we know
this is safe because the slices don't overlap, and therefore alias. However
the implementation requires some unsafety:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::slice::from_raw_parts_mut;
</span><span class="boring">struct FakeSlice&lt;T&gt;(T);
</span><span class="boring">impl&lt;T&gt; FakeSlice&lt;T&gt; {
</span><span class="boring">fn len(&amp;self) -&gt; usize { unimplemented!() }
</span><span class="boring">fn as_mut_ptr(&amp;mut self) -&gt; *mut T { unimplemented!() }
</span>pub fn split_at_mut(&amp;mut self, mid: usize) -&gt; (&amp;mut [T], &amp;mut [T]) {
    let len = self.len();
    let ptr = self.as_mut_ptr();

    unsafe {
        assert!(mid &lt;= len);

        (from_raw_parts_mut(ptr, mid),
         from_raw_parts_mut(ptr.add(mid), len - mid))
    }
}
<span class="boring">}
</span><span class="boring">}</span></code></pre></pre>
<p>This is actually a bit subtle. So as to avoid ever making two <code>&amp;mut</code>'s to the
same value, we explicitly construct brand-new slices through raw pointers.</p>
<p>However more subtle is how iterators that yield mutable references work.
The iterator trait is defined as follows:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait Iterator {
    type Item;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p>Given this definition, Self::Item has <em>no</em> connection to <code>self</code>. This means that
we can call <code>next</code> several times in a row, and hold onto all the results
<em>concurrently</em>. This is perfectly fine for by-value iterators, which have
exactly these semantics. It's also actually fine for shared references, as they
admit arbitrarily many references to the same thing (although the iterator needs
to be a separate object from the thing being shared).</p>
<p>But mutable references make this a mess. At first glance, they might seem
completely incompatible with this API, as it would produce multiple mutable
references to the same object!</p>
<p>However it actually <em>does</em> work, exactly because iterators are one-shot objects.
Everything an IterMut yields will be yielded at most once, so we don't
actually ever yield multiple mutable references to the same piece of data.</p>
<p>Perhaps surprisingly, mutable iterators don't require unsafe code to be
implemented for many types!</p>
<p>For instance here's a singly linked list:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">fn main() {}
</span>type Link&lt;T&gt; = Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;;

struct Node&lt;T&gt; {
    elem: T,
    next: Link&lt;T&gt;,
}

pub struct LinkedList&lt;T&gt; {
    head: Link&lt;T&gt;,
}

pub struct IterMut&lt;'a, T: 'a&gt;(Option&lt;&amp;'a mut Node&lt;T&gt;&gt;);

impl&lt;T&gt; LinkedList&lt;T&gt; {
    fn iter_mut(&amp;mut self) -&gt; IterMut&lt;T&gt; {
        IterMut(self.head.as_mut().map(|node| &amp;mut **node))
    }
}

impl&lt;'a, T&gt; Iterator for IterMut&lt;'a, T&gt; {
    type Item = &amp;'a mut T;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        self.0.take().map(|node| {
            self.0 = node.next.as_mut().map(|node| &amp;mut **node);
            &amp;mut node.elem
        })
    }
}</code></pre></pre>
<p>Here's a mutable slice:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">fn main() {}
</span>use std::mem;

pub struct IterMut&lt;'a, T: 'a&gt;(&amp;'a mut[T]);

impl&lt;'a, T&gt; Iterator for IterMut&lt;'a, T&gt; {
    type Item = &amp;'a mut T;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        let slice = mem::take(&amp;mut self.0);
        if slice.is_empty() { return None; }

        let (l, r) = slice.split_at_mut(1);
        self.0 = r;
        l.get_mut(0)
    }
}

impl&lt;'a, T&gt; DoubleEndedIterator for IterMut&lt;'a, T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        let slice = mem::take(&amp;mut self.0);
        if slice.is_empty() { return None; }

        let new_len = slice.len() - 1;
        let (l, r) = slice.split_at_mut(new_len);
        self.0 = l;
        r.get_mut(0)
    }
}</code></pre></pre>
<p>And here's a binary tree:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">fn main() {}
</span>use std::collections::VecDeque;

type Link&lt;T&gt; = Option&lt;Box&lt;Node&lt;T&gt;&gt;&gt;;

struct Node&lt;T&gt; {
    elem: T,
    left: Link&lt;T&gt;,
    right: Link&lt;T&gt;,
}

pub struct Tree&lt;T&gt; {
    root: Link&lt;T&gt;,
}

struct NodeIterMut&lt;'a, T: 'a&gt; {
    elem: Option&lt;&amp;'a mut T&gt;,
    left: Option&lt;&amp;'a mut Node&lt;T&gt;&gt;,
    right: Option&lt;&amp;'a mut Node&lt;T&gt;&gt;,
}

enum State&lt;'a, T: 'a&gt; {
    Elem(&amp;'a mut T),
    Node(&amp;'a mut Node&lt;T&gt;),
}

pub struct IterMut&lt;'a, T: 'a&gt;(VecDeque&lt;NodeIterMut&lt;'a, T&gt;&gt;);

impl&lt;T&gt; Tree&lt;T&gt; {
    pub fn iter_mut(&amp;mut self) -&gt; IterMut&lt;T&gt; {
        let mut deque = VecDeque::new();
        self.root.as_mut().map(|root| deque.push_front(root.iter_mut()));
        IterMut(deque)
    }
}

impl&lt;T&gt; Node&lt;T&gt; {
    pub fn iter_mut(&amp;mut self) -&gt; NodeIterMut&lt;T&gt; {
        NodeIterMut {
            elem: Some(&amp;mut self.elem),
            left: self.left.as_mut().map(|node| &amp;mut **node),
            right: self.right.as_mut().map(|node| &amp;mut **node),
        }
    }
}


impl&lt;'a, T&gt; Iterator for NodeIterMut&lt;'a, T&gt; {
    type Item = State&lt;'a, T&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        match self.left.take() {
            Some(node) =&gt; Some(State::Node(node)),
            None =&gt; match self.elem.take() {
                Some(elem) =&gt; Some(State::Elem(elem)),
                None =&gt; match self.right.take() {
                    Some(node) =&gt; Some(State::Node(node)),
                    None =&gt; None,
                }
            }
        }
    }
}

impl&lt;'a, T&gt; DoubleEndedIterator for NodeIterMut&lt;'a, T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        match self.right.take() {
            Some(node) =&gt; Some(State::Node(node)),
            None =&gt; match self.elem.take() {
                Some(elem) =&gt; Some(State::Elem(elem)),
                None =&gt; match self.left.take() {
                    Some(node) =&gt; Some(State::Node(node)),
                    None =&gt; None,
                }
            }
        }
    }
}

impl&lt;'a, T&gt; Iterator for IterMut&lt;'a, T&gt; {
    type Item = &amp;'a mut T;
    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        loop {
            match self.0.front_mut().and_then(|node_it| node_it.next()) {
                Some(State::Elem(elem)) =&gt; return Some(elem),
                Some(State::Node(node)) =&gt; self.0.push_front(node.iter_mut()),
                None =&gt; if let None = self.0.pop_front() { return None },
            }
        }
    }
}

impl&lt;'a, T&gt; DoubleEndedIterator for IterMut&lt;'a, T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        loop {
            match self.0.back_mut().and_then(|node_it| node_it.next_back()) {
                Some(State::Elem(elem)) =&gt; return Some(elem),
                Some(State::Node(node)) =&gt; self.0.push_back(node.iter_mut()),
                None =&gt; if let None = self.0.pop_back() { return None },
            }
        }
    }
}</code></pre></pre>
<p>All of these are completely safe and work on stable Rust! This ultimately
falls out of the simple struct case we saw before: Rust understands that you
can safely split a mutable reference into subfields. We can then encode
permanently consuming a reference via Options (or in the case of slices,
replacing with an empty slice).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="type-conversions"><a class="header" href="#type-conversions">Type Conversions</a></h1>
<p>At the end of the day, everything is just a pile of bits somewhere, and type
systems are just there to help us use those bits right. There are two common
problems with typing bits: needing to reinterpret those exact bits as a
different type, and needing to change the bits to have equivalent meaning for
a different type. Because Rust encourages encoding important properties in the
type system, these problems are incredibly pervasive. As such, Rust
consequently gives you several ways to solve them.</p>
<p>First we'll look at the ways that Safe Rust gives you to reinterpret values.
The most trivial way to do this is to just destructure a value into its
constituent parts and then build a new type out of them. e.g.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Foo {
    x: u32,
    y: u16,
}

struct Bar {
    a: u32,
    b: u16,
}

fn reinterpret(foo: Foo) -&gt; Bar {
    let Foo { x, y } = foo;
    Bar { a: x, b: y }
}
<span class="boring">}</span></code></pre></pre>
<p>But this is, at best, annoying. For common conversions, Rust provides
more ergonomic alternatives.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coercions"><a class="header" href="#coercions">Coercions</a></h1>
<p>Types can implicitly be coerced to change in certain contexts.
These changes are generally just <em>weakening</em> of types, largely focused around pointers and lifetimes.
They mostly exist to make Rust "just work" in more cases, and are largely harmless.</p>
<p>For an exhaustive list of all the types of coercions, see the <a href="../reference/type-coercions.html#coercion-types">Coercion types</a> section on the reference.</p>
<p>Note that we do not perform coercions when matching traits (except for receivers, see the <a href="./dot-operator.html">next page</a>).
If there is an <code>impl</code> for some type <code>U</code> and <code>T</code> coerces to <code>U</code>, that does not constitute an implementation for <code>T</code>.
For example, the following will not type check, even though it is OK to coerce <code>t</code> to <code>&amp;T</code> and there is an <code>impl</code> for <code>&amp;T</code>:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">trait Trait {}

fn foo&lt;X: Trait&gt;(t: X) {}

impl&lt;'a&gt; Trait for &amp;'a i32 {}

fn main() {
    let t: &amp;mut i32 = &amp;mut 0;
    foo(t);
}</code></pre></pre>
<p>which fails like as follows:</p>
<pre><code class="language-text">error[E0277]: the trait bound `&amp;mut i32: Trait` is not satisfied
 --&gt; src/main.rs:9:9
  |
3 | fn foo&lt;X: Trait&gt;(t: X) {}
  |           ----- required by this bound in `foo`
...
9 |     foo(t);
  |         ^ the trait `Trait` is not implemented for `&amp;mut i32`
  |
  = help: the following implementations were found:
            &lt;&amp;'a i32 as Trait&gt;
  = note: `Trait` is implemented for `&amp;i32`, but not for `&amp;mut i32`
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-dot-operator"><a class="header" href="#the-dot-operator">The Dot Operator</a></h1>
<p>The dot operator will perform a lot of magic to convert types.
It will perform auto-referencing, auto-dereferencing, and coercion until types
match.
The detailed mechanics of method lookup are defined <a href="https://rustc-dev-guide.rust-lang.org/method-lookup.html">here</a>,
but here is a brief overview that outlines the main steps.</p>
<p>Suppose we have a function <code>foo</code> that has a receiver (a <code>self</code>, <code>&amp;self</code> or
<code>&amp;mut self</code> parameter).
If we call <code>value.foo()</code>, the compiler needs to determine what type <code>Self</code> is before
it can call the correct implementation of the function.
For this example, we will say that <code>value</code> has type <code>T</code>.</p>
<p>We will use <a href="../book/ch19-03-advanced-traits.html#fully-qualified-syntax-for-disambiguation-calling-methods-with-the-same-name">fully-qualified syntax</a> to be more clear about exactly which
type we are calling a function on.</p>
<ul>
<li>First, the compiler checks if it can call <code>T::foo(value)</code> directly.
This is called a "by value" method call.</li>
<li>If it can't call this function (for example, if the function has the wrong type
or a trait isn't implemented for <code>Self</code>), then the compiler tries to add in an
automatic reference.
This means that the compiler tries <code>&lt;&amp;T&gt;::foo(value)</code> and <code>&lt;&amp;mut T&gt;::foo(value)</code>.
This is called an "autoref" method call.</li>
<li>If none of these candidates worked, it dereferences <code>T</code> and tries again.
This uses the <code>Deref</code> trait - if <code>T: Deref&lt;Target = U&gt;</code> then it tries again with
type <code>U</code> instead of <code>T</code>.
If it can't dereference <code>T</code>, it can also try <em>unsizing</em> <code>T</code>.
This just means that if <code>T</code> has a size parameter known at compile time, it "forgets"
it for the purpose of resolving methods.
For instance, this unsizing step can convert <code>[i32; 2]</code> into <code>[i32]</code> by "forgetting"
the size of the array.</li>
</ul>
<p>Here is an example of the method lookup algorithm:</p>
<pre><code class="language-rust ignore">let array: Rc&lt;Box&lt;[T; 3]&gt;&gt; = ...;
let first_entry = array[0];</code></pre>
<p>How does the compiler actually compute <code>array[0]</code> when the array is behind so
many indirections?
First, <code>array[0]</code> is really just syntax sugar for the <a href="../std/ops/trait.Index.html"><code>Index</code></a> trait -
the compiler will convert <code>array[0]</code> into <code>array.index(0)</code>.
Now, the compiler checks to see if <code>array</code> implements <code>Index</code>, so that it can call
the function.</p>
<p>Then, the compiler checks if <code>Rc&lt;Box&lt;[T; 3]&gt;&gt;</code> implements <code>Index</code>, but it
does not, and neither do <code>&amp;Rc&lt;Box&lt;[T; 3]&gt;&gt;</code> or <code>&amp;mut Rc&lt;Box&lt;[T; 3]&gt;&gt;</code>.
Since none of these worked, the compiler dereferences the <code>Rc&lt;Box&lt;[T; 3]&gt;&gt;</code> into
<code>Box&lt;[T; 3]&gt;</code> and tries again.
<code>Box&lt;[T; 3]&gt;</code>, <code>&amp;Box&lt;[T; 3]&gt;</code>, and <code>&amp;mut Box&lt;[T; 3]&gt;</code> do not implement <code>Index</code>,
so it dereferences again.
<code>[T; 3]</code> and its autorefs also do not implement <code>Index</code>.
It can't dereference <code>[T; 3]</code>, so the compiler unsizes it, giving <code>[T]</code>.
Finally, <code>[T]</code> implements <code>Index</code>, so it can now call the actual <code>index</code> function.</p>
<p>Consider the following more complicated example of the dot operator at work:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn do_stuff&lt;T: Clone&gt;(value: &amp;T) {
    let cloned = value.clone();
}
<span class="boring">}</span></code></pre></pre>
<p>What type is <code>cloned</code>?
First, the compiler checks if it can call by value.
The type of <code>value</code> is <code>&amp;T</code>, and so the <code>clone</code> function has signature
<code>fn clone(&amp;T) -&gt; T</code>.
It knows that <code>T: Clone</code>, so the compiler finds that <code>cloned: T</code>.</p>
<p>What would happen if the <code>T: Clone</code> restriction was removed? It would not be able
to call by value, since there is no implementation of <code>Clone</code> for <code>T</code>.
So the compiler tries to call by autoref.
In this case, the function has the signature <code>fn clone(&amp;&amp;T) -&gt; &amp;T</code> since
<code>Self = &amp;T</code>.
The compiler sees that <code>&amp;T: Clone</code>, and then deduces that <code>cloned: &amp;T</code>.</p>
<p>Here is another example where the autoref behavior is used to create some subtle
effects:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::Arc;
</span><span class="boring">
</span>#[derive(Clone)]
struct Container&lt;T&gt;(Arc&lt;T&gt;);

fn clone_containers&lt;T&gt;(foo: &amp;Container&lt;i32&gt;, bar: &amp;Container&lt;T&gt;) {
    let foo_cloned = foo.clone();
    let bar_cloned = bar.clone();
}
<span class="boring">}</span></code></pre></pre>
<p>What types are <code>foo_cloned</code> and <code>bar_cloned</code>?
We know that <code>Container&lt;i32&gt;: Clone</code>, so the compiler calls <code>clone</code> by value to give
<code>foo_cloned: Container&lt;i32&gt;</code>.
However, <code>bar_cloned</code> actually has type <code>&amp;Container&lt;T&gt;</code>.
Surely this doesn't make sense - we added <code>#[derive(Clone)]</code> to <code>Container</code>, so it
must implement <code>Clone</code>!
Looking closer, the code generated by the <code>derive</code> macro is (roughly):</p>
<pre><code class="language-rust ignore">impl&lt;T&gt; Clone for Container&lt;T&gt; where T: Clone {
    fn clone(&amp;self) -&gt; Self {
        Self(Arc::clone(&amp;self.0))
    }
}</code></pre>
<p>The derived <code>Clone</code> implementation is <a href="../std/clone/trait.Clone.html#derivable">only defined where <code>T: Clone</code></a>,
so there is no implementation for <code>Container&lt;T&gt;: Clone</code> for a generic <code>T</code>.
The compiler then looks to see if <code>&amp;Container&lt;T&gt;</code> implements <code>Clone</code>, which it does.
So it deduces that <code>clone</code> is called by autoref, and so <code>bar_cloned</code> has type
<code>&amp;Container&lt;T&gt;</code>.</p>
<p>We can fix this by implementing <code>Clone</code> manually without requiring <code>T: Clone</code>:</p>
<pre><code class="language-rust ignore">impl&lt;T&gt; Clone for Container&lt;T&gt; {
    fn clone(&amp;self) -&gt; Self {
        Self(Arc::clone(&amp;self.0))
    }
}</code></pre>
<p>Now, the type checker deduces that <code>bar_cloned: Container&lt;T&gt;</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="casts"><a class="header" href="#casts">Casts</a></h1>
<p>Casts are a superset of coercions: every coercion can be explicitly invoked via a cast.
However some conversions require a cast.
While coercions are pervasive and largely harmless, these "true casts" are rare and potentially dangerous.
As such, casts must be explicitly invoked using the <code>as</code> keyword: <code>expr as Type</code>.</p>
<p>You can find an exhaustive list of <a href="../reference/expressions/operator-expr.html#type-cast-expressions">all the true casts</a> and <a href="../reference/expressions/operator-expr.html#semantics">casting semantics</a> on the reference.</p>
<h2 id="safety-of-casting"><a class="header" href="#safety-of-casting">Safety of casting</a></h2>
<p>True casts generally revolve around raw pointers and the primitive numeric types.
Even though they're dangerous, these casts are infallible at runtime.
If a cast triggers some subtle corner case no indication will be given that this occurred.
The cast will simply succeed.
That said, casts must be valid at the type level, or else they will be prevented statically.
For instance, <code>7u8 as bool</code> will not compile.</p>
<p>That said, casts aren't <code>unsafe</code> because they generally can't violate memory safety <em>on their own</em>.
For instance, converting an integer to a raw pointer can very easily lead to terrible things.
However the act of creating the pointer itself is safe, because actually using a raw pointer is already marked as <code>unsafe</code>.</p>
<h2 id="some-notes-about-casting"><a class="header" href="#some-notes-about-casting">Some notes about casting</a></h2>
<h3 id="lengths-when-casting-raw-slices"><a class="header" href="#lengths-when-casting-raw-slices">Lengths when casting raw slices</a></h3>
<p>Note that lengths are not adjusted when casting raw slices; <code>*const [u16] as *const [u8]</code> creates a slice that only includes half of the original memory.</p>
<h3 id="transitivity"><a class="header" href="#transitivity">Transitivity</a></h3>
<p>Casting is not transitive, that is, even if <code>e as U1 as U2</code> is a valid expression, <code>e as U2</code> is not necessarily so.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transmutes"><a class="header" href="#transmutes">Transmutes</a></h1>
<p>Get out of our way type system! We're going to reinterpret these bits or die
trying! Even though this book is all about doing things that are unsafe, I
really can't emphasize enough that you should deeply think about finding Another Way
than the operations covered in this section. This is really, truly, the most
horribly unsafe thing you can do in Rust. The guardrails here are dental floss.</p>
<p><a href="../std/mem/fn.transmute.html"><code>mem::transmute&lt;T, U&gt;</code></a> takes a value of type <code>T</code> and reinterprets
it to have type <code>U</code>. The only restriction is that the <code>T</code> and <code>U</code> are verified
to have the same size. The ways to cause Undefined Behavior with this are mind
boggling.</p>
<ul>
<li>
<p>First and foremost, creating an instance of <em>any</em> type with an invalid state
is going to cause arbitrary chaos that can't really be predicted. Do not
transmute <code>3</code> to <code>bool</code>. Even if you never <em>do</em> anything with the <code>bool</code>. Just
don't.</p>
</li>
<li>
<p>Transmute has an overloaded return type. If you do not specify the return type
it may produce a surprising type to satisfy inference.</p>
</li>
<li>
<p>Transmuting an <code>&amp;</code> to <code>&amp;mut</code> is Undefined Behavior. While certain usages may
<em>appear</em> safe, note that the Rust optimizer is free to assume that a shared
reference won't change through its lifetime and thus such transmutation will
run afoul of those assumptions. So:</p>
<ul>
<li>Transmuting an <code>&amp;</code> to <code>&amp;mut</code> is <em>always</em> Undefined Behavior.</li>
<li>No you can't do it.</li>
<li>No you're not special.</li>
</ul>
</li>
<li>
<p>Transmuting to a reference without an explicitly provided lifetime
produces an <a href="./unbounded-lifetimes.html">unbounded lifetime</a>.</p>
</li>
<li>
<p>When transmuting between different compound types, you have to make sure they
are laid out the same way! If layouts differ, the wrong fields are going to
get filled with the wrong data, which will make you unhappy and can also be
Undefined Behavior (see above).</p>
<p>So how do you know if the layouts are the same? For <code>repr(C)</code> types and
<code>repr(transparent)</code> types, layout is precisely defined. But for your
run-of-the-mill <code>repr(Rust)</code>, it is not. Even different instances of the same
generic type can have wildly different layout. <code>Vec&lt;i32&gt;</code> and <code>Vec&lt;u32&gt;</code>
<em>might</em> have their fields in the same order, or they might not. The details of
what exactly is and is not guaranteed for data layout are still being worked
out over <a href="https://rust-lang.github.io/unsafe-code-guidelines/layout.html">at the UCG WG</a>.</p>
</li>
</ul>
<p><a href="../std/mem/fn.transmute_copy.html"><code>mem::transmute_copy&lt;T, U&gt;</code></a> somehow manages to be <em>even more</em>
wildly unsafe than this. It copies <code>size_of&lt;U&gt;</code> bytes out of an <code>&amp;T</code> and
interprets them as a <code>U</code>.  The size check that <code>mem::transmute</code> has is gone (as
it may be valid to copy out a prefix), though it is Undefined Behavior for <code>U</code>
to be larger than <code>T</code>.</p>
<p>Also of course you can get all of the functionality of these functions using raw
pointer casts or <code>union</code>s, but without any of the lints or other basic sanity
checks. Raw pointer casts and <code>union</code>s do not magically avoid the above rules.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="working-with-uninitialized-memory"><a class="header" href="#working-with-uninitialized-memory">Working With Uninitialized Memory</a></h1>
<p>All runtime-allocated memory in a Rust program begins its life as
<em>uninitialized</em>. In this state the value of the memory is an indeterminate pile
of bits that may or may not even reflect a valid state for the type that is
supposed to inhabit that location of memory. Attempting to interpret this memory
as a value of <em>any</em> type will cause Undefined Behavior. Do Not Do This.</p>
<p>Rust provides mechanisms to work with uninitialized memory in checked (safe) and
unchecked (unsafe) ways.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="checked-uninitialized-memory"><a class="header" href="#checked-uninitialized-memory">Checked Uninitialized Memory</a></h1>
<p>Like C, all stack variables in Rust are uninitialized until a value is
explicitly assigned to them. Unlike C, Rust statically prevents you from ever
reading them until you do:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">fn main() {
    let x: i32;
    println!("{}", x);
}</code></pre></pre>
<pre><code class="language-text">  |
3 |     println!("{}", x);
  |                    ^ use of possibly uninitialized `x`
</code></pre>
<p>This is based off of a basic branch analysis: every branch must assign a value
to <code>x</code> before it is first used. For short, we also say that "<code>x</code> is init" or
"<code>x</code> is uninit".</p>
<p>Interestingly, Rust doesn't require the variable
to be mutable to perform a delayed initialization if every branch assigns
exactly once. However the analysis does not take advantage of constant analysis
or anything like that. So this compiles:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn main() {
    let x: i32;

    if true {
        x = 1;
    } else {
        x = 2;
    }

    println!("{}", x);
}</code></pre></pre>
<p>but this doesn't:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021">fn main() {
    let x: i32;
    if true {
        x = 1;
    }
    println!("{}", x);
}</code></pre></pre>
<pre><code class="language-text">  |
6 |     println!("{}", x);
  |                    ^ use of possibly uninitialized `x`
</code></pre>
<p>while this does:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn main() {
    let x: i32;
    if true {
        x = 1;
        println!("{}", x);
    }
    // Don't care that there are branches where it's not initialized
    // since we don't use the value in those branches
}</code></pre></pre>
<p>Of course, while the analysis doesn't consider actual values, it does
have a relatively sophisticated understanding of dependencies and control
flow. For instance, this works:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x: i32;

loop {
    // Rust doesn't understand that this branch will be taken unconditionally,
    // because it relies on actual values.
    if true {
        // But it does understand that it will only be taken once because
        // we unconditionally break out of it. Therefore `x` doesn't
        // need to be marked as mutable.
        x = 0;
        break;
    }
}
// It also knows that it's impossible to get here without reaching the break.
// And therefore that `x` must be initialized here!
println!("{}", x);
<span class="boring">}</span></code></pre></pre>
<p>If a value is moved out of a variable, that variable becomes logically
uninitialized if the type of the value isn't Copy. That is:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn main() {
    let x = 0;
    let y = Box::new(0);
    let z1 = x; // x is still valid because i32 is Copy
    let z2 = y; // y is now logically uninitialized because Box isn't Copy
}</code></pre></pre>
<p>However reassigning <code>y</code> in this example <em>would</em> require <code>y</code> to be marked as
mutable, as a Safe Rust program could observe that the value of <code>y</code> changed:</p>
<pre><pre class="playground"><code class="language-rust edition2021">fn main() {
    let mut y = Box::new(0);
    let z = y; // y is now logically uninitialized because Box isn't Copy
    y = Box::new(1); // reinitialize y
}</code></pre></pre>
<p>Otherwise it's like <code>y</code> is a brand new variable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drop-flags"><a class="header" href="#drop-flags">Drop Flags</a></h1>
<p>The examples in the previous section introduce an interesting problem for Rust.
We have seen that it's possible to conditionally initialize, deinitialize, and
reinitialize locations of memory totally safely. For Copy types, this isn't
particularly notable since they're just a random pile of bits. However types
with destructors are a different story: Rust needs to know whether to call a
destructor whenever a variable is assigned to, or a variable goes out of scope.
How can it do this with conditional initialization?</p>
<p>Note that this is not a problem that all assignments need worry about. In
particular, assigning through a dereference unconditionally drops, and assigning
in a <code>let</code> unconditionally doesn't drop:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = Box::new(0); // let makes a fresh variable, so never need to drop
let y = &amp;mut x;
*y = Box::new(1); // Deref assumes the referent is initialized, so always drops
<span class="boring">}</span></code></pre></pre>
<p>This is only a problem when overwriting a previously initialized variable or
one of its subfields.</p>
<p>It turns out that Rust actually tracks whether a type should be dropped or not
<em>at runtime</em>. As a variable becomes initialized and uninitialized, a <em>drop flag</em>
for that variable is toggled. When a variable might need to be dropped, this
flag is evaluated to determine if it should be dropped.</p>
<p>Of course, it is often the case that a value's initialization state can be
statically known at every point in the program. If this is the case, then the
compiler can theoretically generate more efficient code! For instance, straight-
line code has such <em>static drop semantics</em>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = Box::new(0); // x was uninit; just overwrite.
let mut y = x;           // y was uninit; just overwrite and make x uninit.
x = Box::new(0);         // x was uninit; just overwrite.
y = x;                   // y was init; Drop y, overwrite it, and make x uninit!
                         // y goes out of scope; y was init; Drop y!
                         // x goes out of scope; x was uninit; do nothing.
<span class="boring">}</span></code></pre></pre>
<p>Similarly, branched code where all branches have the same behavior with respect
to initialization has static drop semantics:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">let condition = true;
</span>let mut x = Box::new(0);    // x was uninit; just overwrite.
if condition {
    drop(x)                 // x gets moved out; make x uninit.
} else {
    println!("{}", x);
    drop(x)                 // x gets moved out; make x uninit.
}
x = Box::new(0);            // x was uninit; just overwrite.
                            // x goes out of scope; x was init; Drop x!
<span class="boring">}</span></code></pre></pre>
<p>However code like this <em>requires</em> runtime information to correctly Drop:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">let condition = true;
</span>let x;
if condition {
    x = Box::new(0);        // x was uninit; just overwrite.
    println!("{}", x);
}
                            // x goes out of scope; x might be uninit;
                            // check the flag!
<span class="boring">}</span></code></pre></pre>
<p>Of course, in this case it's trivial to retrieve static drop semantics:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">let condition = true;
</span>if condition {
    let x = Box::new(0);
    println!("{}", x);
}
<span class="boring">}</span></code></pre></pre>
<p>The drop flags are tracked on the stack.
In old Rust versions, drop flags were stashed in a hidden field of types that implement <code>Drop</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unchecked-uninitialized-memory"><a class="header" href="#unchecked-uninitialized-memory">Unchecked Uninitialized Memory</a></h1>
<p>One interesting exception to this rule is working with arrays. Safe Rust doesn't
permit you to partially initialize an array. When you initialize an array, you
can either set every value to the same thing with <code>let x = [val; N]</code>, or you can
specify each member individually with <code>let x = [val1, val2, val3]</code>.
Unfortunately this is pretty rigid, especially if you need to initialize your
array in a more incremental or dynamic way.</p>
<p>Unsafe Rust gives us a powerful tool to handle this problem:
<a href="../core/mem/union.MaybeUninit.html"><code>MaybeUninit</code></a>. This type can be used to handle memory that has not been fully
initialized yet.</p>
<p>With <code>MaybeUninit</code>, we can initialize an array element by element as follows:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::mem::{self, MaybeUninit};

// Size of the array is hard-coded but easy to change (meaning, changing just
// the constant is sufficient). This means we can't use [a, b, c] syntax to
// initialize the array, though, as we would have to keep that in sync
// with `SIZE`!
const SIZE: usize = 10;

let x = {
    // Create an uninitialized array of `MaybeUninit`. The `assume_init` is
    // safe because the type we are claiming to have initialized here is a
    // bunch of `MaybeUninit`s, which do not require initialization.
    let mut x: [MaybeUninit&lt;Box&lt;u32&gt;&gt;; SIZE] = unsafe {
        MaybeUninit::uninit().assume_init()
    };

    // Dropping a `MaybeUninit` does nothing. Thus using raw pointer
    // assignment instead of `ptr::write` does not cause the old
    // uninitialized value to be dropped.
    // Exception safety is not a concern because Box can't panic
    for i in 0..SIZE {
        x[i] = MaybeUninit::new(Box::new(i as u32));
    }

    // Everything is initialized. Transmute the array to the
    // initialized type.
    unsafe { mem::transmute::&lt;_, [Box&lt;u32&gt;; SIZE]&gt;(x) }
};

dbg!(x);
<span class="boring">}</span></code></pre></pre>
<p>This code proceeds in three steps:</p>
<ol>
<li>
<p>Create an array of <code>MaybeUninit&lt;T&gt;</code>. With current stable Rust, we have to use
unsafe code for this: we take some uninitialized piece of memory
(<code>MaybeUninit::uninit()</code>) and claim we have fully initialized it
(<a href="../core/mem/union.MaybeUninit.html#method.assume_init"><code>assume_init()</code></a>). This seems ridiculous, because we didn't!
The reason this is correct is that the array consists itself entirely of
<code>MaybeUninit</code>, which do not actually require initialization. For most other
types, doing <code>MaybeUninit::uninit().assume_init()</code> produces an invalid
instance of said type, so you got yourself some Undefined Behavior.</p>
</li>
<li>
<p>Initialize the array. The subtle aspect of this is that usually, when we use
<code>=</code> to assign to a value that the Rust type checker considers to already be
initialized (like <code>x[i]</code>), the old value stored on the left-hand side gets
dropped. This would be a disaster. However, in this case, the type of the
left-hand side is <code>MaybeUninit&lt;Box&lt;u32&gt;&gt;</code>, and dropping that does not do
anything! See below for some more discussion of this <code>drop</code> issue.</p>
</li>
<li>
<p>Finally, we have to change the type of our array to remove the
<code>MaybeUninit</code>. With current stable Rust, this requires a <code>transmute</code>.
This transmute is legal because in memory, <code>MaybeUninit&lt;T&gt;</code> looks the same as <code>T</code>.</p>
<p>However, note that in general, <code>Container&lt;MaybeUninit&lt;T&gt;&gt;&gt;</code> does <em>not</em> look
the same as <code>Container&lt;T&gt;</code>! Imagine if <code>Container</code> was <code>Option</code>, and <code>T</code> was
<code>bool</code>, then <code>Option&lt;bool&gt;</code> exploits that <code>bool</code> only has two valid values,
but <code>Option&lt;MaybeUninit&lt;bool&gt;&gt;</code> cannot do that because the <code>bool</code> does not
have to be initialized.</p>
<p>So, it depends on <code>Container</code> whether transmuting away the <code>MaybeUninit</code> is
allowed. For arrays, it is (and eventually the standard library will
acknowledge that by providing appropriate methods).</p>
</li>
</ol>
<p>It's worth spending a bit more time on the loop in the middle, and in particular
the assignment operator and its interaction with <code>drop</code>. If we wrote something like:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">*x[i].as_mut_ptr() = Box::new(i as u32); // WRONG!</code></pre>
<p>we would actually overwrite a <code>Box&lt;u32&gt;</code>, leading to <code>drop</code> of uninitialized
data, which would cause much sadness and pain.</p>
<p>The correct alternative, if for some reason we cannot use <code>MaybeUninit::new</code>, is
to use the <a href="../core/ptr/index.html"><code>ptr</code></a> module. In particular, it provides three functions that allow
us to assign bytes to a location in memory without dropping the old value:
<a href="../core/ptr/fn.write.html"><code>write</code></a>, <a href="../std/ptr/fn.copy.html"><code>copy</code></a>, and <a href="../std/ptr/fn.copy_nonoverlapping.html"><code>copy_nonoverlapping</code></a>.</p>
<ul>
<li><code>ptr::write(ptr, val)</code> takes a <code>val</code> and moves it into the address pointed
to by <code>ptr</code>.</li>
<li><code>ptr::copy(src, dest, count)</code> copies the bits that <code>count</code> T items would occupy
from src to dest. (this is equivalent to C's memmove -- note that the argument
order is reversed!)</li>
<li><code>ptr::copy_nonoverlapping(src, dest, count)</code> does what <code>copy</code> does, but a
little faster on the assumption that the two ranges of memory don't overlap.
(this is equivalent to C's memcpy -- note that the argument order is reversed!)</li>
</ul>
<p>It should go without saying that these functions, if misused, will cause serious
havoc or just straight up Undefined Behavior. The only requirement of these
functions <em>themselves</em> is that the locations you want to read and write
are allocated and properly aligned. However, the ways writing arbitrary bits to
arbitrary locations of memory can break things are basically uncountable!</p>
<p>It's worth noting that you don't need to worry about <code>ptr::write</code>-style
shenanigans with types which don't implement <code>Drop</code> or contain <code>Drop</code> types,
because Rust knows not to try to drop them. This is what we relied on in the
above example.</p>
<p>However when working with uninitialized memory you need to be ever-vigilant for
Rust trying to drop values you make like this before they're fully initialized.
Every control path through that variable's scope must initialize the value
before it ends, if it has a destructor.
<em><a href="unwinding.html">This includes code panicking</a></em>. <code>MaybeUninit</code> helps a bit
here, because it does not implicitly drop its content - but all this really
means in case of a panic is that instead of a double-free of the not yet
initialized parts, you end up with a memory leak of the already initialized
parts.</p>
<p>Note that, to use the <code>ptr</code> methods, you need to first obtain a <em>raw pointer</em> to
the data you want to initialize. It is illegal to construct a <em>reference</em> to
uninitialized data, which implies that you have to be careful when obtaining
said raw pointer:</p>
<ul>
<li>For an array of <code>T</code>, you can use <code>base_ptr.add(idx)</code> where <code>base_ptr: *mut T</code>
to compute the address of array index <code>idx</code>. This relies on
how arrays are laid out in memory.</li>
<li>For a struct, however, in general we do not know how it is laid out, and we
also cannot use <code>&amp;mut base_ptr.field</code> as that would be creating a
reference. So, you must carefully use the <a href="../core/ptr/macro.addr_of_mut.html"><code>addr_of_mut</code></a> macro. This creates
a raw pointer to the field without creating an intermediate reference:</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::{ptr, mem::MaybeUninit};

struct Demo {
    field: bool,
}

let mut uninit = MaybeUninit::&lt;Demo&gt;::uninit();
// `&amp;uninit.as_mut().field` would create a reference to an uninitialized `bool`,
// and thus be Undefined Behavior!
let f1_ptr = unsafe { ptr::addr_of_mut!((*uninit.as_mut_ptr()).field) };
unsafe { f1_ptr.write(true); }

let init = unsafe { uninit.assume_init() };
<span class="boring">}</span></code></pre></pre>
<p>One last remark: when reading old Rust code, you might stumble upon the
deprecated <code>mem::uninitialized</code> function.  That function used to be the only way
to deal with uninitialized memory on the stack, but it turned out to be
impossible to properly integrate with the rest of the language.  Always use
<code>MaybeUninit</code> instead in new code, and port old code over when you get the
opportunity.</p>
<p>And that's about it for working with uninitialized memory! Basically nothing
anywhere expects to be handed uninitialized memory, so if you're going to pass
it around at all, be sure to be <em>really</em> careful.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-perils-of-ownership-based-resource-management-obrm"><a class="header" href="#the-perils-of-ownership-based-resource-management-obrm">The Perils Of Ownership Based Resource Management (OBRM)</a></h1>
<p>OBRM (AKA RAII: Resource Acquisition Is Initialization) is something you'll
interact with a lot in Rust. Especially if you use the standard library.</p>
<p>Roughly speaking the pattern is as follows: to acquire a resource, you create an
object that manages it. To release the resource, you simply destroy the object,
and it cleans up the resource for you. The most common "resource" this pattern
manages is simply <em>memory</em>. <code>Box</code>, <code>Rc</code>, and basically everything in
<code>std::collections</code> is a convenience to enable correctly managing memory. This is
particularly important in Rust because we have no pervasive GC to rely on for
memory management. Which is the point, really: Rust is about control. However we
are not limited to just memory. Pretty much every other system resource like a
thread, file, or socket is exposed through this kind of API.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="constructors"><a class="header" href="#constructors">Constructors</a></h1>
<p>There is exactly one way to create an instance of a user-defined type: name it,
and initialize all its fields at once:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Foo {
    a: u8,
    b: u32,
    c: bool,
}

enum Bar {
    X(u32),
    Y(bool),
}

struct Unit;

let foo = Foo { a: 0, b: 1, c: false };
let bar = Bar::X(0);
let empty = Unit;
<span class="boring">}</span></code></pre></pre>
<p>That's it. Every other way you make an instance of a type is just calling a
totally vanilla function that does some stuff and eventually bottoms out to The
One True Constructor.</p>
<p>Unlike C++, Rust does not come with a slew of built-in kinds of constructor.
There are no Copy, Default, Assignment, Move, or whatever constructors. The
reasons for this are varied, but it largely boils down to Rust's philosophy of
<em>being explicit</em>.</p>
<p>Move constructors are meaningless in Rust because we don't enable types to
"care" about their location in memory. Every type must be ready for it to be
blindly memcopied to somewhere else in memory. This means pure on-the-stack-but-
still-movable intrusive linked lists are simply not happening in Rust (safely).</p>
<p>Assignment and copy constructors similarly don't exist because move semantics
are the only semantics in Rust. At most <code>x = y</code> just moves the bits of y into
the x variable. Rust does provide two facilities for providing C++'s copy-
oriented semantics: <code>Copy</code> and <code>Clone</code>. Clone is our moral equivalent of a copy
constructor, but it's never implicitly invoked. You have to explicitly call
<code>clone</code> on an element you want to be cloned. Copy is a special case of Clone
where the implementation is just "copy the bits". Copy types <em>are</em> implicitly
cloned whenever they're moved, but because of the definition of Copy this just
means not treating the old copy as uninitialized -- a no-op.</p>
<p>While Rust provides a <code>Default</code> trait for specifying the moral equivalent of a
default constructor, it's incredibly rare for this trait to be used. This is
because variables <a href="uninitialized.html">aren't implicitly initialized</a>. Default is basically
only useful for generic programming. In concrete contexts, a type will provide a
static <code>new</code> method for any kind of "default" constructor. This has no relation
to <code>new</code> in other languages and has no special meaning. It's just a naming
convention.</p>
<p>TODO: talk about "placement new"?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="destructors"><a class="header" href="#destructors">Destructors</a></h1>
<p>What the language <em>does</em> provide is full-blown automatic destructors through the
<code>Drop</code> trait, which provides the following method:</p>
<!-- ignore: function header -->
<pre><code class="language-rust ignore">fn drop(&amp;mut self);</code></pre>
<p>This method gives the type time to somehow finish what it was doing.</p>
<p><strong>After <code>drop</code> is run, Rust will recursively try to drop all of the fields
of <code>self</code>.</strong></p>
<p>This is a convenience feature so that you don't have to write "destructor
boilerplate" to drop children. If a struct has no special logic for being
dropped other than dropping its children, then it means <code>Drop</code> doesn't need to
be implemented at all!</p>
<p><strong>There is no stable way to prevent this behavior in Rust 1.0.</strong></p>
<p>Note that taking <code>&amp;mut self</code> means that even if you could suppress recursive
Drop, Rust will prevent you from e.g. moving fields out of self. For most types,
this is totally fine.</p>
<p>For instance, a custom implementation of <code>Box</code> might write <code>Drop</code> like this:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(ptr_internals, allocator_api)]

use std::alloc::{Allocator, Global, GlobalAlloc, Layout};
use std::mem;
use std::ptr::{drop_in_place, NonNull, Unique};

struct Box&lt;T&gt;{ ptr: Unique&lt;T&gt; }

impl&lt;T&gt; Drop for Box&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            drop_in_place(self.ptr.as_ptr());
            let c: NonNull&lt;T&gt; = self.ptr.into();
            Global.deallocate(c.cast(), Layout::new::&lt;T&gt;())
        }
    }
}
<span class="boring">fn main() {}</span></code></pre></pre>
<p>and this works fine because when Rust goes to drop the <code>ptr</code> field it just sees
a <a href="phantom-data.html">Unique</a> that has no actual <code>Drop</code> implementation. Similarly nothing can
use-after-free the <code>ptr</code> because when drop exits, it becomes inaccessible.</p>
<p>However this wouldn't work:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(allocator_api, ptr_internals)]

use std::alloc::{Allocator, Global, GlobalAlloc, Layout};
use std::ptr::{drop_in_place, Unique, NonNull};
use std::mem;

struct Box&lt;T&gt;{ ptr: Unique&lt;T&gt; }

impl&lt;T&gt; Drop for Box&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            drop_in_place(self.ptr.as_ptr());
            let c: NonNull&lt;T&gt; = self.ptr.into();
            Global.deallocate(c.cast(), Layout::new::&lt;T&gt;());
        }
    }
}

struct SuperBox&lt;T&gt; { my_box: Box&lt;T&gt; }

impl&lt;T&gt; Drop for SuperBox&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            // Hyper-optimized: deallocate the box's contents for it
            // without `drop`ing the contents
            let c: NonNull&lt;T&gt; = self.my_box.ptr.into();
            Global.deallocate(c.cast::&lt;u8&gt;(), Layout::new::&lt;T&gt;());
        }
    }
}
<span class="boring">fn main() {}</span></code></pre></pre>
<p>After we deallocate the <code>box</code>'s ptr in SuperBox's destructor, Rust will
happily proceed to tell the box to Drop itself and everything will blow up with
use-after-frees and double-frees.</p>
<p>Note that the recursive drop behavior applies to all structs and enums
regardless of whether they implement Drop. Therefore something like</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Boxy&lt;T&gt; {
    data1: Box&lt;T&gt;,
    data2: Box&lt;T&gt;,
    info: u32,
}
<span class="boring">}</span></code></pre></pre>
<p>will have its data1 and data2's fields destructors whenever it "would" be
dropped, even though it itself doesn't implement Drop. We say that such a type
<em>needs Drop</em>, even though it is not itself Drop.</p>
<p>Similarly,</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum Link {
    Next(Box&lt;Link&gt;),
    None,
}
<span class="boring">}</span></code></pre></pre>
<p>will have its inner Box field dropped if and only if an instance stores the
Next variant.</p>
<p>In general this works really nicely because you don't need to worry about
adding/removing drops when you refactor your data layout. Still there's
certainly many valid use cases for needing to do trickier things with
destructors.</p>
<p>The classic safe solution to overriding recursive drop and allowing moving out
of Self during <code>drop</code> is to use an Option:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(allocator_api, ptr_internals)]

use std::alloc::{Allocator, GlobalAlloc, Global, Layout};
use std::ptr::{drop_in_place, Unique, NonNull};
use std::mem;

struct Box&lt;T&gt;{ ptr: Unique&lt;T&gt; }

impl&lt;T&gt; Drop for Box&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            drop_in_place(self.ptr.as_ptr());
            let c: NonNull&lt;T&gt; = self.ptr.into();
            Global.deallocate(c.cast(), Layout::new::&lt;T&gt;());
        }
    }
}

struct SuperBox&lt;T&gt; { my_box: Option&lt;Box&lt;T&gt;&gt; }

impl&lt;T&gt; Drop for SuperBox&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            // Hyper-optimized: deallocate the box's contents for it
            // without `drop`ing the contents. Need to set the `box`
            // field as `None` to prevent Rust from trying to Drop it.
            let my_box = self.my_box.take().unwrap();
            let c: NonNull&lt;T&gt; = my_box.ptr.into();
            Global.deallocate(c.cast(), Layout::new::&lt;T&gt;());
            mem::forget(my_box);
        }
    }
}
<span class="boring">fn main() {}</span></code></pre></pre>
<p>However this has fairly odd semantics: you're saying that a field that <em>should</em>
always be Some <em>may</em> be None, just because that happens in the destructor. Of
course this conversely makes a lot of sense: you can call arbitrary methods on
self during the destructor, and this should prevent you from ever doing so after
deinitializing the field. Not that it will prevent you from producing any other
arbitrarily invalid state in there.</p>
<p>On balance this is an ok choice. Certainly what you should reach for by default.
However, in the future we expect there to be a first-class way to announce that
a field shouldn't be automatically dropped.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leaking"><a class="header" href="#leaking">Leaking</a></h1>
<p>Ownership-based resource management is intended to simplify composition. You
acquire resources when you create the object, and you release the resources when
it gets destroyed. Since destruction is handled for you, it means you can't
forget to release the resources, and it happens as soon as possible! Surely this
is perfect and all of our problems are solved.</p>
<p>Everything is terrible and we have new and exotic problems to try to solve.</p>
<p>Many people like to believe that Rust eliminates resource leaks. In practice,
this is basically true. You would be surprised to see a Safe Rust program
leak resources in an uncontrolled way.</p>
<p>However from a theoretical perspective this is absolutely not the case, no
matter how you look at it. In the strictest sense, "leaking" is so abstract as
to be unpreventable. It's quite trivial to initialize a collection at the start
of a program, fill it with tons of objects with destructors, and then enter an
infinite event loop that never refers to it. The collection will sit around
uselessly, holding on to its precious resources until the program terminates (at
which point all those resources would have been reclaimed by the OS anyway).</p>
<p>We may consider a more restricted form of leak: failing to drop a value that is
unreachable. Rust also doesn't prevent this. In fact Rust <em>has a function for
doing this</em>: <code>mem::forget</code>. This function consumes the value it is passed <em>and
then doesn't run its destructor</em>.</p>
<p>In the past <code>mem::forget</code> was marked as unsafe as a sort of lint against using
it, since failing to call a destructor is generally not a well-behaved thing to
do (though useful for some special unsafe code). However this was generally
determined to be an untenable stance to take: there are many ways to fail to
call a destructor in safe code. The most famous example is creating a cycle of
reference-counted pointers using interior mutability.</p>
<p>It is reasonable for safe code to assume that destructor leaks do not happen, as
any program that leaks destructors is probably wrong. However <em>unsafe</em> code
cannot rely on destructors to be run in order to be safe. For most types this
doesn't matter: if you leak the destructor then the type is by definition
inaccessible, so it doesn't matter, right? For instance, if you leak a <code>Box&lt;u8&gt;</code>
then you waste some memory but that's hardly going to violate memory-safety.</p>
<p>However where we must be careful with destructor leaks are <em>proxy</em> types. These
are types which manage access to a distinct object, but don't actually own it.
Proxy objects are quite rare. Proxy objects you'll need to care about are even
rarer. However we'll focus on three interesting examples in the standard
library:</p>
<ul>
<li><code>vec::Drain</code></li>
<li><code>Rc</code></li>
<li><code>thread::scoped::JoinGuard</code></li>
</ul>
<h2 id="drain"><a class="header" href="#drain">Drain</a></h2>
<p><code>drain</code> is a collections API that moves data out of the container without
consuming the container. This enables us to reuse the allocation of a <code>Vec</code>
after claiming ownership over all of its contents. It produces an iterator
(Drain) that returns the contents of the Vec by-value.</p>
<p>Now, consider Drain in the middle of iteration: some values have been moved out,
and others haven't. This means that part of the Vec is now full of logically
uninitialized data! We could backshift all the elements in the Vec every time we
remove a value, but this would have pretty catastrophic performance
consequences.</p>
<p>Instead, we would like Drain to fix the Vec's backing storage when it is
dropped. It should run itself to completion, backshift any elements that weren't
removed (drain supports subranges), and then fix Vec's <code>len</code>. It's even
unwinding-safe! Easy!</p>
<p>Now consider the following:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let mut vec = vec![Box::new(0); 4];

{
    // start draining, vec can no longer be accessed
    let mut drainer = vec.drain(..);

    // pull out two elements and immediately drop them
    drainer.next();
    drainer.next();

    // get rid of drainer, but don't call its destructor
    mem::forget(drainer);
}

// Oops, vec[0] was dropped, we're reading a pointer into free'd memory!
println!("{}", vec[0]);</code></pre>
<p>This is pretty clearly Not Good. Unfortunately, we're kind of stuck between a
rock and a hard place: maintaining consistent state at every step has an
enormous cost (and would negate any benefits of the API). Failing to maintain
consistent state gives us Undefined Behavior in safe code (making the API
unsound).</p>
<p>So what can we do? Well, we can pick a trivially consistent state: set the Vec's
len to be 0 when we start the iteration, and fix it up if necessary in the
destructor. That way, if everything executes like normal we get the desired
behavior with minimal overhead. But if someone has the <em>audacity</em> to
mem::forget us in the middle of the iteration, all that does is <em>leak even more</em>
(and possibly leave the Vec in an unexpected but otherwise consistent state).
Since we've accepted that mem::forget is safe, this is definitely safe. We call
leaks causing more leaks a <em>leak amplification</em>.</p>
<h2 id="rc"><a class="header" href="#rc">Rc</a></h2>
<p>Rc is an interesting case because at first glance it doesn't appear to be a
proxy value at all. After all, it manages the data it points to, and dropping
all the Rcs for a value will drop that value. Leaking an Rc doesn't seem like it
would be particularly dangerous. It will leave the refcount permanently
incremented and prevent the data from being freed or dropped, but that seems
just like Box, right?</p>
<p>Nope.</p>
<p>Let's consider a simplified implementation of Rc:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">struct Rc&lt;T&gt; {
    ptr: *mut RcBox&lt;T&gt;,
}

struct RcBox&lt;T&gt; {
    data: T,
    ref_count: usize,
}

impl&lt;T&gt; Rc&lt;T&gt; {
    fn new(data: T) -&gt; Self {
        unsafe {
            // Wouldn't it be nice if heap::allocate worked like this?
            let ptr = heap::allocate::&lt;RcBox&lt;T&gt;&gt;();
            ptr::write(ptr, RcBox {
                data,
                ref_count: 1,
            });
            Rc { ptr }
        }
    }

    fn clone(&amp;self) -&gt; Self {
        unsafe {
            (*self.ptr).ref_count += 1;
        }
        Rc { ptr: self.ptr }
    }
}

impl&lt;T&gt; Drop for Rc&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            (*self.ptr).ref_count -= 1;
            if (*self.ptr).ref_count == 0 {
                // drop the data and then free it
                ptr::read(self.ptr);
                heap::deallocate(self.ptr);
            }
        }
    }
}</code></pre>
<p>This code contains an implicit and subtle assumption: <code>ref_count</code> can fit in a
<code>usize</code>, because there can't be more than <code>usize::MAX</code> Rcs in memory. However
this itself assumes that the <code>ref_count</code> accurately reflects the number of Rcs
in memory, which we know is false with <code>mem::forget</code>. Using <code>mem::forget</code> we can
overflow the <code>ref_count</code>, and then get it down to 0 with outstanding Rcs. Then
we can happily use-after-free the inner data. Bad Bad Not Good.</p>
<p>This can be solved by just checking the <code>ref_count</code> and doing <em>something</em>. The
standard library's stance is to just abort, because your program has become
horribly degenerate. Also <em>oh my gosh</em> it's such a ridiculous corner case.</p>
<h2 id="threadscopedjoinguard"><a class="header" href="#threadscopedjoinguard">thread::scoped::JoinGuard</a></h2>
<blockquote>
<p>Note: This API has already been removed from std, for more information
you may refer <a href="https://github.com/rust-lang/rust/issues/24292">issue #24292</a>.</p>
<p>This section remains here because we think this example is still
important, regardless of whether it is part of std or not.</p>
</blockquote>
<p>The thread::scoped API intended to allow threads to be spawned that reference
data on their parent's stack without any synchronization over that data by
ensuring the parent joins the thread before any of the shared data goes out
of scope.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub fn scoped&lt;'a, F&gt;(f: F) -&gt; JoinGuard&lt;'a&gt;
    where F: FnOnce() + Send + 'a</code></pre>
<p>Here <code>f</code> is some closure for the other thread to execute. Saying that
<code>F: Send + 'a</code> is saying that it closes over data that lives for <code>'a</code>, and it
either owns that data or the data was Sync (implying <code>&amp;data</code> is Send).</p>
<p>Because JoinGuard has a lifetime, it keeps all the data it closes over
borrowed in the parent thread. This means the JoinGuard can't outlive
the data that the other thread is working on. When the JoinGuard <em>does</em> get
dropped it blocks the parent thread, ensuring the child terminates before any
of the closed-over data goes out of scope in the parent.</p>
<p>Usage looked like:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let mut data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
{
    let mut guards = vec![];
    for x in &amp;mut data {
        // Move the mutable reference into the closure, and execute
        // it on a different thread. The closure has a lifetime bound
        // by the lifetime of the mutable reference `x` we store in it.
        // The guard that is returned is in turn assigned the lifetime
        // of the closure, so it also mutably borrows `data` as `x` did.
        // This means we cannot access `data` until the guard goes away.
        let guard = thread::scoped(move || {
            *x *= 2;
        });
        // store the thread's guard for later
        guards.push(guard);
    }
    // All guards are dropped here, forcing the threads to join
    // (this thread blocks here until the others terminate).
    // Once the threads join, the borrow expires and the data becomes
    // accessible again in this thread.
}
// data is definitely mutated here.</code></pre>
<p>In principle, this totally works! Rust's ownership system perfectly ensures it!
...except it relies on a destructor being called to be safe.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let mut data = Box::new(0);
{
    let guard = thread::scoped(|| {
        // This is at best a data race. At worst, it's also a use-after-free.
        *data += 1;
    });
    // Because the guard is forgotten, expiring the loan without blocking this
    // thread.
    mem::forget(guard);
}
// So the Box is dropped here while the scoped thread may or may not be trying
// to access it.</code></pre>
<p>Dang. Here the destructor running was pretty fundamental to the API, and it had
to be scrapped in favor of a completely different design.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unwinding"><a class="header" href="#unwinding">Unwinding</a></h1>
<p>Rust has a <em>tiered</em> error-handling scheme:</p>
<ul>
<li>If something might reasonably be absent, Option is used.</li>
<li>If something goes wrong and can reasonably be handled, Result is used.</li>
<li>If something goes wrong and cannot reasonably be handled, the thread panics.</li>
<li>If something catastrophic happens, the program aborts.</li>
</ul>
<p>Option and Result are overwhelmingly preferred in most situations, especially
since they can be promoted into a panic or abort at the API user's discretion.
Panics cause the thread to halt normal execution and unwind its stack, calling
destructors as if every function instantly returned.</p>
<p>As of 1.0, Rust is of two minds when it comes to panics. In the long-long-ago,
Rust was much more like Erlang. Like Erlang, Rust had lightweight tasks,
and tasks were intended to kill themselves with a panic when they reached an
untenable state. Unlike an exception in Java or C++, a panic could not be
caught at any time. Panics could only be caught by the owner of the task, at which
point they had to be handled or <em>that</em> task would itself panic.</p>
<p>Unwinding was important to this story because if a task's
destructors weren't called, it would cause memory and other system resources to
leak. Since tasks were expected to die during normal execution, this would make
Rust very poor for long-running systems!</p>
<p>As the Rust we know today came to be, this style of programming grew out of
fashion in the push for less-and-less abstraction. Light-weight tasks were
killed in the name of heavy-weight OS threads. Still, on stable Rust as of 1.0
panics can only be caught by the parent thread. This means catching a panic
requires spinning up an entire OS thread! This unfortunately stands in conflict
to Rust's philosophy of zero-cost abstractions.</p>
<p>There is an API called <a href="https://doc.rust-lang.org/std/panic/fn.catch_unwind.html"><code>catch_unwind</code></a> that enables catching a panic
without spawning a thread. Still, we would encourage you to only do this
sparingly. In particular, Rust's current unwinding implementation is heavily
optimized for the "doesn't unwind" case. If a program doesn't unwind, there
should be no runtime cost for the program being <em>ready</em> to unwind. As a
consequence, actually unwinding will be more expensive than in e.g. Java.
Don't build your programs to unwind under normal circumstances. Ideally, you
should only panic for programming errors or <em>extreme</em> problems.</p>
<p>Rust's unwinding strategy is not specified to be fundamentally compatible
with any other language's unwinding. As such, unwinding into Rust from another
language, or unwinding into another language from Rust is Undefined Behavior.
You must <em>absolutely</em> catch any panics at the FFI boundary! What you do at that
point is up to you, but <em>something</em> must be done. If you fail to do this,
at best, your application will crash and burn. At worst, your application <em>won't</em>
crash and burn, and will proceed with completely clobbered state.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exception-safety"><a class="header" href="#exception-safety">Exception Safety</a></h1>
<p>Although programs should use unwinding sparingly, there's a lot of code that
<em>can</em> panic. If you unwrap a None, index out of bounds, or divide by 0, your
program will panic. On debug builds, every arithmetic operation can panic
if it overflows. Unless you are very careful and tightly control what code runs,
pretty much everything can unwind, and you need to be ready for it.</p>
<p>Being ready for unwinding is often referred to as <em>exception safety</em>
in the broader programming world. In Rust, there are two levels of exception
safety that one may concern themselves with:</p>
<ul>
<li>
<p>In unsafe code, we <em>must</em> be exception safe to the point of not violating
memory safety. We'll call this <em>minimal</em> exception safety.</p>
</li>
<li>
<p>In safe code, it is <em>good</em> to be exception safe to the point of your program
doing the right thing. We'll call this <em>maximal</em> exception safety.</p>
</li>
</ul>
<p>As is the case in many places in Rust, Unsafe code must be ready to deal with
bad Safe code when it comes to unwinding. Code that transiently creates
unsound states must be careful that a panic does not cause that state to be
used. Generally this means ensuring that only non-panicking code is run while
these states exist, or making a guard that cleans up the state in the case of
a panic. This does not necessarily mean that the state a panic witnesses is a
fully coherent state. We need only guarantee that it's a <em>safe</em> state.</p>
<p>Most Unsafe code is leaf-like, and therefore fairly easy to make exception-safe.
It controls all the code that runs, and most of that code can't panic. However
it is not uncommon for Unsafe code to work with arrays of temporarily
uninitialized data while repeatedly invoking caller-provided code. Such code
needs to be careful and consider exception safety.</p>
<h2 id="vecpush_all"><a class="header" href="#vecpush_all">Vec::push_all</a></h2>
<p><code>Vec::push_all</code> is a temporary hack to get extending a Vec by a slice reliably
efficient without specialization. Here's a simple implementation:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T: Clone&gt; Vec&lt;T&gt; {
    fn push_all(&amp;mut self, to_push: &amp;[T]) {
        self.reserve(to_push.len());
        unsafe {
            // can't overflow because we just reserved this
            self.set_len(self.len() + to_push.len());

            for (i, x) in to_push.iter().enumerate() {
                self.ptr().add(i).write(x.clone());
            }
        }
    }
}</code></pre>
<p>We bypass <code>push</code> in order to avoid redundant capacity and <code>len</code> checks on the
Vec that we definitely know has capacity. The logic is totally correct, except
there's a subtle problem with our code: it's not exception-safe! <code>set_len</code>,
<code>add</code>, and <code>write</code> are all fine; <code>clone</code> is the panic bomb we over-looked.</p>
<p>Clone is completely out of our control, and is totally free to panic. If it
does, our function will exit early with the length of the Vec set too large. If
the Vec is looked at or dropped, uninitialized memory will be read!</p>
<p>The fix in this case is fairly simple. If we want to guarantee that the values
we <em>did</em> clone are dropped, we can set the <code>len</code> every loop iteration. If we
just want to guarantee that uninitialized memory can't be observed, we can set
the <code>len</code> after the loop.</p>
<h2 id="binaryheapsift_up"><a class="header" href="#binaryheapsift_up">BinaryHeap::sift_up</a></h2>
<p>Bubbling an element up a heap is a bit more complicated than extending a Vec.
The pseudocode is as follows:</p>
<pre><code class="language-text">bubble_up(heap, index):
    while index != 0 &amp;&amp; heap[index] &lt; heap[parent(index)]:
        heap.swap(index, parent(index))
        index = parent(index)
</code></pre>
<p>A literal transcription of this code to Rust is totally fine, but has an annoying
performance characteristic: the <code>self</code> element is swapped over and over again
uselessly. We would rather have the following:</p>
<pre><code class="language-text">bubble_up(heap, index):
    let elem = heap[index]
    while index != 0 &amp;&amp; elem &lt; heap[parent(index)]:
        heap[index] = heap[parent(index)]
        index = parent(index)
    heap[index] = elem
</code></pre>
<p>This code ensures that each element is copied as little as possible (it is in
fact necessary that elem be copied twice in general). However it now exposes
some exception safety trouble! At all times, there exists two copies of one
value. If we panic in this function something will be double-dropped.
Unfortunately, we also don't have full control of the code: that comparison is
user-defined!</p>
<p>Unlike Vec, the fix isn't as easy here. One option is to break the user-defined
code and the unsafe code into two separate phases:</p>
<pre><code class="language-text">bubble_up(heap, index):
    let end_index = index;
    while end_index != 0 &amp;&amp; heap[end_index] &lt; heap[parent(end_index)]:
        end_index = parent(end_index)

    let elem = heap[index]
    while index != end_index:
        heap[index] = heap[parent(index)]
        index = parent(index)
    heap[index] = elem
</code></pre>
<p>If the user-defined code blows up, that's no problem anymore, because we haven't
actually touched the state of the heap yet. Once we do start messing with the
heap, we're working with only data and functions that we trust, so there's no
concern of panics.</p>
<p>Perhaps you're not happy with this design. Surely it's cheating! And we have
to do the complex heap traversal <em>twice</em>! Alright, let's bite the bullet. Let's
intermix untrusted and unsafe code <em>for reals</em>.</p>
<p>If Rust had <code>try</code> and <code>finally</code> like in Java, we could do the following:</p>
<pre><code class="language-text">bubble_up(heap, index):
    let elem = heap[index]
    try:
        while index != 0 &amp;&amp; elem &lt; heap[parent(index)]:
            heap[index] = heap[parent(index)]
            index = parent(index)
    finally:
        heap[index] = elem
</code></pre>
<p>The basic idea is simple: if the comparison panics, we just toss the loose
element in the logically uninitialized index and bail out. Anyone who observes
the heap will see a potentially <em>inconsistent</em> heap, but at least it won't
cause any double-drops! If the algorithm terminates normally, then this
operation happens to coincide precisely with how we finish up regardless.</p>
<p>Sadly, Rust has no such construct, so we're going to need to roll our own! The
way to do this is to store the algorithm's state in a separate struct with a
destructor for the "finally" logic. Whether we panic or not, that destructor
will run and clean up after us.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">struct Hole&lt;'a, T: 'a&gt; {
    data: &amp;'a mut [T],
    /// `elt` is always `Some` from new until drop.
    elt: Option&lt;T&gt;,
    pos: usize,
}

impl&lt;'a, T&gt; Hole&lt;'a, T&gt; {
    fn new(data: &amp;'a mut [T], pos: usize) -&gt; Self {
        unsafe {
            let elt = ptr::read(&amp;data[pos]);
            Hole {
                data,
                elt: Some(elt),
                pos,
            }
        }
    }

    fn pos(&amp;self) -&gt; usize { self.pos }

    fn removed(&amp;self) -&gt; &amp;T { self.elt.as_ref().unwrap() }

    fn get(&amp;self, index: usize) -&gt; &amp;T { &amp;self.data[index] }

    unsafe fn move_to(&amp;mut self, index: usize) {
        let index_ptr: *const _ = &amp;self.data[index];
        let hole_ptr = &amp;mut self.data[self.pos];
        ptr::copy_nonoverlapping(index_ptr, hole_ptr, 1);
        self.pos = index;
    }
}

impl&lt;'a, T&gt; Drop for Hole&lt;'a, T&gt; {
    fn drop(&amp;mut self) {
        // fill the hole again
        unsafe {
            let pos = self.pos;
            ptr::write(&amp;mut self.data[pos], self.elt.take().unwrap());
        }
    }
}

impl&lt;T: Ord&gt; BinaryHeap&lt;T&gt; {
    fn sift_up(&amp;mut self, pos: usize) {
        unsafe {
            // Take out the value at `pos` and create a hole.
            let mut hole = Hole::new(&amp;mut self.data, pos);

            while hole.pos() != 0 {
                let parent = parent(hole.pos());
                if hole.removed() &lt;= hole.get(parent) { break }
                hole.move_to(parent);
            }
            // Hole will be unconditionally filled here; panic or not!
        }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="poisoning"><a class="header" href="#poisoning">Poisoning</a></h1>
<p>Although all unsafe code <em>must</em> ensure it has minimal exception safety, not all
types ensure <em>maximal</em> exception safety. Even if the type does, your code may
ascribe additional meaning to it. For instance, an integer is certainly
exception-safe, but has no semantics on its own. It's possible that code that
panics could fail to correctly update the integer, producing an inconsistent
program state.</p>
<p>This is <em>usually</em> fine, because anything that witnesses an exception is about
to get destroyed. For instance, if you send a Vec to another thread and that
thread panics, it doesn't matter if the Vec is in a weird state. It will be
dropped and go away forever. However some types are especially good at smuggling
values across the panic boundary.</p>
<p>These types may choose to explicitly <em>poison</em> themselves if they witness a panic.
Poisoning doesn't entail anything in particular. Generally it just means
preventing normal usage from proceeding. The most notable example of this is the
standard library's Mutex type. A Mutex will poison itself if one of its
MutexGuards (the thing it returns when a lock is obtained) is dropped during a
panic. Any future attempts to lock the Mutex will return an <code>Err</code> or panic.</p>
<p>Mutex poisons not for true safety in the sense that Rust normally cares about. It
poisons as a safety-guard against blindly using the data that comes out of a Mutex
that has witnessed a panic while locked. The data in such a Mutex was likely in the
middle of being modified, and as such may be in an inconsistent or incomplete state.
It is important to note that one cannot violate memory safety with such a type
if it is correctly written. After all, it must be minimally exception-safe!</p>
<p>However if the Mutex contained, say, a BinaryHeap that does not actually have the
heap property, it's unlikely that any code that uses it will do
what the author intended. As such, the program should not proceed normally.
Still, if you're double-plus-sure that you can do <em>something</em> with the value,
the Mutex exposes a method to get the lock anyway. It <em>is</em> safe, after all.
Just maybe nonsense.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrency-and-parallelism"><a class="header" href="#concurrency-and-parallelism">Concurrency and Parallelism</a></h1>
<p>Rust as a language doesn't <em>really</em> have an opinion on how to do concurrency or
parallelism. The standard library exposes OS threads and blocking sys-calls
because everyone has those, and they're uniform enough that you can provide
an abstraction over them in a relatively uncontroversial way. Message passing,
green threads, and async APIs are all diverse enough that any abstraction over
them tends to involve trade-offs that we weren't willing to commit to for 1.0.</p>
<p>However the way Rust models concurrency makes it relatively easy to design your own
concurrency paradigm as a library and have everyone else's code Just Work
with yours. Just require the right lifetimes and Send and Sync where appropriate
and you're off to the races. Or rather, off to the... not... having... races.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-races-and-race-conditions"><a class="header" href="#data-races-and-race-conditions">Data Races and Race Conditions</a></h1>
<p>Safe Rust guarantees an absence of data races, which are defined as:</p>
<ul>
<li>two or more threads concurrently accessing a location of memory</li>
<li>one or more of them is a write</li>
<li>one or more of them is unsynchronized</li>
</ul>
<p>A data race has Undefined Behavior, and is therefore impossible to perform in
Safe Rust. Data races are <em>mostly</em> prevented through Rust's ownership system:
it's impossible to alias a mutable reference, so it's impossible to perform a
data race. Interior mutability makes this more complicated, which is largely why
we have the Send and Sync traits (see the next section for more on this).</p>
<p><strong>However Rust does not prevent general race conditions.</strong></p>
<p>This is mathematically impossible in situations where you do not control the
scheduler, which is true for the normal OS environment. If you do control
preemption, it <em>can be</em> possible to prevent general races - this technique is
used by frameworks such as <a href="https://github.com/rtic-rs/rtic">RTIC</a>. However,
actually having control over scheduling is a very uncommon case.</p>
<p>For this reason, it is considered "safe" for Rust to get deadlocked or do
something nonsensical with incorrect synchronization: this is known as a general
race condition or resource race. Obviously such a program isn't very good, but
Rust of course cannot prevent all logic errors.</p>
<p>In any case, a race condition cannot violate memory safety in a Rust program on
its own. Only in conjunction with some other unsafe code can a race condition
actually violate memory safety. For instance, a correct program looks like this:</p>
<pre><pre class="playground"><code class="language-rust no_run edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::thread;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;

let data = vec![1, 2, 3, 4];
// Arc so that the memory the AtomicUsize is stored in still exists for
// the other thread to increment, even if we completely finish executing
// before it. Rust won't compile the program without it, because of the
// lifetime requirements of thread::spawn!
let idx = Arc::new(AtomicUsize::new(0));
let other_idx = idx.clone();

// `move` captures other_idx by-value, moving it into this thread
thread::spawn(move || {
    // It's ok to mutate idx because this value
    // is an atomic, so it can't cause a Data Race.
    other_idx.fetch_add(10, Ordering::SeqCst);
});

// Index with the value loaded from the atomic. This is safe because we
// read the atomic memory only once, and then pass a copy of that value
// to the Vec's indexing implementation. This indexing will be correctly
// bounds checked, and there's no chance of the value getting changed
// in the middle. However our program may panic if the thread we spawned
// managed to increment before this ran. A race condition because correct
// program execution (panicking is rarely correct) depends on order of
// thread execution.
println!("{}", data[idx.load(Ordering::SeqCst)]);
<span class="boring">}</span></code></pre></pre>
<p>We can cause a data race if we instead do the bound check in advance, and then
unsafely access the data with an unchecked value:</p>
<pre><pre class="playground"><code class="language-rust no_run edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::thread;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;

let data = vec![1, 2, 3, 4];

let idx = Arc::new(AtomicUsize::new(0));
let other_idx = idx.clone();

// `move` captures other_idx by-value, moving it into this thread
thread::spawn(move || {
    // It's ok to mutate idx because this value
    // is an atomic, so it can't cause a Data Race.
    other_idx.fetch_add(10, Ordering::SeqCst);
});

if idx.load(Ordering::SeqCst) &lt; data.len() {
    unsafe {
        // Incorrectly loading the idx after we did the bounds check.
        // It could have changed. This is a race condition, *and dangerous*
        // because we decided to do `get_unchecked`, which is `unsafe`.
        println!("{}", data.get_unchecked(idx.load(Ordering::SeqCst)));
    }
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="send-and-sync"><a class="header" href="#send-and-sync">Send and Sync</a></h1>
<p>Not everything obeys inherited mutability, though. Some types allow you to
have multiple aliases of a location in memory while mutating it. Unless these types use
synchronization to manage this access, they are absolutely not thread-safe. Rust
captures this through the <code>Send</code> and <code>Sync</code> traits.</p>
<ul>
<li>A type is Send if it is safe to send it to another thread.</li>
<li>A type is Sync if it is safe to share between threads (T is Sync if and only if <code>&amp;T</code> is Send).</li>
</ul>
<p>Send and Sync are fundamental to Rust's concurrency story. As such, a
substantial amount of special tooling exists to make them work right. First and
foremost, they're <a href="safe-unsafe-meaning.html">unsafe traits</a>. This means that they are unsafe to
implement, and other unsafe code can assume that they are correctly
implemented. Since they're <em>marker traits</em> (they have no associated items like
methods), correctly implemented simply means that they have the intrinsic
properties an implementor should have. Incorrectly implementing Send or Sync can
cause Undefined Behavior.</p>
<p>Send and Sync are also automatically derived traits. This means that, unlike
every other trait, if a type is composed entirely of Send or Sync types, then it
is Send or Sync. Almost all primitives are Send and Sync, and as a consequence
pretty much all types you'll ever interact with are Send and Sync.</p>
<p>Major exceptions include:</p>
<ul>
<li>raw pointers are neither Send nor Sync (because they have no safety guards).</li>
<li><code>UnsafeCell</code> isn't Sync (and therefore <code>Cell</code> and <code>RefCell</code> aren't).</li>
<li><code>Rc</code> isn't Send or Sync (because the refcount is shared and unsynchronized).</li>
</ul>
<p><code>Rc</code> and <code>UnsafeCell</code> are very fundamentally not thread-safe: they enable
unsynchronized shared mutable state. However raw pointers are, strictly
speaking, marked as thread-unsafe as more of a <em>lint</em>. Doing anything useful
with a raw pointer requires dereferencing it, which is already unsafe. In that
sense, one could argue that it would be "fine" for them to be marked as thread
safe.</p>
<p>However it's important that they aren't thread-safe to prevent types that
contain them from being automatically marked as thread-safe. These types have
non-trivial untracked ownership, and it's unlikely that their author was
necessarily thinking hard about thread safety. In the case of <code>Rc</code>, we have a nice
example of a type that contains a <code>*mut</code> that is definitely not thread-safe.</p>
<p>Types that aren't automatically derived can simply implement them if desired:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct MyBox(*mut u8);

unsafe impl Send for MyBox {}
unsafe impl Sync for MyBox {}
<span class="boring">}</span></code></pre></pre>
<p>In the <em>incredibly rare</em> case that a type is inappropriately automatically
derived to be Send or Sync, then one can also unimplement Send and Sync:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span>#![feature(negative_impls)]

<span class="boring">fn main() {
</span>// I have some magic semantics for some synchronization primitive!
struct SpecialThreadToken(u8);

impl !Send for SpecialThreadToken {}
impl !Sync for SpecialThreadToken {}
<span class="boring">}</span></code></pre></pre>
<p>Note that <em>in and of itself</em> it is impossible to incorrectly derive Send and
Sync. Only types that are ascribed special meaning by other unsafe code can
possibly cause trouble by being incorrectly Send or Sync.</p>
<p>Most uses of raw pointers should be encapsulated behind a sufficient abstraction
that Send and Sync can be derived. For instance all of Rust's standard
collections are Send and Sync (when they contain Send and Sync types) in spite
of their pervasive use of raw pointers to manage allocations and complex ownership.
Similarly, most iterators into these collections are Send and Sync because they
largely behave like an <code>&amp;</code> or <code>&amp;mut</code> into the collection.</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p><a href="https://doc.rust-lang.org/std/boxed/struct.Box.html"><code>Box</code></a> is implemented as its own special intrinsic type by the
compiler for <a href="https://manishearth.github.io/blog/2017/01/10/rust-tidbits-box-is-special/">various reasons</a>, but we can implement something
with similar-ish behavior ourselves to see an example of when it is sound to
implement Send and Sync. Let's call it a <code>Carton</code>.</p>
<p>We start by writing code to take a value allocated on the stack and transfer it
to the heap.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">pub mod libc {
</span><span class="boring">   pub use ::std::os::raw::{c_int, c_void};
</span><span class="boring">   #[allow(non_camel_case_types)]
</span><span class="boring">   pub type size_t = usize;
</span><span class="boring">   extern "C" { pub fn posix_memalign(memptr: *mut *mut c_void, align: size_t, size: size_t) -&gt; c_int; }
</span><span class="boring">}
</span>use std::{
    mem::{align_of, size_of},
    ptr,
    cmp::max,
};

struct Carton&lt;T&gt;(ptr::NonNull&lt;T&gt;);

impl&lt;T&gt; Carton&lt;T&gt; {
    pub fn new(value: T) -&gt; Self {
        // Allocate enough memory on the heap to store one T.
        assert_ne!(size_of::&lt;T&gt;(), 0, "Zero-sized types are out of the scope of this example");
        let mut memptr: *mut T = ptr::null_mut();
        unsafe {
            let ret = libc::posix_memalign(
                (&amp;mut memptr as *mut *mut T).cast(),
                max(align_of::&lt;T&gt;(), size_of::&lt;usize&gt;()),
                size_of::&lt;T&gt;()
            );
            assert_eq!(ret, 0, "Failed to allocate or invalid alignment");
        };

        // NonNull is just a wrapper that enforces that the pointer isn't null.
        let ptr = {
            // Safety: memptr is dereferenceable because we created it from a
            // reference and have exclusive access.
            ptr::NonNull::new(memptr)
                .expect("Guaranteed non-null if posix_memalign returns 0")
        };

        // Move value from the stack to the location we allocated on the heap.
        unsafe {
            // Safety: If non-null, posix_memalign gives us a ptr that is valid
            // for writes and properly aligned.
            ptr.as_ptr().write(value);
        }

        Self(ptr)
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This isn't very useful, because once our users give us a value they have no way
to access it. <a href="https://doc.rust-lang.org/std/boxed/struct.Box.html"><code>Box</code></a> implements <a href="https://doc.rust-lang.org/core/ops/trait.Deref.html"><code>Deref</code></a> and
<a href="https://doc.rust-lang.org/core/ops/trait.DerefMut.html"><code>DerefMut</code></a> so that you can access the inner value. Let's do
that.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::ops::{Deref, DerefMut};

<span class="boring">struct Carton&lt;T&gt;(std::ptr::NonNull&lt;T&gt;);
</span><span class="boring">
</span>impl&lt;T&gt; Deref for Carton&lt;T&gt; {
    type Target = T;

    fn deref(&amp;self) -&gt; &amp;Self::Target {
        unsafe {
            // Safety: The pointer is aligned, initialized, and dereferenceable
            //   by the logic in [`Self::new`]. We require readers to borrow the
            //   Carton, and the lifetime of the return value is elided to the
            //   lifetime of the input. This means the borrow checker will
            //   enforce that no one can mutate the contents of the Carton until
            //   the reference returned is dropped.
            self.0.as_ref()
        }
    }
}

impl&lt;T&gt; DerefMut for Carton&lt;T&gt; {
    fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target {
        unsafe {
            // Safety: The pointer is aligned, initialized, and dereferenceable
            //   by the logic in [`Self::new`]. We require writers to mutably
            //   borrow the Carton, and the lifetime of the return value is
            //   elided to the lifetime of the input. This means the borrow
            //   checker will enforce that no one else can access the contents
            //   of the Carton until the mutable reference returned is dropped.
            self.0.as_mut()
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Finally, let's think about whether our <code>Carton</code> is Send and Sync. Something can
safely be Send unless it shares mutable state with something else without
enforcing exclusive access to it. Each <code>Carton</code> has a unique pointer, so
we're good.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct Carton&lt;T&gt;(std::ptr::NonNull&lt;T&gt;);
</span>// Safety: No one besides us has the raw pointer, so we can safely transfer the
// Carton to another thread if T can be safely transferred.
unsafe impl&lt;T&gt; Send for Carton&lt;T&gt; where T: Send {}
<span class="boring">}</span></code></pre></pre>
<p>What about Sync? For <code>Carton</code> to be Sync we have to enforce that you can't
write to something stored in a <code>&amp;Carton</code> while that same something could be read
or written to from another <code>&amp;Carton</code>. Since you need an <code>&amp;mut Carton</code> to
write to the pointer, and the borrow checker enforces that mutable
references must be exclusive, there are no soundness issues making <code>Carton</code>
sync either.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct Carton&lt;T&gt;(std::ptr::NonNull&lt;T&gt;);
</span>// Safety: Since there exists a public way to go from a `&amp;Carton&lt;T&gt;` to a `&amp;T`
// in an unsynchronized fashion (such as `Deref`), then `Carton&lt;T&gt;` can't be
// `Sync` if `T` isn't.
// Conversely, `Carton` itself does not use any interior mutability whatsoever:
// all the mutations are performed through an exclusive reference (`&amp;mut`). This
// means it suffices that `T` be `Sync` for `Carton&lt;T&gt;` to be `Sync`:
unsafe impl&lt;T&gt; Sync for Carton&lt;T&gt; where T: Sync  {}
<span class="boring">}</span></code></pre></pre>
<p>When we assert our type is Send and Sync we usually need to enforce that every
contained type is Send and Sync. When writing custom types that behave like
standard library types we can assert that we have the same requirements.
For example, the following code asserts that a Carton is Send if the same
sort of Box would be Send, which in this case is the same as saying T is Send.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct Carton&lt;T&gt;(std::ptr::NonNull&lt;T&gt;);
</span>unsafe impl&lt;T&gt; Send for Carton&lt;T&gt; where Box&lt;T&gt;: Send {}
<span class="boring">}</span></code></pre></pre>
<p>Right now <code>Carton&lt;T&gt;</code> has a memory leak, as it never frees the memory it allocates.
Once we fix that we have a new requirement we have to ensure we meet to be Send:
we need to know <code>free</code> can be called on a pointer that was yielded by an
allocation done on another thread. We can check this is true in the docs for
<a href="https://linux.die.net/man/3/free"><code>libc::free</code></a>.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct Carton&lt;T&gt;(std::ptr::NonNull&lt;T&gt;);
</span><span class="boring">mod libc {
</span><span class="boring">    pub use ::std::os::raw::c_void;
</span><span class="boring">    extern "C" { pub fn free(p: *mut c_void); }
</span><span class="boring">}
</span>impl&lt;T&gt; Drop for Carton&lt;T&gt; {
    fn drop(&amp;mut self) {
        unsafe {
            libc::free(self.0.as_ptr().cast());
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>A nice example where this does not happen is with a MutexGuard: notice how
<a href="https://doc.rust-lang.org/std/sync/struct.MutexGuard.html#impl-Send">it is not Send</a>. The implementation of MutexGuard
<a href="https://github.com/rust-lang/rust/issues/23465#issuecomment-82730326">uses libraries</a> that require you to ensure you
don't try to free a lock that you acquired in a different thread. If you were
able to Send a MutexGuard to another thread the destructor would run in the
thread you sent it to, violating the requirement. MutexGuard can still be Sync
because all you can send to another thread is an <code>&amp;MutexGuard</code> and dropping a
reference does nothing.</p>
<p>TODO: better explain what can or can't be Send or Sync. Sufficient to appeal
only to data races?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="atomics"><a class="header" href="#atomics">Atomics</a></h1>
<p>Rust pretty blatantly just inherits the memory model for atomics from C++20. This is not
due to this model being particularly excellent or easy to understand. Indeed,
this model is quite complex and known to have <a href="http://plv.mpi-sws.org/c11comp/popl15.pdf">several flaws</a>.
Rather, it is a pragmatic concession to the fact that <em>everyone</em> is pretty bad
at modeling atomics. At very least, we can benefit from existing tooling and
research around the C/C++ memory model.
(You'll often see this model referred to as "C/C++11" or just "C11". C just copies
the C++ memory model; and C++11 was the first version of the model but it has
received some bugfixes since then.)</p>
<p>Trying to fully explain the model in this book is fairly hopeless. It's defined
in terms of madness-inducing causality graphs that require a full book to
properly understand in a practical way. If you want all the nitty-gritty
details, you should check out the <a href="https://en.cppreference.com/w/cpp/atomic/memory_order">C++ specification</a> —
note that Rust atomics correspond to C++’s <code>atomic_ref</code>, since Rust allows
accessing atomics via non-atomic operations when it is safe to do so.
In this section we aim to give an informal overview of the topic to cover the
common problems that Rust developers face.</p>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>The C++ memory model is very large and confusing with lots of seemingly
arbitrary design decisions. To understand the motivation behind this, it can
help to look at what got us in this situation in the first place. There are
three main factors at play here:</p>
<ol>
<li>Users of the language, who want fast, cross-platform code;</li>
<li>compilers, who want to optimize code to make it fast;</li>
<li>and the hardware, which is ready to unleash a wrath of inconsistent chaos on
your program at a moment's notice.</li>
</ol>
<p>The memory model is fundamentally about trying to bridge the gap between these
three, allowing users to write the algorithms they want while the compiler and
hardware perform the arcane magic necessary to make them run fast.</p>
<h3 id="compiler-reordering"><a class="header" href="#compiler-reordering">Compiler Reordering</a></h3>
<p>Compilers fundamentally want to be able to do all sorts of complicated
transformations to reduce data dependencies and eliminate dead code. In
particular, they may radically change the actual order of events, or make events
never occur! If we write something like:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">x = 1;
y = 3;
x = 2;</code></pre>
<p>The compiler may conclude that it would be best if your program did:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">x = 2;
y = 3;</code></pre>
<p>This has inverted the order of events and completely eliminated one event.
From a single-threaded perspective this is completely unobservable: after all
the statements have executed we are in exactly the same state. But if our
program is multi-threaded, we may have been relying on <code>x</code> to actually be
assigned to 1 before <code>y</code> was assigned. We would like the compiler to be
able to make these kinds of optimizations, because they can seriously improve
performance. On the other hand, we'd also like to be able to depend on our
program <em>doing the thing we said</em>.</p>
<h3 id="hardware-reordering"><a class="header" href="#hardware-reordering">Hardware Reordering</a></h3>
<p>On the other hand, even if the compiler totally understood what we wanted and
respected our wishes, our hardware might instead get us in trouble. Trouble
comes from CPUs in the form of memory hierarchies. There is indeed a global
shared memory space somewhere in your hardware, but from the perspective of each
CPU core it is <em>so very far away</em> and <em>so very slow</em>. Each CPU would rather work
with its local cache of the data and only go through all the anguish of
talking to shared memory only when it doesn't actually have that memory in
cache.</p>
<p>After all, that's the whole point of the cache, right? If every read from the
cache had to run back to shared memory to double check that it hadn't changed,
what would the point be? The end result is that the hardware doesn't guarantee
that events that occur in some order on <em>one</em> thread, occur in the same
order on <em>another</em> thread. To guarantee this, we must issue special instructions
to the CPU telling it to be a bit less smart.</p>
<p>For instance, say we convince the compiler to emit this logic:</p>
<pre><code class="language-text">initial state: x = 0, y = 1

THREAD 1        THREAD 2
y = 3;          if x == 1 {
x = 1;              y *= 2;
                }
</code></pre>
<p>Ideally this program has 2 possible final states:</p>
<ul>
<li><code>y = 3</code>: (thread 2 did the check before thread 1 completed)</li>
<li><code>y = 6</code>: (thread 2 did the check after thread 1 completed)</li>
</ul>
<p>However there's a third potential state that the hardware enables:</p>
<ul>
<li><code>y = 2</code>: (thread 2 saw <code>x = 1</code>, but not <code>y = 3</code>, and then overwrote <code>y = 3</code>)</li>
</ul>
<p>It's worth noting that different kinds of CPU provide different guarantees. It
is common to separate hardware into two categories: strongly-ordered and
weakly-ordered, where strongly-ordered hardware implements weak orderings like
<code>Relaxed</code> using strong orderings like <code>Acquire</code>, while weakly-ordered hardware
makes use of the optimization potential that weak orderings like <code>Relaxed</code> give.
Most notably, x86/64 provides strong ordering guarantees, while ARM provides
weak ordering guarantees. This has two consequences for concurrent programming:</p>
<ul>
<li>
<p>Asking for stronger guarantees on strongly-ordered hardware may be cheap or
even free because they already provide strong guarantees unconditionally.
Weaker guarantees may only yield performance wins on weakly-ordered hardware.</p>
</li>
<li>
<p>Asking for guarantees that are too weak on strongly-ordered hardware is
more likely to <em>happen</em> to work, even though your program is strictly
incorrect. If possible, concurrent algorithms should be tested on
weakly-ordered hardware.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multithreaded-execution"><a class="header" href="#multithreaded-execution">Multithreaded Execution</a></h1>
<p>When you write Rust code to run on your computer, it may surprise you but you’re
not actually writing Rust code to run on your computer — instead, you’re writing
Rust code to run on the <em>abstract machine</em> (or AM for short). The abstract
machine, to be contrasted with the physical machine, is an abstract
representation of a theoretical computer: it doesn’t actually exist <em>per se</em>,
but the combination of a compiler, target architecture and target operating
system is capable of emulating a subset of its possible behaviours.</p>
<p>The Abstract Machine has a few properties that are essential to understand:</p>
<ol>
<li>It is architecture and OS-independent. The Abstract Machine doesn’t care
whether you’re on x86_64 or iOS or a Nintendo 3DS, the rules are the same
for everyone. This enables you to write code without having to think about
what the underlying system does or how it does it, as long as you obey the
Abstract Machine’s rules you know you’ll be fine.</li>
<li>It is the lowest common denominator of all supported computer systems. This
means it is allowed to result in executions no sane computer would actually
generate in real life. It is also purposefully built with forward
compatibility in mind, giving compilers the opportunity to make better and
more aggressive optimizations in the future. As a result, it can be quite
hard to test code, especially if you’re on a system that exploits fewer of
the AM’s allowed semantics, so it is highly recommended to utilize tools
that intentionally produce these executions like <a href="https://docs.rs/loom">Loom</a> and <a href="https://github.com/rust-lang/miri">Miri</a>.</li>
<li>Its model is highly formalized and not representative of what goes on
underneath. Because C++ needs to be defined by a formal specification and
not just hand-wavy rules about “this is what is allowed and this is what
isn’t”, the Abstract Machine defines things in a very mathematical and,
well, <em>abstract</em>, way; instead of saying things like “the compiler is
allowed to do X” it will find a way to define the system such that the
compiler’s ability to do X simply follows as a natural consequence. This
makes it very elegant and keeps the mathematicians happy, but you should
keep in mind that this is not how computers actually function, it is merely
a representation of it.</li>
</ol>
<p>With that out of the way, let’s look into how the C++20 Abstract Machine is
actually defined.</p>
<p>The first important thing to understand is that <strong>the abstract machine has no
concept of time</strong>. You might expect there to be a single global ordering of
events across the program where each happens at the same time or one after the
other, but under the abstract model no such ordering exists; instead, a possible
execution of the program must be treated as a single event that happens
instantaneously. There is never any such thing as “now”, or a “latest value”,
and using that terminology will only lead you to more confusion. Of course, in
reality there does exist a concept of time, but you must keep in mind that
you’re not programming for the hardware, you’re programming for the AM.</p>
<p>However, while no global ordering of operations exists <em>between</em> threads, there
does exist a single total ordering <em>within</em> each thread, which is known as its
<em>sequence</em>. For example, given this simple Rust program:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("A");
println!("B");
<span class="boring">}</span></code></pre></pre>
<p>its sequence during one possible execution can be visualized like so:</p>
<pre><code class="language-text">╭───────────────╮
│ println!("A") │
╰───────╥───────╯
╭───────⇓───────╮
│ println!("B") │
╰───────────────╯
</code></pre>
<p>That double arrow in between the two boxes (<code>⇒</code>) represents that the second
statement is <em>sequenced-after</em> the first (and similarly the first statement is
<em>sequenced-before</em> the second). This is the strongest kind of ordering guarantee
between any two operations, and only comes about when those two operations
happen one after the other and on the same thread.</p>
<p>If we add a second thread to the mix:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Thread 1:
println!("A");
println!("B");
// Thread 2:
eprintln!("01");
eprintln!("02");
<span class="boring">}</span></code></pre></pre>
<p>it will simply coexist in parallel, with each thread getting its own independent
sequence:</p>
<pre><code class="language-text">    Thread 1              Thread 2
╭───────────────╮    ╭─────────────────╮
│ println!("A") │    │ eprintln!("01") │
╰───────╥───────╯    ╰────────╥────────╯
╭───────⇓───────╮    ╭────────⇓────────╮
│ println!("B") │    │ eprintln!("02") │
╰───────────────╯    ╰─────────────────╯
</code></pre>
<p>We can say that the prints of <code>A</code> and <code>B</code> are <em>unsequenced</em> with regard to the
prints of <code>01</code> and <code>02</code> that occur in the second thread, since they have no
sequenced-before arrows connecting the boxes together.</p>
<p>Note that these diagrams are <strong>not</strong> a representation of multiple things that
<em>could</em> happen at runtime — instead, this diagram describes exactly what <em>did</em>
happen when the program ran once. This distinction is key, because it highlights
that even the lowest-level representation of a program’s execution does not have
a global ordering between threads; those two disconnected chains are all there
is.</p>
<p>Now let’s make things more interesting by introducing some shared data, and have
both threads read it.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Initial state
let data = 0;
// Thread 1:
println!("{data}");
// Thread 2:
eprintln!("{data}");
<span class="boring">}</span></code></pre></pre>
<p>Each memory location, similarly to threads, can be shown as another column on
our diagram, but holding values instead of instructions, and each access (read
or write) manifests as a line from the instruction that performed the access to
the associated value in the column. So this code can produce (and is in fact
guaranteed to produce) the following execution:</p>
<pre><code class="language-text">Thread 1     data     Thread 2
╭──────╮    ┌────┐    ╭──────╮
│ data ├╌╌╌╌┤  0 ├╌╌╌╌┤ data │
╰──────╯    └────┘    ╰──────╯
</code></pre>
<p>That is, both threads read the same value of <code>0</code> from <code>data</code>, and the two
operations are unsequenced — they have no relative ordering between them.</p>
<p>That’s reads done, so we’ll look at the other kind of data access next: writes.
We’ll also return to a single thread for now, just to keep things simple.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = 0;
data = 1;
<span class="boring">}</span></code></pre></pre>
<p>Here, we have a single variable that the main thread writes to once — this means
that in its lifetime, it holds two values, first <code>0</code>, and then <code>1</code>.
Diagrammatically, this code’s execution can be represented like so:</p>
<pre><code class="language-text"> Thread 1        data
╭───────╮       ┌────┐
│  = 1  ├╌╌╌┐   │  0 │
╰───────╯   ├╌╌╌┼╌╌╌╌┤
            └╌╌╌┼╌╌╌╌┤
                │  1 │
                └────┘
</code></pre>
<p>Note the use of dashed padding in between the values of <code>data</code>’s column. Those
spaces won’t ever contain a value, but they’re used to represent an
unsynchronized (non-atomic) write — it is garbage data and attempting to read it
would result in a data race.</p>
<p>Now let’s put all of our knowledge thus far together, and make a program both
that reads <em>and</em> writes data — woah, scary!</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut data = 0;
data = 1;
println!("{data}");
data = 2;
<span class="boring">}</span></code></pre></pre>
<p>Working out executions of code like this is rather like solving a Sudoku puzzle:
you must first lay out all the facts that you know, and then fill in the blanks
with logical reasoning. The initial information we’ve been given is both the
initial value of <code>data</code> and the sequential order of Thread 1; we also know that
over its lifetime, <code>data</code> takes on a total of three different values that were
caused by two different non-atomic writes. This allows us to start drawing out
some boxes:</p>
<pre><code class="language-text"> Thread 1        data
╭───────╮       ┌────┐
│  = 1  ├╌?     │  0 │
╰───╥───╯     ?╌┼╌╌╌╌┤
╭───⇓───╮     ?╌┼╌╌╌╌┤
│  data ├╌?     │  ? │
╰───╥───╯     ?╌┼╌╌╌╌┤
╭───⇓───╮     ?╌┼╌╌╌╌┤
│  = 2  ├╌?     │  ? │
╰───────╯       └────┘
</code></pre>
<p>We know all of those lines need to be joined <em>somewhere</em>, but we don’t quite
know <em>where</em> yet. This is where we need to bring in our first rule, a rule that
universally governs all accesses to every location in memory:</p>
<blockquote>
<p>From the point at which the access occurs, find every other point that can be
reached by following the reverse direction of arrows, then for each one of
those, take a single step across every line that connects to the relevant
memory location. <strong>It is not allowed for the access to read or write any value
that appears above any one of these points</strong>.</p>
</blockquote>
<p>In our case, there are two potential executions: one, where the first write
corresponds to the first value in <code>data</code>, and two, where the first write
corresponds to the second value in <code>data</code>. Considering the second case for a
moment, it would also force the second write to correspond to the first
value in <code>data</code>. Therefore its diagram would look something like this:</p>
<pre><code class="language-text"> Thread 1        data
╭───────╮       ┌────┐
│  = 1  ├╌╌┐    │  0 │
╰───╥───╯  ┊ ┌╌╌┼╌╌╌╌┤
╭───⇓───╮  ┊ ├╌╌┼╌╌╌╌┤
│  data ├╌?┊ ┊  │  2 │
╰───╥───╯  ├╌┼╌╌┼╌╌╌╌┤
╭───⇓───╮  └╌┼╌╌┼╌╌╌╌┤
│  = 2  ├╌╌╌╌┘  │  1 │
╰───────╯       └────┘
</code></pre>
<p>However, that second line breaks the rule we just established! Following up the
arrows from the third operation in Thread 1, we reach the first operation, and
from there we can take a single step to reach the space in between the <code>2</code> and
the <code>1</code>, which excludes the third access from writing any value above that point
— including the <code>2</code> that it is currently writing!</p>
<p>So evidently, this execution is no good. We can therefore conclude that the only
possible execution of this program is the other one, in which the <code>1</code> appears
above the <code>2</code>:</p>
<pre><code class="language-text"> Thread 1     data
╭───────╮     ┌────┐
│  = 1  ├╌╌┐  │  0 │
╰───╥───╯  ├╌╌┼╌╌╌╌┤
╭───⇓───╮  └╌╌┼╌╌╌╌┤
│  data ├╌?   │  1 │
╰───╥───╯  ┌╌╌┼╌╌╌╌┤
╭───⇓───╮  ├╌╌┼╌╌╌╌┤
│  = 2  ├╌╌┘  │  2 │
╰───────╯     └────┘
</code></pre>
<p>Now to sort out the read operation in the middle. We can use the same rule as
before to trace up to the first write and rule out us reading either the <code>0</code>
value or the garbage that exists between it and <code>1</code>, but how do we choose
between the <code>1</code> and the <code>2</code>? Well, as it turns out there is a complement to the
rule we already defined which gives us the exact answer we need:</p>
<blockquote>
<p>From the point at which the access occurs, find every other point that can be
reached by following the <em>forward</em> direction of arrows, then for each one of
those, take a single step across every line that connects to the relevant
memory location. <strong>It is not allowed for the access to read or write any value
that appears below any one of these points</strong>.</p>
</blockquote>
<p>Using this rule, we can follow the arrow downwards and then across and finally
rule out <code>2</code> as well as the garbage before it. This leaves us with exactly <em>one</em>
value that the read operation can return, and exactly one possible execution
guaranteed by the Abstract Machine:</p>
<pre><code class="language-text"> Thread 1      data
╭───────╮     ┌────┐
│  = 1  ├╌╌┐  │  0 │
╰───╥───╯  ├╌╌┼╌╌╌╌┤
╭───⇓───╮  └╌╌┼╌╌╌╌┤
│  data ├╌╌╌╌╌┤  1 │
╰───╥───╯  ┌╌╌┼╌╌╌╌┤
╭───⇓───╮  ├╌╌┼╌╌╌╌┤
│  = 2  ├╌╌┘  │  2 │
╰───────╯     └────┘
</code></pre>
<p>These two rules combined make up the more generalized rule known as <em>coherence</em>,
which is put in place to guarantee that a thread will never see a value earlier
than the last one it read or later than a one it will in future write. Coherence
is basically required for any program to act in a sane way, so luckily the C++20
standard guarantees it as one of its most fundamental principles.</p>
<p>You might be thinking that all this has been is the longest, most convoluted
explanation ever of the most basic intuitive semantics of programming — and
you’d be absolutely right. But it’s essential to grasp these fundamentals,
because once you have this model in mind, the extension into multiple threads
and the complicated semantics of real atomics becomes completely natural.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="relaxed"><a class="header" href="#relaxed">Relaxed</a></h1>
<p>Now we’ve got single-threaded mutation semantics out of the way, we can try
reintroducing a second thread. We’ll have one thread perform a write to the
memory location, and a second thread read from it, like so:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Initial state
let mut data = 0;
// Thread 1:
data = 1;
// Thread 2:
println!("{data}");
<span class="boring">}</span></code></pre></pre>
<p>Of course, any Rust programmer will immediately tell you that this code doesn’t
compile, and indeed it definitely does not, and for good reason. But suspend
your disbelief for a moment, and imagine what would happen if it did. Let’s draw
a diagram, leaving out the reading lines for now:</p>
<pre><code class="language-text">Thread 1     data    Thread 2
╭───────╮   ┌────┐   ╭───────╮
│  = 1  ├╌┐ │  0 │ ?╌┤  data │
╰───────╯ ├╌┼╌╌╌╌┤   ╰───────╯
          └╌┼╌╌╌╌┤
            │  1 │
            └────┘
</code></pre>
<p>Unfortunately, coherence doesn’t help us in finding out where Thread 2’s line
joins up to, since there are no arrows connecting that operation to anything and
therefore we can’t immediately rule any values out. As a result, we end up
facing a situation we haven’t faced before: there is <em>more than one</em> potential
value for Thread 2 to read.</p>
<p>And this is where we encounter the big limitation with unsynchronized data
accesses: the price we pay for their speed and optimization capability is that
this situation is considered <strong>Undefined Behavior</strong>. For an unsynchronized read
to be acceptable, there has to be <em>exactly one</em> potential value for it to read,
and when there are multiple like in this situation it is considered a data race.</p>
<p>So what can we do about this? Well, two things need to be changed. First of all,
Thread 1 has to use an atomic store instead of an unsynchronized write, and
secondly Thread 2 has to use an atomic load instead of an unsynchronized read.
You’ll also notice that all the atomic functions accept one (and sometimes two)
parameters of <code>atomic::Ordering</code>s — we’ll explore the details of the differences
between them later, but for now we’ll use <code>Relaxed</code> because it is by far the
simplest of the lot.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>// Initial state
let data = AtomicU32::new(0);
// Thread 1:
data.store(1, atomic::Ordering::Relaxed);
// Thread 2:
data.load(atomic::Ordering::Relaxed);
<span class="boring">}</span></code></pre></pre>
<p>The use of the atomic store provides one additional ability in comparison to an
unsynchronized store, and that is that there is no “in-between” state between
the old and new values — instead, it immediately updates, resulting in a diagram
that look a bit more like this:</p>
<pre><code class="language-text">Thread 1     data
╭───────╮   ┌────┐
│  = 1  ├─┐ │  0 │
╰───────╯ │ └────┘
          └─┬────┐
            │  1 │
            └────┘
</code></pre>
<p>We have now established a <em>modification order</em> for <code>data</code>: a total, ordered list
of distinct, separated values that it takes over its lifetime.</p>
<p>On the loading side, we also obtain one additional ability: when there are
multiple possible values to choose from in the modification order, instead of it
triggering UB, exactly one (but it is unspecified which) value is chosen. This
means that there are now <em>two</em> potential executions of our program, with no way
for us to control which one occurs:</p>
<pre><code class="language-text">     Possible Execution 1       ┃       Possible Execution 2
                                ┃
Thread 1     data    Thread 2   ┃  Thread 1     data    Thread 2
╭───────╮   ┌────┐   ╭───────╮  ┃  ╭───────╮   ┌────┐   ╭───────╮
│ store ├─┐ │  0 ├───┤  load │  ┃  │ store ├─┐ │  0 │ ┌─┤  load │
╰───────╯ │ └────┘   ╰───────╯  ┃  ╰───────╯ │ └────┘ │ ╰───────╯
          └─┬────┐              ┃            └─┬────┐ │
            │  1 │              ┃              │  1 ├─┘
            └────┘              ┃              └────┘
</code></pre>
<p>Note that <strong>both sides must be atomic to avoid the data race</strong>: if only the
writing side used atomic operations, the reading side would still have multiple
values to choose from (UB), and if only the reading side used atomic operations
it could end up reading the garbage data “in-between” <code>0</code> and <code>1</code> (also UB).</p>
<blockquote>
<p><strong>NOTE:</strong> This description of why both sides are needed to be atomic
operations, while neat and intuitive, is not strictly correct: in reality the
answer is simply “because the spec says so”. However, it is functionally
equivalent to the real rules, so it can aid in understanding.</p>
</blockquote>
<h2 id="read-modify-write-operations"><a class="header" href="#read-modify-write-operations">Read-modify-write operations</a></h2>
<p>Loads and stores are pretty neat in avoiding data races, but you can’t get very
far with them. For example, suppose you wanted to implement a global shared
counter that can be used to assign unique IDs to objects. Naïvely, you might try
to write code like this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU64};
</span>static COUNTER: AtomicU64 = AtomicU64::new(0);
pub fn get_id() -&gt; u64 {
    let value = COUNTER.load(atomic::Ordering::Relaxed);
    COUNTER.store(value + 1, atomic::Ordering::Relaxed);
    value
}
<span class="boring">}</span></code></pre></pre>
<p>But then calling that function from multiple threads opens you up to an
execution like below that results in two threads obtaining the same ID (note
that the duplication of <code>1</code> in the modification order is intentional; even if
two values are the same, they always get separate entries in the order if they
were caused by different accesses):</p>
<pre><code class="language-text">Thread 1   COUNTER   Thread 2
╭───────╮   ┌───┐   ╭───────╮
│ load  ├───┤ 0 ├───┤  load │
╰───╥───╯   └───┘   ╰────╥──╯
╭───⇓───╮ ┌─┬───┐   ╭────⇓──╮
│ store ├─┘ │ 1 │ ┌─┤ store │
╰───────╯   └───┘ │ ╰───────╯
            ┌───┬─┘
            │ 1 │
            └───┘
</code></pre>
<p>This is known as a a <strong>race condition</strong> — a logic error in a program caused by a
specific unintended execution of concurrent code. Note that this is distinct
from a <em>data race</em>: while a data race is caused by two threads performing
unsynchronized operations at the same time and is always undefined behaviour,
race conditions are totally OK and defined behaviour from the AM’s perspective,
but are only harmful because the programmer didn’t expect it to be possible. You
can think of the distinction between the two as analagous to the difference
between indexing out-of-bounds and indexing in-bounds, but to the wrong element:
both are bugs, but only one is universally a bug, and the other is merely a
logic problem.</p>
<p>Technically, I believe it is <em>possible</em> to solve this problem with just loads
and stores, if you try hard enough and use several atomics. But luckily, you
don’t have to because there also exists another kind of operation, the
read-modify-write, which is specifically suited to this purpose.</p>
<p>A read-modify-write operation (shortened to RMW) is a special kind of atomic
operation that reads, changes and writes back a value <em>in one step</em>. This means
that there are guaranteed to exist no other values in the modification order in
between the read and the write; it happens as a single operation. I would also
like to point out that this is true of <strong>all</strong> atomic orderings, since a common
misconception is that the <code>Relaxed</code> ordering somehow negates this guarantee.</p>
<blockquote>
<p>Another common confusion about RMWs is that they are guaranteed to “see the
latest value” of an atomic, which I believe came from a misinterpretation of
the C++ specification and was later spread by rumour. Of course, this makes no
sense, since atomics have no latest value due to the lack of the concept of
time. The original statement in the specification was actually just specifying
that atomic RMWs are atomic: they only consider the directly previous value in
the modification order and not any value before it, and gave no additional
guarantee.</p>
</blockquote>
<p>There are many different RMW operations to choose from, but the one most
appropriate for this use case is <code>fetch_add</code>, which adds a number to the atomic,
as well as returns the old value. So our code can be rewritten as this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU64};
</span>static COUNTER: AtomicU64 = AtomicU64::new(0);
pub fn get_id() -&gt; u64 {
    COUNTER.fetch_add(1, atomic::Ordering::Relaxed)
}
<span class="boring">}</span></code></pre></pre>
<p>And then, no matter how many threads there are, that race condition from earlier
can never occur. Executions will have to look more like this:</p>
<pre><code class="language-text">  Thread 1     COUNTER     Thread 2
╭───────────╮   ┌───┐   ╭───────────╮
│ fetch_add ├─┐ │ 0 │ ┌─┤ fetch_add │
╰───────────╯ │ └───┘ │ ╰───────────╯
              └─┬───┐ │
                │ 1 │ │
                └───┘ │
                ┌───┬─┘
                │ 2 │
                └───┘
</code></pre>
<p>There is one problem with this code however, and that is that if <code>get_id()</code> is
called over 18 446 744 073 709 551 615 times, the counter will overflow and it
will start generating duplicate IDs. Of course, this won’t feasibly happen, but
it can be problematic if you need to <em>prove</em> that it can’t happen (e.g. for
safety purposes) or you’re using a smaller integer type like <code>u32</code>.</p>
<p>So we’re going to modify this function so that instead of returning a plain
<code>u64</code> it returns an <code>Option&lt;u64&gt;</code>, where <code>None</code> is used to indicate that an
overflow occurred and no more IDs could be generated. Additionally, it’s not
enough to just return <code>None</code> once, because if there are multiple threads
involved they will not see that result if it just occurs on a single thread —
instead, it needs to continue to return <code>None</code> <em>until the end of time</em> (or,
well, this execution of the program).</p>
<p>That means we have to do away with <code>fetch_add</code>, because <code>fetch_add</code> will always
overflow and there’s no <code>checked_fetch_add</code> equivalent. We’ll return to our racy
algorithm for a minute, this time thinking more about what went wrong. The steps
look something like this:</p>
<ol>
<li>Load a value of the atomic</li>
<li>Perform the checked add, propagating <code>None</code></li>
<li>Store in the new value of the atomic</li>
</ol>
<p>The problem here is that the store does not necessarily occur directly after the
load in the atomic’s modification order, and that leads to the races. What we
need is some way to say, “add this new value to the modification order, but
<em>only if</em> it occurs directly after the value we loaded”. And luckily for us,
there exists a function that does exactly* this: <code>compare_exchange</code>.</p>
<p><code>compare_exchange</code> is a bit like a store, but instead of unconditionally storing
the value, it will first check the value directly before the <code>compare_exchange</code>
in the modification order to see whether it is what we expect, and if not it
will simply tell us that and not make any changes. It is an RMW operation, so
all of this happens fully atomically — there is no chance for a race condition.</p>
<blockquote>
<p>* It’s not quite the same, because <code>compare_exchange</code> can suffer from ABA
problems in which it will see a later value in the modification order that
just happened to be same and succeed. For example, if the modification order
contained <code>1, 2, 1</code> and a thread loaded the first <code>1</code>,
<code>compare_exchange(1, 3)</code> could succeed in replacing either the first or second
<code>1</code>, giving either <code>1, 3, 2, 1</code> or <code>1, 2, 1, 3</code>.</p>
<p>For some algorithms, this is problematic and needs to be taken into account
with additional checks; however for us, values can never be reused so we don’t
have to worry about it.</p>
</blockquote>
<p>In our case, we can simply replace the store with a compare exchange of the old
value and itself plus one (returning <code>None</code> instead if the addition overflowed,
to prevent overflowing the atomic). Should the <code>compare_exchange</code> fail, we know
that some other thread inserted a value in the modification order after the
value we loaded. This isn’t really a problem — we can just try again and again
until we succeed, and <code>compare_exchange</code> is even nice enough to give us the
updated value so we don’t have to load again. Also note that after we’ve updated
our value of the atomic, we’re guaranteed to never see the old value again, by
the coherence rules from the previous chapter.</p>
<p>So here’s how it looks with these changes appplied:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU64};
</span>static COUNTER: AtomicU64 = AtomicU64::new(0);
pub fn get_id() -&gt; Option&lt;u64&gt; {
    // Load the counter’s initial value from some place in the modification
    // order (it doesn’t matter where, because the compare exchange makes sure
    // that our new value appears directly after it).
    let mut value = COUNTER.load(atomic::Ordering::Relaxed);
    loop {
        // Attempt to add one to the atomic.
        let res = COUNTER.compare_exchange(
            value,
            value.checked_add(1)?,
            atomic::Ordering::Relaxed,
            atomic::Ordering::Relaxed,
        );
        // Check what happened…
        match res {
            // If there was no value in between the value we loaded and our
            // newly written value in the modification order, the compare
            // exchange suceeded and so we are done.
            Ok(_) =&gt; break,

            // Otherwise, there was a value in between and so we need to retry
            // the addition and continue looping.
            Err(updated_value) =&gt; value = updated_value,
        }
    }
    Some(value)
}
<span class="boring">}</span></code></pre></pre>
<p>This <code>compare_exchange</code> loop enables the algorithm to succeed even under
contention; it will simply try again (and again and again). In the below
execution, Thread 1 gets raced to storing its value of <code>1</code> to the counter, but
that’s okay because it will just add <code>1</code> to the <code>1</code>, making <code>2</code>, and retry the
compare exchange with that, eventually resulting in a unique ID.</p>
<pre><code class="language-text">Thread 1   COUNTER   Thread 2
╭───────╮   ┌───┐   ╭───────╮
│ load  ├───┤ 0 ├───┤ load  │
╰───╥───╯   └───┘   ╰───╥───╯
╭───⇓───╮   ┌───┬─┐ ╭───⇓───╮
│  cas  ├───┤ 1 │ └─┤  cas  │
╰───╥───╯   └───┘   ╰───────╯
╭───⇓───╮ ┌─┬───┐
│  cas  ├─┘ │ 2 │
╰───────╯   └───┘
</code></pre>
<blockquote>
<p><code>compare_exchange</code> is abbreviated to CAS here (which stands for
compare-and-swap), since that is the more general name for the operation. It
is not to be confused with <code>compare_and_swap</code>, a deprecated method on Rust
atomics that performs the same task as <code>compare_exchange</code> but has an inferior
design in some ways.</p>
</blockquote>
<p>There are two additional improvements we can make here. First, because our
algorithm occurs in a loop, it is actually perfectly fine for the CAS to fail
even when there wasn’t a value inserted in the modification order in between,
since we’ll just run it again. This allows to switch out our call to
<code>compare_exchange</code> with a call to the weaker <code>compare_exchange_weak</code>, that
unlike the former function is allowed to <em>spuriously</em> (i.e. randomly, from the
programmer’s perspective) fail. This often results in better performance on
architectures like ARM, since their <code>compare_exchange</code> is really just a loop
around the underlying <code>compare_exchange_weak</code>. x86_64 however will see no
difference in performance.</p>
<p>The second improvement is that this pattern is so common that the standard
library even provides a helper function for it, called <code>fetch_update</code>. It
implements the boilerplate <code>load</code>-<code>loop</code>-<code>match</code> parts for us, so all we have to
do is provide the closure that calls <code>checked_add(1)</code> and it will all just work.
This leads us to our final code for this example:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU64};
</span>static COUNTER: AtomicU64 = AtomicU64::new(0);
pub fn get_id() -&gt; Option&lt;u64&gt; {
    COUNTER.fetch_update(
        atomic::Ordering::Relaxed,
        atomic::Ordering::Relaxed,
        |value| value.checked_add(1),
    )
    .ok()
}
<span class="boring">}</span></code></pre></pre>
<p>These CAS loops are the absolute bread and butter of concurrent programming;
they’re absolutely everywhere and essential to know about. Every other RMW
operation on atomics can (and often is, if the hardware doesn’t have a more
efficient implementation) be implemented via a CAS loop. This is why CAS is seen
as the canonical example of an RMW — it’s pretty much the most fundamental
operation you can get on atomics.</p>
<p>I’d also like to briefly bring attention to the atomic orderings used in this
section. They were mostly glossed over, but we were exclusively using <code>Relaxed</code>,
and that’s because for something as simple as a global ID counter, <em>you never
need more than <code>Relaxed</code></em>. The more complex cases which we’ll look at later
definitely do need stronger orderings, but as a general rule, if:</p>
<ul>
<li>you only have one atomic, and</li>
<li>you have no other related pieces of data</li>
</ul>
<p><code>Relaxed</code> is more than sufficient.</p>
<h2 id="out-of-thin-air-values"><a class="header" href="#out-of-thin-air-values">“Out-of-thin-air” values</a></h2>
<p>One peculiar consequence of the semantics of <code>Relaxed</code> operations is that it is
theoretically possible for values to come into existence “out-of-thin-air”
(commonly abbreviated to OOTA) — that is, a value could appear despite not ever
being calculated anywhere in code. In particular, consider this setup:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>let x = AtomicU32::new(0);
let y = AtomicU32::new(0);

// Thread 1:
let r1 = y.load(atomic::Ordering::Relaxed);
x.store(r1, atomic::Ordering::Relaxed);

// Thread 2:
let r2 = x.load(atomic::Ordering::Relaxed);
y.store(r2, atomic::Ordering::Relaxed);
<span class="boring">}</span></code></pre></pre>
<p>When starting to draw a diagram for a possible execution of this program, we
have to first lay out the basic facts that we know:</p>
<ul>
<li><code>x</code> and <code>y</code> both start out as zero</li>
<li>Thread 1 performs a load of <code>y</code> followed by a store of <code>x</code></li>
<li>Thread 2 performs a load of <code>x</code> followed by a store of <code>y</code></li>
<li>Each of <code>x</code> and <code>y</code> take on exactly two values in their lifetime</li>
</ul>
<p>Then we can start to construct boxes:</p>
<pre><code class="language-text">Thread 1      x       y      Thread 2
╭───────╮   ┌───┐   ┌───┐   ╭───────╮
│  load ├─┐ │ 0 │   │ 0 │ ┌─┤ load  │
╰───╥───╯ │ └───┘   └───┘ │ ╰───╥───╯
    ║     │   ?───────────┘     ║
╭───⇓───╮ └───────────?     ╭───⇓───╮
│ store ├───┬───┐   ┌───┬───┤ store │
╰───────╯   │ ? │   │ ? │   ╰───────╯
            └───┘   └───┘
</code></pre>
<p>At this point, if either of those lines were to connect to the higher box then
the execution would be simple: that thread would forward the value to its lower
box, which the other thread would then either read, or load the same value
(zero) from the box above it, and we’d end up with zero in both atomics. But
what if they were to connect downwards? Then we’d end up with an execution that
looks like this:</p>
<pre><code class="language-text">Thread 1      x       y      Thread 2
╭───────╮   ┌───┐   ┌───┐   ╭───────╮
│  load ├─┐ │ 0 │   │ 0 │ ┌─┤ load  │
╰───╥───╯ │ └───┘   └───┘ │ ╰───╥───╯
    ║     │   ┌───────────┘     ║
╭───⇓───╮ └───┼───────┐     ╭───⇓───╮
│ store ├───┬─┴─┐   ┌─┴─┬───┤ store │
╰───────╯   │ ? │   │ ? │   ╰───────╯
            └───┘   └───┘
</code></pre>
<p>But hang on — it’s not fully resolved yet, we still haven’t put in a value in
those lower question marks. So what value should it be? Well, the second value
of <code>x</code> is just copied from from the second value of <code>y</code>, so we just have to find
the value of that — but the second value of <code>y</code> is itself copied from the second
value of <code>x</code>! This means that we can actually put any value we like in that box,
including <code>0</code> or <code>42</code>, and the logic will check out perfectly fine — meaning if
this program were to execute in this fashion, it would end up reading a value
produced out of thin air!</p>
<p>Now, if we were to strictly follow the rules we’ve laid out thus far, then this
would be totally valid thing to happen. But luckily, the authors of the C++
specification have recognized this as a problem, and as such refined the
semantics of <code>Relaxed</code> to implement a thorough, logically sound, mathematically
proven formal model that prevents it, that’s just too complex and technical to
explain here—</p>
<blockquote>
<p>No “out-of-thin-air” values can be computed that circularly depend on their
own computations.</p>
</blockquote>
<p>Just kidding. Turns out, it’s a <em>really</em> difficult problem to solve, and to my
knowledge even now there is no known formal way to express how to prevent it. So
in the specification they just kind of hand-wave and say that it shouldn’t
happen, and that the above program must always give zero in both atomics,
despite the theoretical execution that could result in something else. Well, it
generally works in practice so I can’t complain — it’s just a very interesting
detail to know about.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="acquire-and-release"><a class="header" href="#acquire-and-release">Acquire and Release</a></h1>
<p>Next, we’re going to try and implement one of the simplest concurrent utilities
possible — a mutex, but without support for waiting (since that’s not really
related to what we’re doing now). It will hold both an atomic flag that
indicates whether it is locked or not, and the protected data itself. In code
this translates to:</p>
<pre><code class="language-rs">use std::cell::UnsafeCell;
use std::sync::atomic::AtomicBool;

pub struct Mutex&lt;T&gt; {
    locked: AtomicBool,
    data: UnsafeCell&lt;T&gt;,
}

impl&lt;T&gt; Mutex&lt;T&gt; {
    pub const fn new(data: T) -&gt; Self {
        Self {
            locked: AtomicBool::new(false),
            data: UnsafeCell::new(data),
        }
    }
}
</code></pre>
<p>Now for the lock function. We need to use an RMW here, since we need to both
check whether it is locked and lock it if it isn’t in a single atomic step; this
can be most simply done with a <code>compare_exchange</code> (unlike before, it doesn’t
need to be in a loop this time). For the ordering, we’ll just use <code>Relaxed</code>
since we don’t know of any others yet.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::cell::UnsafeCell;
</span><span class="boring">use std::sync::atomic::{self, AtomicBool};
</span><span class="boring">pub struct Mutex&lt;T&gt; {
</span><span class="boring">    locked: AtomicBool,
</span><span class="boring">    data: UnsafeCell&lt;T&gt;,
</span><span class="boring">}
</span>impl&lt;T&gt; Mutex&lt;T&gt; {
    pub fn lock(&amp;self) -&gt; Option&lt;Guard&lt;'_, T&gt;&gt; {
        match self.locked.compare_exchange(
            false,
            true,
            atomic::Ordering::Relaxed,
            atomic::Ordering::Relaxed,
        ) {
            Ok(_) =&gt; Some(Guard(self)),
            Err(_) =&gt; None,
        }
    }
}

pub struct Guard&lt;'mutex, T&gt;(&amp;'mutex Mutex&lt;T&gt;);
// Deref impl omitted…
<span class="boring">}</span></code></pre></pre>
<p>We also need to implement <code>Drop</code> for <code>Guard</code> to make sure the lock on the mutex
is released once the guard is destroyed. Again we’re just using the <code>Relaxed</code>
ordering.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::cell::UnsafeCell;
</span><span class="boring">use std::sync::atomic::{self, AtomicBool};
</span><span class="boring">pub struct Mutex&lt;T&gt; {
</span><span class="boring">    locked: AtomicBool,
</span><span class="boring">    data: UnsafeCell&lt;T&gt;,
</span><span class="boring">}
</span><span class="boring">pub struct Guard&lt;'mutex, T&gt;(&amp;'mutex Mutex&lt;T&gt;);
</span>impl&lt;T&gt; Drop for Guard&lt;'_, T&gt; {
    fn drop(&amp;mut self) {
        self.0.locked.store(false, atomic::Ordering::Relaxed);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Great! In the normal operation then, this primitive should allow unique access
to the data of the mutex to be transferred across different threads. Usual usage
could look like this:</p>
<pre><code class="language-rust ignore">// Initial state
let mutex = Mutex::new(0);
// Thread 1
if let Some(guard) = mutex.lock() {
    *guard += 1;
}
// Thread 2
if let Some(guard) = mutex.lock() {
    println!("{}", *guard);
}</code></pre>
<p>Now, there are many possible executions of this code. For example, Thread 2 (the
reader thread) could lock the mutex first, and Thread 1 (the writer thread)
could fail to lock it:</p>
<pre><code class="language-text">Thread 1      locked    data      Thread 2
╭───────╮   ┌────────┐ ┌───┐     ╭───────╮
│  cas  ├─┐ │ false  │ │ 0 ├╌┐ ┌─┤  cas  │
╰───────╯ │ └────────┘ └───┘ ┊ │ ╰───╥───╯
          │ ┌────────┬───────┼─┘ ╭───⇓───╮
          └─┤  true  │       └╌╌╌┤ guard │
            └────────┘           ╰───╥───╯
            ┌────────┬─────────┐ ╭───⇓───╮
            │ false  │         └─┤ store │
            └────────┘           ╰───────╯
</code></pre>
<p>Or potentially Thread <em>1</em> could lock the mutex first, and Thread <em>2</em> could fail
to lock it:</p>
<pre><code class="language-text">Thread 1      locked      data    Thread 2
╭───────╮   ┌────────┐   ┌───┐   ╭───────╮
│  cas  ├─┐ │ false  │ ┌─│ 0 │───┤  cas  │
╰───╥───╯ │ └────────┘ │┌┼╌╌╌┤   ╰───────╯
╭───⇓───╮ └─┬────────┐ │├┼╌╌╌┤
│ += 1; ├╌┐ │  true  ├─┘┊│ 1 │
╰───╥───╯ ┊ └────────┘  ┊└───┘
╭───⇓───╮ └╌╌╌╌╌╌╌╌╌╌╌╌╌┘
│ store ├───┬────────┐
╰───────╯   │ false  │
            └────────┘
</code></pre>
<p>But the interesting case comes in when Thread 1 successfully locks and unlocks
the mutex, and then Thread 2 locks it. Let’s draw that one out too:</p>
<pre><code class="language-text">Thread 1      locked     data       Thread 2
╭───────╮   ┌────────┐   ┌───┐     ╭───────╮
│  cas  ├─┐ │ false  │   │ 0 │ ┌───┤  cas  │
╰───╥───╯ │ └────────┘  ┌┼╌╌╌┤ │   ╰───╥───╯
╭───⇓───╮ └─┬────────┐  ├┼╌╌╌┤ │   ╭───⇓───╮
│ += 1; ├╌┐ │  true  │  ┊│ 1 │ │ ?╌┤ guard │
╰───╥───╯ ┊ └────────┘  ┊└───┘ │   ╰───╥───╯
╭───⇓───╮ └╌╌╌╌╌╌╌╌╌╌╌╌╌┘      │   ╭───⇓───╮
│ store ├───┬────────┐         │ ┌─┤ store │
╰───────╯   │ false  │         │ │ ╰───────╯
            └────────┘         │ │
            ┌────────┬─────────┘ │
            │  true  │           │
            └────────┘           │
            ┌────────┬───────────┘
            │ false  │
            └────────┘
</code></pre>
<p>Look at the second operation Thread 2 performs (the read of <code>data</code>), for which
we haven’t yet joined the line. Where should it connect to? Well actually, it
has multiple options…wait, we’ve seen this before! It’s a data race!</p>
<p>That’s not good. Last time the solution was to use atomics instead — but in this
case that doesn’t seem to be enough, since even if atomics were used it still
would have the <em>option</em> of reading <code>0</code> instead of <code>1</code>, and really if we want our
mutex to be sane, it should only be able to read <code>1</code>.</p>
<p>So it seems that what we <em>want</em> is to be able to apply the coherence rules from
before to completely rule out zero from the set of the possible values — if we
were able to draw a large arrow from the Thread 1’s <code>+= 1;</code> to Thread 2’s
<code>guard</code>, then we could trivially then use the rule to rule out <code>0</code> as a value
that could be read.</p>
<p>This is where the <code>Acquire</code> and <code>Release</code> orderings come in. Informally put, a
<em>release store</em> will cause an arrow instead of a line to be drawn from the
operation to the destination; and similarly an <em>acquire load</em> will cause an
arrow to be drawn from the destination to the operation. To give a useless
example that illustrates this, for the given program:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>// Initial state
let a = AtomicU32::new(0);
// Thread 1
a.store(1, atomic::Ordering::Release);
// Thread 2
a.load(atomic::Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>The two possible executions look like this:</p>
<pre><code class="language-text">    Possible Execution 1      ┃      Possible Execution 2
                              ┃
Thread 1      a     Thread 2  ┃  Thread 1      a     Thread 2
╭───────╮   ┌───┐   ╭──────╮  ┃  ╭───────╮   ┌───┐   ╭──────╮
│ store ├─┐ │ 0 │ ┌─→ load │  ┃  │ store ├─┐ │ 0 ├───→ load │
╰───────╯ │ └───┘ │ ╰──────╯  ┃  ╰───────╯ │ └───┘   ╰──────╯
          └─↘───┐ │           ┃            └─↘───┐
            │ 1 ├─┘           ┃              │ 1 │
            └───┘             ┃              └───┘
</code></pre>
<p>These arrows are a new kind of arrow we haven’t seen yet; they are known as
<em>happens-before</em> (or happens-after) relations and are represented as thin arrows
(→) on these diagrams. They are weaker than the <em>sequenced-before</em>
double-arrows (⇒) that occur inside a single thread, but can still be used with
the coherence rules to determine which values of a memory location are valid to
read.</p>
<p>When a happens-before arrow stores a data value to an atomic (via a release
operation) which is then loaded by another happens-before arrow (via an acquire
operation) we say that the release operation <em>synchronized-with</em> the acquire
operation, which in doing so establishes that the release operation
<em>happens-before</em> the acquire operation. Therefore, we can say that in the first
possible execution, Thread 1’s <code>store</code> synchronizes-with Thread 2’s <code>load</code>,
which causes that <code>store</code> and everything sequenced-before it to happen-before
the <code>load</code> and everything sequenced-after it.</p>
<blockquote>
<p>More formally, we can say that A happens-before B if any of the following
conditions are true:</p>
<ol>
<li>A is sequenced-before B (i.e. A occurs before B on the same thread)</li>
<li>A synchronizes-with B (i.e. A is a <code>Release</code> operation and B is an
<code>Acquire</code> operation that reads the value written by A)</li>
<li>A happens-before X, and X happens-before B (transitivity)</li>
</ol>
</blockquote>
<p>There is one more rule required for these to be useful, and that is <em>release
sequences</em>: after a release store is performed on an atomic, happens-before
arrows will connect together each subsequent value of the atomic as long as the
new value is caused by an RMW and not just a plain store (this means any
subsequent normal store, no matter the ordering, will end the sequence).</p>
<blockquote>
<p>In the C++11 memory model, any subsequent store by the same thread that
performed the original <code>Release</code> store would also contribute to the release
sequence. However, this was removed in C++20 for simplicity and better
optimizations and so <strong>must not</strong> be relied upon.</p>
</blockquote>
<p>With those rules in mind, converting Thread 1’s second store to use a <code>Release</code>
ordering as well as converting Thread 2’s CAS to use an <code>Acquire</code> ordering
allows us to effectively draw that arrow we needed before:</p>
<pre><code class="language-text">Thread 1     locked     data       Thread 2
╭───────╮   ┌───────┐   ┌───┐     ╭───────╮
│  cas  ├─┐ │ false │   │ 0 │ ┌───→  cas  │
╰───╥───╯ │ └───────┘  ┌┼╌╌╌┤ │   ╰───╥───╯
╭───⇓───╮ └─┬───────┐  ├┼╌╌╌┤ │   ╭───⇓───╮
│ += 1; ├╌┐ │ true  │  ┊│ 1 ├╌│╌╌╌┤ guard │
╰───╥───╯ ┊ └───────┘  ┊└───┘ │   ╰───╥───╯
╭───⇓───╮ └╌╌╌╌╌╌╌╌╌╌╌╌┘      │   ╭───⇓───╮
│ store ├───↘───────┐         │ ┌─┤ store │
╰───────╯   │ false │         │ │ ╰───────╯
            └───┬───┘         │ │
            ┌───↓───┬─────────┘ │
            │ true  │           │
            └───────┘           │
            ┌───────┬───────────┘
            │ false │
            └───────┘
</code></pre>
<p>We now can trace back along the reverse direction of arrows from the <code>guard</code>
bubble to the <code>+= 1</code> bubble; we have established that Thread 2’s load
happens-after the <code>+= 1</code> side effect, because Thread 2’s CAS synchronizes-with
Thread 1’s store. This both avoids the data race and gives the guarantee that
<code>1</code> will be always read by Thread 2 (as long as it locks after Thread 1, of
course).</p>
<p>However, that is not the only execution of the program possible. Even with this
setup, there is another execution that can also cause UB: if Thread 2 locks the
mutex before Thread 1 does.</p>
<pre><code class="language-text">Thread 1       locked     data      Thread 2
╭───────╮     ┌───────┐   ┌───┐    ╭───────╮
│  cas  ├───┐ │ false │┌──│ 0 │────→  cas  │
╰───╥───╯   │ └───────┘│ ┌┼╌╌╌┤    ╰───╥───╯
╭───⇓───╮   │ ┌───────┬┘ ├┼╌╌╌┤    ╭───⇓───╮
│ += 1; ├╌┐ │ │ true  │  ┊│ 1 │  ?╌┤ guard │
╰───╥───╯ ┊ │ └───────┘  ┊└───┘    ╰───╥───╯
╭───⇓───╮ └╌│╌╌╌╌╌╌╌╌╌╌╌╌┘         ╭───⇓───╮
│ store ├─┐ │ ┌───────┬────────────┤ store │
╰───────╯ │ │ │ false │            ╰───────╯
          │ │ └───────┘
          │ └─┬───────┐
          │   │ true  │
          │   └───────┘
          └───↘───────┐
              │ false │
              └───────┘
</code></pre>
<p>Once again <code>guard</code> has multiple options for values to read. This one’s a bit
more counterintuitive than the previous one, since it requires “travelling
forward in time” to understand why the <code>1</code> is even there in the first place —
but since the abstract machine has no concept of time, it’s just a valid UB as
any other.</p>
<p>Luckily, we’ve already solved this problem once, so it easy to solve again: just
like before, we’ll have the CAS become acquire and the store become release, and
then we can use the second coherence rule from before to follow <em>forward</em> the
arrow from the <code>guard</code> bubble all the way to the <code>+= 1;</code>, determining that it is
only possible for that read to see <code>0</code> as its value, as in the execution below.</p>
<pre><code class="language-text">Thread 1       locked     data      Thread 2
╭───────╮     ┌───────┐   ┌───┐    ╭───────╮
│  cas  ←───┐ │ false │┌──│ 0 ├╌┐──→  cas  │
╰───╥───╯   │ └───────┘│ ┌┼╌╌╌┤ ┊  ╰───╥───╯
╭───⇓───╮   │ ┌───────┬┘ ├┼╌╌╌┤ ┊  ╭───⇓───╮
│ += 1; ├╌┐ │ │ true  │  ┊│ 1 │ └─╌┤ guard │
╰───╥───╯ ┊ │ └───────┘  ┊└───┘    ╰───╥───╯
╭───⇓───╮ └╌│╌╌╌╌╌╌╌╌╌╌╌╌┘         ╭───⇓───╮
│ store ├─┐ │ ┌───────↙────────────┤ store │
╰───────╯ │ │ │ false │            ╰───────╯
          │ │ └───┬───┘
          │ └─┬───↓───┐
          │   │ true  │
          │   └───────┘
          └───↘───────┐
              │ false │
              └───────┘
</code></pre>
<p>This leads us to the proper memory orderings for any mutex (and other locks like
RW locks too, even): use <code>Acquire</code> to lock it, and <code>Release</code> to unlock it. So
let’s go back to and update our original mutex definition with this knowledge.</p>
<p>But wait, <code>compare_exchange</code> takes two ordering parameters, not just one! That’s
right — it also takes a second one to apply when the exchange fails (in our case,
when the mutex is already locked). But we don’t need an <code>Acquire</code> here, since in
that case we won’t be reading from the <code>data</code> value anyway, so we’ll just stick
with <code>Relaxed</code>.</p>
<pre><code class="language-rust ignore">impl&lt;T&gt; Mutex&lt;T&gt; {
    pub fn lock(&amp;self) -&gt; Option&lt;Guard&lt;'_, T&gt;&gt; {
        match self.locked.compare_exchange(
            false,
            true,
            atomic::Ordering::Acquire,
            atomic::Ordering::Relaxed,
        ) {
            Ok(_) =&gt; Some(Guard(self)),
            Err(_) =&gt; None,
        }
    }
}

impl&lt;T&gt; Drop for Guard&lt;'_, T&gt; {
    fn drop(&amp;mut self) {
        self.0.locked.store(false, atomic::Ordering::Release);
    }
}</code></pre>
<p>Note that similarly to how atomic operations only make sense when paired with
other atomic operations on the same locations, <code>Acquire</code> only makes sense when
paired with <code>Release</code> and vice versa. That is, both an <code>Acquire</code> with no
corresponding <code>Release</code> and a <code>Release</code> with no corresponding <code>Acquire</code> are
useless, since the arrows will be unable to connect to anything.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="seqcst"><a class="header" href="#seqcst">SeqCst</a></h1>
<p><code>SeqCst</code> is probably the most interesting ordering, because it is simultaneously
the simplest and most complex atomic memory ordering in existence. It’s
simple, because if you do only use <code>SeqCst</code> everywhere then you can kind of
maybe pretend like the Abstract Machine has a concept of time; phrases like
“latest value” make sense, the program can be thought of as a set of steps that
interleave, there is a universal “now” and “before” and wouldn’t that be nice?
But it’s also the most complex, because as soon as look under the hood you
realize just how incredibly convoluted and hard to follow the actual rules
behind it are, and it gets really ugly really fast as soon as you try to mix it
with any other ordering.</p>
<p>To understand <code>SeqCst</code>, we first have to understand the problem it exists to
solve. A simple example used to show where weaker orderings produce
counterintuitive results is this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicBool};
</span>use std::thread;

// Set this to Relaxed, Acquire, Release, AcqRel, doesn’t matter — the result is
// the same (modulo panics caused by attempting acquire stores or release
// loads).
const ORDERING: atomic::Ordering = atomic::Ordering::Relaxed;

static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let a = thread::spawn(|| { X.store(true, ORDERING); Y.load(ORDERING) });
let b = thread::spawn(|| { Y.store(true, ORDERING); X.load(ORDERING) });

let a = a.join().unwrap();
let b = b.join().unwrap();

<span class="boring">return;
</span>// This assert is allowed to fail.
assert!(a || b);
<span class="boring">}</span></code></pre></pre>
<p>The basic setup of this code, for all of its possible executions, looks like
this:</p>
<pre><code class="language-text">     a        static X    static Y         b
╭─────────╮   ┌───────┐   ┌───────┐   ╭─────────╮
│ store X ├─┐ │ false │   │ false │ ┌─┤ store Y │
╰────╥────╯ │ └───────┘   └───────┘ │ ╰────╥────╯
╭────⇓────╮ └─┬───────┐   ┌───────┬─┘ ╭────⇓────╮
│ load Y  ├─? │ true  │   │ true  │ ?─┤ load X  │
╰─────────╯   └───────┘   └───────┘   ╰─────────╯
</code></pre>
<p>In other words, <code>a</code> and <code>b</code> are guaranteed to store <code>true</code> into <code>X</code> and <code>Y</code>
respectively, and then attempt to load from the other thread’s atomic. The
question now is: is it possible for them <em>both</em> to load <code>false</code>?</p>
<p>And looking at this diagram, there’s absolutely no reason why not. There isn’t
even a single arrow connecting the left and right hand sides so far, so the
loads have no coherence-based restrictions on which values they are allowed to
pick, and we could end up with an execution like this:</p>
<pre><code class="language-text">     a        static X    static Y         b
╭─────────╮   ┌───────┐   ┌───────┐   ╭─────────╮
│ store X ├┐  │ false ├─┐┌┤ false │  ┌┤ store Y │
╰────╥────╯│  └───────┘┌─┘└───────┘  │╰────╥────╯
     ║     │ ┌─────────┘└───────────┐│     ║
╭────⇓────╮└─│┬───────┐   ┌───────┬─│┘╭────⇓────╮
│ load Y  ├──┘│ true  │   │ true  │ └─┤ load X  │
╰─────────╯   └───────┘   └───────┘   ╰─────────╯
</code></pre>
<p>Which results in a failed assert. This execution is brought about because the
model of separate modification orders means that there is no relative ordering
between <code>X</code> and <code>Y</code> being changed, and so each thread is allowed to “see” either
order. However, some algorithms will require a globally agreed-upon ordering,
and this is where <code>SeqCst</code> can come in useful.</p>
<p>This ordering, first and foremost, inherits the guarantees from all the other
orderings — it is an acquire operation for loads, a release operation for stores
and an acquire-release operation for RMWs. In addition to this, it gives some
guarantees unique to <code>SeqCst</code> about what values it is allowed to load. Note that
these guarantees are not about preventing data races: unless you have some
unrelated code that triggers a data race given an unexpected condition, using
<code>SeqCst</code> can only prevent you from race conditions because its guarantees only
apply to other <code>SeqCst</code> operations rather than all data accesses.</p>
<h2 id="s"><a class="header" href="#s">S</a></h2>
<p><code>SeqCst</code> is fundamentally about <em>S</em>, which is the global ordering of all
<code>SeqCst</code> operations in an execution of the program. It is consistent between
every atomic and every thread, and all stores, fences and RMWs that use a
sequentially consistent ordering have a place in it (but no other operations
do). It is in contrast to modification orders, which are similarly total but
only scoped to a single atomic rather than the whole program.</p>
<p>Other than an edge case involving <code>SeqCst</code> mixed with weaker orderings (detailed
later on), <em>S</em> is primarily controlled by the happens-before relations in a
program: this means that if an action <em>A</em> happens-before an action <em>B</em>, it is
also guaranteed to appear before <em>B</em> in <em>S</em>. Other than that restriction, <em>S</em> is
unspecified and will be chosen arbitrarily during execution.</p>
<p>Once a particular <em>S</em> has been established, every atomic’s modification order is
then guaranteed to be consistent with it, so a <code>SeqCst</code> load will never see a
value that has been overwritten by a write that occurred before it in <em>S</em>, or a
value that has been written by a write that occured after it in <em>S</em> (note that a
<code>Relaxed</code>/<code>Acquire</code> load however might, since there is no “before” or “after” as
it is not in <em>S</em> in the first place).</p>
<p>More formally, this guarantee can be described with <em>coherence orderings</em>, a
relation which expresses which of two operations appears before the other in an
atomic’s modification order. It is said that an operation <em>A</em> is
<em>coherence-ordered-before</em> another operation <em>B</em> if any of the following
conditions are met:</p>
<ol>
<li><em>A</em> is a store or RMW, <em>B</em> is a store or RMW, and <em>A</em> appears before <em>B</em> in
the modification order.</li>
<li><em>A</em> is a store or RMW, <em>B</em> is a load, and <em>B</em> reads the value stored by <em>A</em>.</li>
<li><em>A</em> is a load, <em>B</em> is a store or RMW, and <em>A</em> takes its value from a place in
the modification order that appears before <em>B</em>.</li>
<li><em>A</em> is coherence-ordered-before a different operation <em>X</em>, and <em>X</em> is
coherence-ordered-before <em>B</em> (the basic transitivity property).</li>
</ol>
<p>The following diagram gives examples for the main three rules (in each case <em>A</em>
is coherence-ordered-before <em>B</em>):</p>
<pre><code class="language-text">        Rule 1        ┃         Rule 2        ┃         Rule 3
                      ┃                       ┃
╭───╮ ┌─┬───┐   ╭───╮ ┃ ╭───╮ ┌─┬───┐   ╭───╮ ┃ ╭───╮   ┌───┐   ╭───╮
│ A ├─┘ │   │ ┌─┤ B │ ┃ │ A ├─┘ │   ├───┤ B │ ┃ │ A ├───┤   │ ┌─┤ B │
╰───╯   └───┘ │ ╰───╯ ┃ ╰───╯   └───┘   ╰───╯ ┃ ╰───╯   └───┘ │ ╰───╯
        ┌───┬─┘       ┃                       ┃         ┌───┬─┘
        │   │         ┃                       ┃         │   │
        └───┘         ┃                       ┃         └───┘
</code></pre>
<p>The only important thing to note is that for two loads of the same value in the
modification order, neither is coherence-ordered-before the other, as in the
following example where <em>A</em> has no coherence ordering relation to <em>B</em>:</p>
<pre><code class="language-text">╭───╮   ┌───┐   ╭───╮
│ A ├───┤   ├───┤ B │
╰───╯   └───┘   ╰───╯
</code></pre>
<p>Because of this, “<em>A</em> is coherence-ordered-before <em>B</em>” is subtly different from
“<em>A</em> is not coherence-ordered-after <em>B</em>”; only the latter phrase includes the
above situation, and is synonymous with “either <em>A</em> is coherence-ordered-before
<em>B</em> or <em>A</em> and <em>B</em> are both loads, and see the same value in the atomic’s
modification order”. “Not coherence-ordered-after” is generally a more useful
relation than “coherence-ordered-before”, and so it’s important to understand
what it means.</p>
<p>With this terminology applied, we can use a more precise definition of
<code>SeqCst</code>’s guarantee: for two <code>SeqCst</code> operations on the same atomic <em>A</em> and
<em>B</em>, where <em>A</em> precedes <em>B</em> in <em>S</em>, <em>A</em> is not coherence-ordered-after <em>B</em>.
Effectively, this one rule ensures that <em>S</em>’s order “propagates”
throughout all the atomics of the program — you can imagine each operation in
<em>S</em> as storing a snapshot of the world, so that every subsequent operation is
consistent with it.</p>
<h2 id="applying-seqcst"><a class="header" href="#applying-seqcst">Applying <code>SeqCst</code></a></h2>
<p>So, looking back at our program, let’s consider how we could use <code>SeqCst</code> to
make that execution invalid. As a refresher, here’s the framework for every
possible execution of the program:</p>
<pre><code class="language-text">     a        static X    static Y         b
╭─────────╮   ┌───────┐   ┌───────┐   ╭─────────╮
│ store X ├─┐ │ false │   │ false │ ┌─┤ store Y │
╰────╥────╯ │ └───────┘   └───────┘ │ ╰────╥────╯
╭────⇓────╮ └─┬───────┐   ┌───────┬─┘ ╭────⇓────╮
│ load Y  ├─? │ true  │   │ true  │ ?─┤ load X  │
╰─────────╯   └───────┘   └───────┘   ╰─────────╯
</code></pre>
<p>First of all, both the final loads (<code>a</code> and <code>b</code>’s second operations) need to
become <code>SeqCst</code>, because they need to be aware of the total ordering that
determines whether <code>X</code> or <code>Y</code> becomes <code>true</code> first. And secondly, we need to
establish that ordering in the first place, and that needs to be done by making
sure that there is always one operation in <em>S</em> that both sees one of the atomics
as <code>true</code> and precedes both final loads in <em>S</em>, so that the coherence ordering
guarantee will apply (the final loads themselves don’t work for this since
although they “know” that their corresponding atomic is <code>true</code> they don’t
interact with it directly so <em>S</em> doesn’t care) — for this, we must set both
stores to use the <code>SeqCst</code> ordering.</p>
<p>This leaves us with the correct version of the above program, which is
guaranteed to never panic:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicBool};
</span>use std::thread;

const ORDERING: atomic::Ordering = atomic::Ordering::SeqCst;

static X: AtomicBool = AtomicBool::new(false);
static Y: AtomicBool = AtomicBool::new(false);

let a = thread::spawn(|| { X.store(true, ORDERING); Y.load(ORDERING) });
let b = thread::spawn(|| { Y.store(true, ORDERING); X.load(ORDERING) });

let a = a.join().unwrap();
let b = b.join().unwrap();

<span class="boring">return;
</span>// This assert is **not** allowed to fail.
assert!(a || b);
<span class="boring">}</span></code></pre></pre>
<p>As there are four <code>SeqCst</code> operations with a partial order between two pairs in
them (caused by the sequenced-before relation), there are six possible
executions of this program:</p>
<ul>
<li>All of <code>a</code>’s operations precede <code>b</code>’s operations:
<ol>
<li><code>a</code> stores <code>true</code> into <code>X</code></li>
<li><code>a</code> loads <code>Y</code> (gives <code>false</code>)</li>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>b</code> loads <code>X</code> (required to give <code>true</code>)</li>
</ol>
</li>
<li>All of <code>b</code>’s operations precede <code>a</code>’s operations:
<ol>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>b</code> loads <code>X</code> (gives <code>false</code>)</li>
<li><code>a</code> stores <code>true</code> into <code>X</code></li>
<li><code>a</code> loads <code>Y</code> (required to give <code>true</code>)</li>
</ol>
</li>
<li>The stores precede the loads,
<code>a</code>’s store precedes <code>b</code>’s and <code>a</code>’s load precedes <code>b</code>’s:
<ol>
<li><code>a</code> stores <code>true</code> to <code>X</code></li>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>a</code> loads <code>Y</code> (required to give <code>true</code>)</li>
<li><code>b</code> loads <code>X</code> (required to give <code>true</code>)</li>
</ol>
</li>
<li>The stores precede the loads,
<code>a</code>’s store precedes <code>b</code>’s and <code>b</code>’s load precedes <code>a</code>’s:
<ol>
<li><code>a</code> stores <code>true</code> to <code>X</code></li>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>b</code> loads <code>X</code> (required to give <code>true</code>)</li>
<li><code>a</code> loads <code>Y</code> (required to give <code>true</code>)</li>
</ol>
</li>
<li>The stores precede the loads,
<code>b</code>’s store precedes <code>a</code>’s and <code>a</code>’s load precedes <code>b</code>’s:
<ol>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>a</code> stores <code>true</code> to <code>X</code></li>
<li><code>a</code> loads <code>Y</code> (required to give <code>true</code>)</li>
<li><code>b</code> loads <code>X</code> (required to give <code>true</code>)</li>
</ol>
</li>
<li>The stores precede the loads,
<code>b</code>’s store precedes <code>a</code>’s and <code>b</code>’s load precedes <code>a</code>’s:
<ol>
<li><code>b</code> stores <code>true</code> into <code>Y</code></li>
<li><code>a</code> stores <code>true</code> to <code>X</code></li>
<li><code>b</code> loads <code>X</code> (required to give <code>true</code>)</li>
<li><code>a</code> loads <code>Y</code> (required to give <code>true</code>)</li>
</ol>
</li>
</ul>
<p>All the places where the load was required to give <code>true</code> were caused by a
preceding store in <em>S</em> of the same atomic of <code>true</code> — otherwise, the load would
be coherence-ordered-before a store which precedes it in <em>S</em>, and that is
impossible.</p>
<h2 id="the-mixed-seqcst-special-case"><a class="header" href="#the-mixed-seqcst-special-case">The mixed-<code>SeqCst</code> special case</a></h2>
<p>As I’ve been alluding to for a while, I wasn’t being totally truthful when I
said that <em>S</em> is consistent with happens-before relations — in reality, it is
only consistent with <em>strongly happens-before</em> relations, which presents a
subtly-defined subset of happens-before relations. In particular, it excludes
two situations:</p>
<ol>
<li>The <code>SeqCst</code> operation A synchronizes-with an <code>Acquire</code> or <code>AcqRel</code> operation
B which is sequenced-before another <code>SeqCst</code> operation C. Here, despite the
fact that A happens-before C, A does not <em>strongly</em> happen-before C and so is
not guaranteed to precede C in <em>S</em>.</li>
<li>The <code>SeqCst</code> operation A is sequenced-before the <code>Release</code> or <code>AcqRel</code>
operation B, which synchronizes-with another <code>SeqCst</code> operation C. Similarly,
despite the fact that A happens-before C, A might not precede C in <em>S</em>.</li>
</ol>
<p>The first situation is illustrated below, with <code>SeqCst</code> accesses repesented with
asterisks:</p>
<pre><code class="language-text">  t_1       x       t_2
╭─────╮ ┌─↘───┐   ╭─────╮
│ *A* ├─┘ │ 1 ├───→  B  │
╰─────╯   └───┘   ╰──╥──╯
                  ╭──⇓──╮
                  │ *C* │
                  ╰─────╯
</code></pre>
<p>A happens-before, but does not strongly happen-before, C — and anything
sequenced-after C will have the same treatment (unless more synchronization is
used). This means that C is actually allowed to <em>precede</em> A in <em>S</em>, despite
conceptually occuring after it. However, anything sequenced-before A, because
there is at least one sequence on either side of the synchronization, will
strongly happen-before C.</p>
<p>But this is all highly theoretical at the moment, so let’s make an example to
show how that rule can actually affect the execution of code. So, if C were to
precede A in <em>S</em> (and they are not both loads) then that means C is always
coherence-ordered-before A. Let’s say then that C loads from <code>x</code> (the atomic
that A has to access), it may load the value that came before A if it were to
precede A in <em>S</em>:</p>
<pre><code class="language-text">  t_1       x       t_2
╭─────╮   ┌───┐   ╭─────╮
│ *A* ├─┐ │ 0 ├─┐┌→  B  │
╰─────╯ │ └───┘ ││╰──╥──╯
        └─↘───┐┌─┘╭──⇓──╮
          │ 1 ├┘└─→ *C* │
          └───┘   ╰─────╯
</code></pre>
<p>Ah wait no, that doesn’t work because regular coherence still mandates that <code>1</code>
is the only value that can be loaded. In fact, once <code>1</code> is loaded <em>S</em>’s required
consistency with coherence orderings means that A <em>is</em> required to precede C in
<em>S</em> after all.</p>
<p>So somehow, to observe this difference we need to have a <em>different</em> <code>SeqCst</code>
operation, let’s call it E, be the one that loads from <code>x</code>, where C is
guaranteed to precede it in <em>S</em> (so we can observe the “weird” state in between
C and A) but C also doesn’t happen-before it (to avoid coherence getting in the
way) — and to do that, all we have to do is have C appear before a <code>SeqCst</code>
operation D in the modification order of another atomic, but have D be a store
so as to avoid C synchronizing with it, and then our desired load E can simply
be sequenced-after D (this will carry over the “precedes in <em>S</em>” guarantee, but
does not restore the happens-after relation to C since that was already dropped
by having D be a store).</p>
<p>In diagram form, that looks like this:</p>
<pre><code class="language-text">  t_1       x       t_2     helper      t_3
╭─────╮   ┌───┐   ╭─────╮   ┌─────┐   ╭─────╮
│ *A* ├─┐ │ 0 ├┐┌─→  B  │ ┌─┤  0  │ ┌─┤ *D* │
╰─────╯ │ └───┘││ ╰──╥──╯ │ └─────┘ │ ╰──╥──╯
        │      └│────║────│─────────│┐   ║
        └─↘───┐ │ ╭──⇓──╮ │ ┌─────↙─┘│╭──⇓──╮
          │ 1 ├─┘ │ *C* ←─┘ │  1  │  └→ *E* │
          └───┘   ╰─────╯   └─────┘   ╰─────╯

S = C → D → E → A
</code></pre>
<p>C is guaranteed to precede D in <em>S</em>, and D is guaranteed to precede E, but
because this exception means that A is <em>not</em> guaranteed to precede C, it is
totally possible for it to come at the end, resulting in the surprising but
totally valid outcome of E loading <code>0</code> from <code>x</code>. In code, this can be expressed
as the following code <em>not</em> being guaranteed to panic:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{AtomicU8, Ordering::{Acquire, SeqCst}};
</span><span class="boring">return;
</span>static X: AtomicU8 = AtomicU8::new(0);
static HELPER: AtomicU8 = AtomicU8::new(0);

// thread_1
X.store(1, SeqCst); // A

// thread_2
assert_eq!(X.load(Acquire), 1); // B
assert_eq!(HELPER.load(SeqCst), 0); // C

// thread_3
HELPER.store(1, SeqCst); // D
assert_eq!(X.load(SeqCst), 0); // E
<span class="boring">}</span></code></pre></pre>
<p>The second situation listed above has very similar consequences. Its abstract
form is the following execution in which A is not guaranteed to precede C in
<em>S</em>, despite A happening-before C:</p>
<pre><code class="language-text">  t_1       x       t_2
╭─────╮ ┌─↘───┐   ╭─────╮
│ *A* │ │ │ 0 ├───→ *C* │
╰──╥──╯ │ └───┘   ╰─────╯
╭──⇓──╮ │
│  B  ├─┘
╰─────╯
</code></pre>
<p>Similarly to before, we can’t just have A access <code>x</code> to show why A not
necessarily preceding C in <em>S</em> matters; instead, we have to introduce a second
atomic and third thread to break the happens-before chain first. And finally, a
single relaxed load F at the end is added just to prove that the weird execution
actually happened (leaving <code>x</code> as 2 instead of 1).</p>
<pre><code class="language-text">  t_3     helper      t_1       x       t_2
╭─────╮   ┌─────┐   ╭─────╮   ┌───┐   ╭─────╮
│ *D* ├┐┌─┤  0  │ ┌─┤ *A* │   │ 0 │ ┌─→ *C* │
╰──╥──╯││ └─────┘ │ ╰──╥──╯   └───┘ │ ╰──╥──╯
   ║   └│─────────│────║─────┐      │    ║
╭──⇓──╮ │ ┌─────↙─┘ ╭──⇓──╮ ┌─↘───┐ │ ╭──⇓──╮
│ *E* ←─┘ │  1  │   │  B  ├─┘││ 1 ├─┘┌┤  F  │
╰─────╯   └─────┘   ╰─────╯  │└───┘  │╰─────╯
                             └↘───┐  │
                              │ 2 ├──┘
                              └───┘
S = C → D → E → A
</code></pre>
<p>This execution mandates both C preceding A in <em>S</em> and A happening-before C,
something that is only possible through these two mixed-<code>SeqCst</code> special
exceptions. It can be expressed in code as well:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{AtomicU8, Ordering::{Release, Relaxed, SeqCst}};
</span><span class="boring">return;
</span>static X: AtomicU8 = AtomicU8::new(0);
static HELPER: AtomicU8 = AtomicU8::new(0);

// thread_3
X.store(2, SeqCst); // D
assert_eq!(HELPER.load(SeqCst), 0); // E

// thread_1
HELPER.store(1, SeqCst); // A
X.store(1, Release); // B

// thread_2
assert_eq!(X.load(SeqCst), 1); // C
assert_eq!(X.load(Relaxed), 2); // F
<span class="boring">}</span></code></pre></pre>
<p>If this seems ridiculously specific and obscure, that’s because it is.
Originally, back in C++11, this special case didn’t exist — but then six years
later it was discovered that in practice atomics on Power, Nvidia GPUs and
sometimes ARMv7 <em>would</em> have this special case, and fixing the implementations
would make atomics significantly slower. So instead, in C++20 they simply
encoded it into the specification.</p>
<p>Generally however, this rule is so complex it’s best to just avoid it entirely
by never mixing <code>SeqCst</code> and non-<code>SeqCst</code> on a single atomic in the first place.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fences"><a class="header" href="#fences">Fences</a></h1>
<p>As well as loads, stores, and RMWs, there is one more kind of atomic operation
to be aware of: fences. Fences can be triggered by the
<a href="https://doc.rust-lang.org/stable/core/sync/atomic/fn.fence.html"><code>core::sync::atomic::fence</code></a> function, which accepts a single ordering
parameter and returns nothing. They don’t do anything on their own, but can be
thought of as events that strengthen the ordering of nearby atomic operations.</p>
<h2 id="acquire-fences"><a class="header" href="#acquire-fences">Acquire fences</a></h2>
<p>The most common kind of fence is an <em>acquire fence</em>, which can be triggered in
three different ways:</p>
<ol>
<li><code>atomic::fence(atomic::Ordering::Acquire)</code></li>
<li><code>atomic::fence(atomic::Ordering::AcqRel)</code></li>
<li><code>atomic::fence(atomic::Ordering::SeqCst)</code></li>
</ol>
<p>An acquire fence retroactively makes every single non-<code>Acquire</code> operation that
was sequenced-before it act like an <code>Acquire</code> operation that occurred at the
fence — in other words, it causes every prior <code>Release</code>d value that was
previously loaded on the thread to synchronize-with the fence. For example, the
following code:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>static X: AtomicU32 = AtomicU32::new(0);

// t_1
X.store(1, atomic::Ordering::Release);

// t_2
let value = X.load(atomic::Ordering::Relaxed);
atomic::fence(atomic::Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>Can result in two possible executions:</p>
<pre><code class="language-text">      Possible Execution 1      ┃      Possible Execution 2
                                ┃
   t_1        X        t_2      ┃      t_1        X        t_2
╭───────╮   ┌───┐   ╭───────╮   ┃   ╭───────╮   ┌───┐   ╭───────╮
│ store ├─┐ │ 0 │ ┌─┤ load  │   ┃   │ store ├─┐ │ 0 ├───┤ load  │
╰───────╯ │ └───┘ │ ╰───╥───╯   ┃   ╰───────╯ │ └───┘   ╰───╥───╯
          └─↘───┐ │ ╭───⇓───╮   ┃             └─↘───┐   ╭───⇓───╮
            │ 1 ├─┘┌→ fence │   ┃               │ 1 │   │ fence │
            └───┴──┘╰───────╯   ┃               └───┘   ╰───────╯
</code></pre>
<p>In the first execution, <code>t_1</code>’s store synchronizes-with and therefore
happens-before <code>t_2</code>’s fence due to the prior load, but note that it does <em>not</em>
happen-before <code>t_2</code>’s load.</p>
<p>Acquire fences work on any number of atomics, and on release sequences too. A
more complex example is as follows:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>static X: AtomicU32 = AtomicU32::new(0);
static Y: AtomicU32 = AtomicU32::new(0);

// t_1
X.store(1, atomic::Ordering::Release);
X.fetch_add(1, atomic::Ordering::Relaxed);

// t_2
Y.store(1, atomic::Ordering::Release);

// t_3
let x = X.load(atomic::Ordering::Relaxed);
let y = Y.load(atomic::Ordering::Relaxed);
atomic::fence(atomic::Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>This can result in an execution like so:</p>
<pre><code class="language-text">   t_1        X        t_3        Y        t_2
╭───────╮   ┌───┐   ╭───────╮   ┌───┐   ╭───────╮
│ store ├─┐ │ 0 │ ┌─┤ load  │   │ 0 │ ┌─┤ store │
╰───╥───╯ │ └───┘ │ ╰───╥───╯   └───┘ │ ╰───────╯
╭───⇓───╮ └─↘───┐ │ ╭───⇓───╮   ┌───↙─┘
│  rmw  ├─┐ │ 1 │ │ │ load  ├───┤ 1 │
╰───────╯ │ └─┬─┘ │ ╰───╥───╯ ┌─┴───┘
          └─┬─↓─┐ │ ╭───⇓───╮ │
            │ 2 ├─┘┌→ fence ←─┘
            └───┴──┘╰───────╯
</code></pre>
<p>There are two common scenarios in which acquire fences are used:</p>
<ol>
<li>When an <code>Acquire</code> ordering is only necessary when a specific value is loaded.
For example, you may only wish to acquire when an <code>initialized</code> boolean is
<code>true</code>, since otherwise you won’t be reading the shared state at all. In
this case, you can load with a <code>Relaxed</code> ordering and then issue an
<code>Acquire</code> fence afterward only if that condition is met, which can aid in
performance sometimes (since the acquire operation is avoided when
<code>initialized == false</code>).</li>
<li>When several <code>Acquire</code> operations on different locations need to be performed
in a row, but individually each operation doesn’t need <code>Acquire</code> ordering;
it is often faster to perform all the loads as <code>Relaxed</code> first and use a
single <code>Acquire</code> fence at the end then it is to make each one separately use
<code>Acquire</code>.</li>
</ol>
<h2 id="release-fences"><a class="header" href="#release-fences">Release fences</a></h2>
<p>Release fences are the natural complement to acquire fences, and they similarly
can be triggered in three different ways:</p>
<ol>
<li><code>atomic::fence(atomic::Ordering::Release)</code></li>
<li><code>atomic::fence(atomic::Ordering::AcqRel)</code></li>
<li><code>atomic::fence(atomic::Ordering::SeqCst)</code></li>
</ol>
<p>Release fences convert every subsequent atomic access in the same thread into a
release operation that has its arrow starting from the fence — in other words,
every <code>Acquire</code> operation that sees a value that was written by the fence’s
thread after the release fence will synchronize-with the release fence. For
example, the following code:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>static X: AtomicU32 = AtomicU32::new(0);

// t_1
atomic::fence(atomic::Ordering::Release);
X.store(1, atomic::Ordering::Relaxed);

// t_2
X.load(atomic::Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>Can result in this execution:</p>
<pre><code class="language-text">   t_1        X        t_2
╭───────╮   ┌───┐   ╭───────╮
│ fence ├─┐ │ 0 │ ┌─→ load  │
╰───╥───╯ │ └───┘ │ ╰───────╯
╭───⇓───╮ └─↘───┐ │
│ store ├───┤ 1 ├─┘
╰───────╯   └───┘
</code></pre>
<p>As well as it being possible for a release fence to synchronize-with an acquire
load (fence–atomic synchronization) and a release store to synchronize-with an
acquire fence (atomic–fence synchronization), it is also possible for release
fences to synchronize with acquire fences (fence–fence synchronization). In this
code snippet, only fences and <code>Relaxed</code> operations are used to establish a
happens-before relation (in some executions):</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::{self, AtomicU32};
</span>static X: AtomicU32 = AtomicU32::new(0);

// t_1
atomic::fence(atomic::Ordering::Release);
X.store(1, atomic::Ordering::Relaxed);

// t_2
X.load(atomic::Ordering::Relaxed);
atomic::fence(atomic::Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>The execution with the relation looks like this:</p>
<pre><code class="language-text">   t_1        X        t_2
╭───────╮   ┌───┐   ╭───────╮
│ fence ├─┐ │ 0 │ ┌─┤ load  │
╰───╥───╯ │ └───┘ │ ╰───╥───╯
╭───⇓───╮ └─↘───┐ │ ╭───⇓───╮
│ store ├───┤ 1 ├─┘┌→ fence │
╰───────╯   └───┴──┘╰───────╯
</code></pre>
<p>Like with acquire fences, release fences can be used to optimize over a series
of atomic stores that don’t individually need to be <code>Release</code>, since in some
conditions and on some architectures it’s faster to put a single release fence
at the start and use <code>Relaxed</code> from that point on than it is to use <code>Release</code>
every time.</p>
<h2 id="acqrel-fences"><a class="header" href="#acqrel-fences"><code>AcqRel</code> fences</a></h2>
<p><code>AcqRel</code> fences are just the combined behaviour of an <code>Acquire</code> fence and a
<code>Release</code> fence in one operation. There isn’t much special to note about them,
other than that they behave more like an acquire fence followed by a release
fence than the other way around, which is useful to know in situations like the
following:</p>
<pre><code class="language-text">   t_1        X        t_2        Y        t_3
╭───────╮   ┌───┐   ╭───────╮   ┌───┐   ╭───────╮
│   A   │   │ 0 │ ┌─┤ load  │   │ 0 │ ┌─→ load  │
╰───╥───╯   └───┘ │ ╰───╥───╯   └───┘ │ ╰───╥───╯
╭───⇓───╮ ┌─↘───┐ │ ╭───⇓───╮┌──↘───┐ │ ╭───⇓───╮
│ store ├─┘ │ 1 ├─┘┌→ fence ├┘┌─┤ 1 ├─┘ │   B   │
╰───────╯   └───┴──┘╰───╥───╯ │ └───┘   ╰───────╯
                    ╭───⇓───╮ │
                    │ store ├─┘
                    ╰───────╯
</code></pre>
<p>Here, A happens-before B, which is singularly due to the <code>AcqRel</code> fence’s
ability to “carry over” happens-before relations within itself.</p>
<h2 id="seqcst-fences"><a class="header" href="#seqcst-fences"><code>SeqCst</code> fences</a></h2>
<p><code>SeqCst</code> fences are the strongest kind of fence. They first of all inherit the
behaviour from an <code>AcqRel</code> fence, meaning they have both acquire and release
semantics at the same time, but being <code>SeqCst</code> operations they also participate
in <em>S</em>. Just as with all other <code>SeqCst</code> operations, their placement in <em>S</em> is
primarily determined by strongly happens-before relations (including the
<a href="atomics/seqcst.html#the-mixed-seqcst-special-case">mixed-<code>SeqCst</code> caveat</a> that comes with it), which then gives additional
guarantees to your code.</p>
<p>Namely, the power of <code>SeqCst</code> fences can be summarized in three points:</p>
<ul>
<li>Everything that happens-before a <code>SeqCst</code> fence is not coherence-ordered-after
any <code>SeqCst</code> operation that the fence precedes in <em>S</em>.</li>
<li>Everything that happens-after a <code>SeqCst</code> fence is not coherence-ordered-before
any <code>SeqCst</code> operation that the fence succeeds in <em>S</em>.</li>
<li>Everything that happens-before a <code>SeqCst</code> fence X is not
coherence-ordered-after anything that happens-after another <code>SeqCst</code> fence
Y, if X preceeds Y in <em>S</em>.</li>
</ul>
<blockquote>
<p>In C++11, the above three statements were similar, except they only talked
about what was sequenced-before and sequenced-after the <code>SeqCst</code> fences; C++20
strengthened this to also include happens-before, because in practice this
theoretical optimization was not being exploited by anybody. However do note
that as of the time of writing, <a href="https://github.com/rust-lang/miri/issues/2301">Miri only implements the old, weaker
semantics</a> and so you may see false positives when testing with
it.</p>
</blockquote>
<p>The “motivating use-case” for <code>SeqCst</code> demonstrated in the <code>SeqCst</code> chapter can
also be rewritten to use exclusively <code>SeqCst</code> fences and <code>Relaxed</code> operations,
by inserting fences in between the operations in the two threads:</p>
<pre><code class="language-text">     a        static X    static Y         b
╭─────────╮   ┌───────┐   ┌───────┐   ╭─────────╮
│ store X ├─┐ │ false │   │ false │ ┌─┤ store Y │
╰────╥────╯ │ └───────┘   └───────┘ │ ╰────╥────╯
╭────⇓────╮ └─┬───────┐   ┌───────┬─┘ ╭────⇓────╮
│ *fence* │   │ true  │   │ true  │   │ *fence* │
╰────╥────╯   └───────┘   └───────┘   ╰────╥────╯
╭────⇓────╮                           ╭────⇓────╮
│ load Y  ├─?                       ?─┤ load X  │
╰─────────╯                           ╰─────────╯
</code></pre>
<p>There are two executions to consider here, depending on which way round the
fences appear in <em>S</em>. Should <code>a</code>’s fence appear first, the fence–fence <code>SeqCst</code>
guarantee tells us that <code>b</code>’s load of <code>X</code> is not coherence-ordered-after <code>a</code>’s
store of <code>X</code>, which forbids <code>b</code>’s load of <code>X</code> from seeing the value <code>false</code>. The
same logic can be applied should the fences appear the other way around, proving
that at least one thread must load <code>true</code> in the end.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-implementing-vec"><a class="header" href="#example-implementing-vec">Example: Implementing Vec</a></h1>
<p>To bring everything together, we're going to write <code>std::Vec</code> from scratch.
We will limit ourselves to stable Rust. In particular we won't use any
intrinsics that could make our code a little bit nicer or efficient because
intrinsics are permanently unstable. Although many intrinsics <em>do</em> become
stabilized elsewhere (<code>std::ptr</code> and <code>std::mem</code> consist of many intrinsics).</p>
<p>Ultimately this means our implementation may not take advantage of all
possible optimizations, though it will be by no means <em>naive</em>. We will
definitely get into the weeds over nitty-gritty details, even
when the problem doesn't <em>really</em> merit it.</p>
<p>You wanted advanced. We're gonna go advanced.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layout"><a class="header" href="#layout">Layout</a></h1>
<p>First off, we need to come up with the struct layout. A Vec has three parts:
a pointer to the allocation, the size of the allocation, and the number of
elements that have been initialized.</p>
<p>Naively, this means we just want this design:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub struct Vec&lt;T&gt; {
    ptr: *mut T,
    cap: usize,
    len: usize,
}</code></pre>
<p>And indeed this would compile. Unfortunately, it would be too strict. The
compiler will give us too strict variance. So a <code>&amp;Vec&lt;&amp;'static str&gt;</code>
couldn't be used where a <code>&amp;Vec&lt;&amp;'a str&gt;</code> was expected. See <a href="vec/../ownership.html">the chapter
on ownership and lifetimes</a> for all the details on variance.</p>
<p>As we saw in the ownership chapter, the standard library uses <code>Unique&lt;T&gt;</code> in place of
<code>*mut T</code> when it has a raw pointer to an allocation that it owns. Unique is unstable,
so we'd like to not use it if possible, though.</p>
<p>As a recap, Unique is a wrapper around a raw pointer that declares that:</p>
<ul>
<li>We are covariant over <code>T</code></li>
<li>We may own a value of type <code>T</code> (this is not relevant for our example here, but see
<a href="vec/../phantom-data.html">the chapter on PhantomData</a> on why the real <code>std::vec::Vec&lt;T&gt;</code> needs this)</li>
<li>We are Send/Sync if <code>T</code> is Send/Sync</li>
<li>Our pointer is never null (so <code>Option&lt;Vec&lt;T&gt;&gt;</code> is null-pointer-optimized)</li>
</ul>
<p>We can implement all of the above requirements in stable Rust. To do this, instead
of using <code>Unique&lt;T&gt;</code> we will use <a href="vec/../../std/ptr/struct.NonNull.html"><code>NonNull&lt;T&gt;</code></a>, another wrapper around a
raw pointer, which gives us two of the above properties, namely it is covariant
over <code>T</code> and is declared to never be null. By implementing Send/Sync if <code>T</code> is,
we get the same results as using <code>Unique&lt;T&gt;</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use std::ptr::NonNull;

pub struct Vec&lt;T&gt; {
    ptr: NonNull&lt;T&gt;,
    cap: usize,
    len: usize,
}

unsafe impl&lt;T: Send&gt; Send for Vec&lt;T&gt; {}
unsafe impl&lt;T: Sync&gt; Sync for Vec&lt;T&gt; {}
<span class="boring">fn main() {}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="allocating-memory"><a class="header" href="#allocating-memory">Allocating Memory</a></h1>
<p>Using <code>NonNull</code> throws a wrench in an important feature of Vec (and indeed all of
the std collections): creating an empty Vec doesn't actually allocate at all. This
is not the same as allocating a zero-sized memory block, which is not allowed by
the global allocator (it results in undefined behavior!). So if we can't allocate,
but also can't put a null pointer in <code>ptr</code>, what do we do in <code>Vec::new</code>? Well, we
just put some other garbage in there!</p>
<p>This is perfectly fine because we already have <code>cap == 0</code> as our sentinel for no
allocation. We don't even need to handle it specially in almost any code because
we usually need to check if <code>cap &gt; len</code> or <code>len &gt; 0</code> anyway. The recommended
Rust value to put here is <code>mem::align_of::&lt;T&gt;()</code>. <code>NonNull</code> provides a convenience
for this: <code>NonNull::dangling()</code>. There are quite a few places where we'll
want to use <code>dangling</code> because there's no real allocation to talk about but
<code>null</code> would make the compiler do bad things.</p>
<p>So:</p>
<!-- ignore: explanation code -->
<pre><code class="language-rust ignore">use std::mem;

impl&lt;T&gt; Vec&lt;T&gt; {
    pub fn new() -&gt; Self {
        assert!(mem::size_of::&lt;T&gt;() != 0, "We're not ready to handle ZSTs");
        Vec {
            ptr: NonNull::dangling(),
            len: 0,
            cap: 0,
        }
    }
}
<span class="boring">fn main() {}</span></code></pre>
<p>I slipped in that assert there because zero-sized types will require some
special handling throughout our code, and I want to defer the issue for now.
Without this assert, some of our early drafts will do some Very Bad Things.</p>
<p>Next we need to figure out what to actually do when we <em>do</em> want space. For that,
we use the global allocation functions <a href="vec/../../alloc/alloc/fn.alloc.html"><code>alloc</code></a>, <a href="vec/../../alloc/alloc/fn.realloc.html"><code>realloc</code></a>,
and <a href="vec/../../alloc/alloc/fn.dealloc.html"><code>dealloc</code></a> which are available in stable Rust in
<a href="vec/../../alloc/alloc/index.html"><code>std::alloc</code></a>. These functions are expected to become deprecated in
favor of the methods of <a href="vec/../../std/alloc/struct.Global.html"><code>std::alloc::Global</code></a> after this type is stabilized.</p>
<p>We'll also need a way to handle out-of-memory (OOM) conditions. The standard
library provides a function <a href="vec/../../alloc/alloc/fn.handle_alloc_error.html"><code>alloc::handle_alloc_error</code></a>,
which will abort the program in a platform-specific manner.
The reason we abort and don't panic is because unwinding can cause allocations
to happen, and that seems like a bad thing to do when your allocator just came
back with "hey I don't have any more memory".</p>
<p>Of course, this is a bit silly since most platforms don't actually run out of
memory in a conventional way. Your operating system will probably kill the
application by another means if you legitimately start using up all the memory.
The most likely way we'll trigger OOM is by just asking for ludicrous quantities
of memory at once (e.g. half the theoretical address space). As such it's
<em>probably</em> fine to panic and nothing bad will happen. Still, we're trying to be
like the standard library as much as possible, so we'll just kill the whole
program.</p>
<p>Okay, now we can write growing. Roughly, we want to have this logic:</p>
<pre><code class="language-text">if cap == 0:
    allocate()
    cap = 1
else:
    reallocate()
    cap *= 2
</code></pre>
<p>But Rust's only supported allocator API is so low level that we'll need to do a
fair bit of extra work. We also need to guard against some special
conditions that can occur with really large allocations or empty allocations.</p>
<p>In particular, <code>ptr::offset</code> will cause us a lot of trouble, because it has
the semantics of LLVM's GEP inbounds instruction. If you're fortunate enough to
not have dealt with this instruction, here's the basic story with GEP: alias
analysis, alias analysis, alias analysis. It's super important to an optimizing
compiler to be able to reason about data dependencies and aliasing.</p>
<p>As a simple example, consider the following fragment of code:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">*x *= 7;
*y *= 3;</code></pre>
<p>If the compiler can prove that <code>x</code> and <code>y</code> point to different locations in
memory, the two operations can in theory be executed in parallel (by e.g.
loading them into different registers and working on them independently).
However the compiler can't do this in general because if x and y point to
the same location in memory, the operations need to be done to the same value,
and they can't just be merged afterwards.</p>
<p>When you use GEP inbounds, you are specifically telling LLVM that the offsets
you're about to do are within the bounds of a single "allocated" entity. The
ultimate payoff being that LLVM can assume that if two pointers are known to
point to two disjoint objects, all the offsets of those pointers are <em>also</em>
known to not alias (because you won't just end up in some random place in
memory). LLVM is heavily optimized to work with GEP offsets, and inbounds
offsets are the best of all, so it's important that we use them as much as
possible.</p>
<p>So that's what GEP's about, how can it cause us trouble?</p>
<p>The first problem is that we index into arrays with unsigned integers, but
GEP (and as a consequence <code>ptr::offset</code>) takes a signed integer. This means
that half of the seemingly valid indices into an array will overflow GEP and
actually go in the wrong direction! As such we must limit all allocations to
<code>isize::MAX</code> elements. This actually means we only need to worry about
byte-sized objects, because e.g. <code>&gt; isize::MAX</code> <code>u16</code>s will truly exhaust all of
the system's memory. However in order to avoid subtle corner cases where someone
reinterprets some array of <code>&lt; isize::MAX</code> objects as bytes, std limits all
allocations to <code>isize::MAX</code> bytes.</p>
<p>On all 64-bit targets that Rust currently supports we're artificially limited
to significantly less than all 64 bits of the address space (modern x64
platforms only expose 48-bit addressing), so we can rely on just running out of
memory first. However on 32-bit targets, particularly those with extensions to
use more of the address space (PAE x86 or x32), it's theoretically possible to
successfully allocate more than <code>isize::MAX</code> bytes of memory.</p>
<p>However since this is a tutorial, we're not going to be particularly optimal
here, and just unconditionally check, rather than use clever platform-specific
<code>cfg</code>s.</p>
<p>The other corner-case we need to worry about is empty allocations. There will
be two kinds of empty allocations we need to worry about: <code>cap = 0</code> for all T,
and <code>cap &gt; 0</code> for zero-sized types.</p>
<p>These cases are tricky because they come
down to what LLVM means by "allocated". LLVM's notion of an
allocation is significantly more abstract than how we usually use it. Because
LLVM needs to work with different languages' semantics and custom allocators,
it can't really intimately understand allocation. Instead, the main idea behind
allocation is "doesn't overlap with other stuff". That is, heap allocations,
stack allocations, and globals don't randomly overlap. Yep, it's about alias
analysis. As such, Rust can technically play a bit fast and loose with the notion of
an allocation as long as it's <em>consistent</em>.</p>
<p>Getting back to the empty allocation case, there are a couple of places where
we want to offset by 0 as a consequence of generic code. The question is then:
is it consistent to do so? For zero-sized types, we have concluded that it is
indeed consistent to do a GEP inbounds offset by an arbitrary number of
elements. This is a runtime no-op because every element takes up no space,
and it's fine to pretend that there's infinite zero-sized types allocated
at <code>0x01</code>. No allocator will ever allocate that address, because they won't
allocate <code>0x00</code> and they generally allocate to some minimal alignment higher
than a byte. Also generally the whole first page of memory is
protected from being allocated anyway (a whole 4k, on many platforms).</p>
<p>However what about for positive-sized types? That one's a bit trickier. In
principle, you can argue that offsetting by 0 gives LLVM no information: either
there's an element before the address or after it, but it can't know which.
However we've chosen to conservatively assume that it may do bad things. As
such we will guard against this case explicitly.</p>
<p><em>Phew</em></p>
<p>Ok with all the nonsense out of the way, let's actually allocate some memory:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::alloc::{self, Layout};

impl&lt;T&gt; Vec&lt;T&gt; {
    fn grow(&amp;mut self) {
        let (new_cap, new_layout) = if self.cap == 0 {
            (1, Layout::array::&lt;T&gt;(1).unwrap())
        } else {
            // This can't overflow since self.cap &lt;= isize::MAX.
            let new_cap = 2 * self.cap;

            // `Layout::array` checks that the number of bytes is &lt;= usize::MAX,
            // but this is redundant since old_layout.size() &lt;= isize::MAX,
            // so the `unwrap` should never fail.
            let new_layout = Layout::array::&lt;T&gt;(new_cap).unwrap();
            (new_cap, new_layout)
        };

        // Ensure that the new allocation doesn't exceed `isize::MAX` bytes.
        assert!(new_layout.size() &lt;= isize::MAX as usize, "Allocation too large");

        let new_ptr = if self.cap == 0 {
            unsafe { alloc::alloc(new_layout) }
        } else {
            let old_layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            let old_ptr = self.ptr.as_ptr() as *mut u8;
            unsafe { alloc::realloc(old_ptr, old_layout, new_layout.size()) }
        };

        // If allocation fails, `new_ptr` will be null, in which case we abort.
        self.ptr = match NonNull::new(new_ptr as *mut T) {
            Some(p) =&gt; p,
            None =&gt; alloc::handle_alloc_error(new_layout),
        };
        self.cap = new_cap;
    }
}
<span class="boring">fn main() {}</span></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="push-and-pop"><a class="header" href="#push-and-pop">Push and Pop</a></h1>
<p>Alright. We can initialize. We can allocate. Let's actually implement some
functionality! Let's start with <code>push</code>. All it needs to do is check if we're
full to grow, unconditionally write to the next index, and then increment our
length.</p>
<p>To do the write we have to be careful not to evaluate the memory we want to write
to. At worst, it's truly uninitialized memory from the allocator. At best it's the
bits of some old value we popped off. Either way, we can't just index to the memory
and dereference it, because that will evaluate the memory as a valid instance of
T. Worse, <code>foo[idx] = x</code> will try to call <code>drop</code> on the old value of <code>foo[idx]</code>!</p>
<p>The correct way to do this is with <code>ptr::write</code>, which just blindly overwrites the
target address with the bits of the value we provide. No evaluation involved.</p>
<p>For <code>push</code>, if the old len (before push was called) is 0, then we want to write
to the 0th index. So we should offset by the old len.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub fn push(&amp;mut self, elem: T) {
    if self.len == self.cap { self.grow(); }

    unsafe {
        ptr::write(self.ptr.as_ptr().add(self.len), elem);
    }

    // Can't fail, we'll OOM first.
    self.len += 1;
}</code></pre>
<p>Easy! How about <code>pop</code>? Although this time the index we want to access is
initialized, Rust won't just let us dereference the location of memory to move
the value out, because that would leave the memory uninitialized! For this we
need <code>ptr::read</code>, which just copies out the bits from the target address and
interprets it as a value of type T. This will leave the memory at this address
logically uninitialized, even though there is in fact a perfectly good instance
of T there.</p>
<p>For <code>pop</code>, if the old len is 1, for example, we want to read out of the 0th
index. So we should offset by the new len.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub fn pop(&amp;mut self) -&gt; Option&lt;T&gt; {
    if self.len == 0 {
        None
    } else {
        self.len -= 1;
        unsafe {
            Some(ptr::read(self.ptr.as_ptr().add(self.len)))
        }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deallocating"><a class="header" href="#deallocating">Deallocating</a></h1>
<p>Next we should implement Drop so that we don't massively leak tons of resources.
The easiest way is to just call <code>pop</code> until it yields None, and then deallocate
our buffer. Note that calling <code>pop</code> is unneeded if <code>T: !Drop</code>. In theory we can
ask Rust if <code>T</code> <code>needs_drop</code> and omit the calls to <code>pop</code>. However in practice
LLVM is <em>really</em> good at removing simple side-effect free code like this, so I
wouldn't bother unless you notice it's not being stripped (in this case it is).</p>
<p>We must not call <code>alloc::dealloc</code> when <code>self.cap == 0</code>, as in this case we
haven't actually allocated any memory.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Drop for Vec&lt;T&gt; {
    fn drop(&amp;mut self) {
        if self.cap != 0 {
            while let Some(_) = self.pop() { }
            let layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            unsafe {
                alloc::dealloc(self.ptr.as_ptr() as *mut u8, layout);
            }
        }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deref"><a class="header" href="#deref">Deref</a></h1>
<p>Alright! We've got a decent minimal stack implemented. We can push, we can
pop, and we can clean up after ourselves. However there's a whole mess of
functionality we'd reasonably want. In particular, we have a proper array, but
none of the slice functionality. That's actually pretty easy to solve: we can
implement <code>Deref&lt;Target=[T]&gt;</code>. This will magically make our Vec coerce to, and
behave like, a slice in all sorts of conditions.</p>
<p>All we need is <code>slice::from_raw_parts</code>. It will correctly handle empty slices
for us. Later once we set up zero-sized type support it will also Just Work
for those too.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::ops::Deref;

impl&lt;T&gt; Deref for Vec&lt;T&gt; {
    type Target = [T];
    fn deref(&amp;self) -&gt; &amp;[T] {
        unsafe {
            std::slice::from_raw_parts(self.ptr.as_ptr(), self.len)
        }
    }
}</code></pre>
<p>And let's do DerefMut too:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::ops::DerefMut;

impl&lt;T&gt; DerefMut for Vec&lt;T&gt; {
    fn deref_mut(&amp;mut self) -&gt; &amp;mut [T] {
        unsafe {
            std::slice::from_raw_parts_mut(self.ptr.as_ptr(), self.len)
        }
    }
}</code></pre>
<p>Now we have <code>len</code>, <code>first</code>, <code>last</code>, indexing, slicing, sorting, <code>iter</code>,
<code>iter_mut</code>, and all other sorts of bells and whistles provided by slice. Sweet!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="insert-and-remove"><a class="header" href="#insert-and-remove">Insert and Remove</a></h1>
<p>Something <em>not</em> provided by slice is <code>insert</code> and <code>remove</code>, so let's do those
next.</p>
<p>Insert needs to shift all the elements at the target index to the right by one.
To do this we need to use <code>ptr::copy</code>, which is our version of C's <code>memmove</code>.
This copies some chunk of memory from one location to another, correctly
handling the case where the source and destination overlap (which will
definitely happen here).</p>
<p>If we insert at index <code>i</code>, we want to shift the <code>[i .. len]</code> to <code>[i+1 .. len+1]</code>
using the old len.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub fn insert(&amp;mut self, index: usize, elem: T) {
    // Note: `&lt;=` because it's valid to insert after everything
    // which would be equivalent to push.
    assert!(index &lt;= self.len, "index out of bounds");
    if self.len == self.cap { self.grow(); }

    unsafe {
        // ptr::copy(src, dest, len): "copy from src to dest len elems"
        ptr::copy(
            self.ptr.as_ptr().add(index),
            self.ptr.as_ptr().add(index + 1),
            self.len - index,
        );
        ptr::write(self.ptr.as_ptr().add(index), elem);
    }

    self.len += 1;
}</code></pre>
<p>Remove behaves in the opposite manner. We need to shift all the elements from
<code>[i+1 .. len + 1]</code> to <code>[i .. len]</code> using the <em>new</em> len.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub fn remove(&amp;mut self, index: usize) -&gt; T {
    // Note: `&lt;` because it's *not* valid to remove after everything
    assert!(index &lt; self.len, "index out of bounds");
    unsafe {
        self.len -= 1;
        let result = ptr::read(self.ptr.as_ptr().add(index));
        ptr::copy(
            self.ptr.as_ptr().add(index + 1),
            self.ptr.as_ptr().add(index),
            self.len - index,
        );
        result
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intoiter"><a class="header" href="#intoiter">IntoIter</a></h1>
<p>Let's move on to writing iterators. <code>iter</code> and <code>iter_mut</code> have already been
written for us thanks to The Magic of Deref. However there's two interesting
iterators that Vec provides that slices can't: <code>into_iter</code> and <code>drain</code>.</p>
<p>IntoIter consumes the Vec by-value, and can consequently yield its elements
by-value. In order to enable this, IntoIter needs to take control of Vec's
allocation.</p>
<p>IntoIter needs to be DoubleEnded as well, to enable reading from both ends.
Reading from the back could just be implemented as calling <code>pop</code>, but reading
from the front is harder. We could call <code>remove(0)</code> but that would be insanely
expensive. Instead we're going to just use ptr::read to copy values out of
either end of the Vec without mutating the buffer at all.</p>
<p>To do this we're going to use a very common C idiom for array iteration. We'll
make two pointers; one that points to the start of the array, and one that
points to one-element past the end. When we want an element from one end, we'll
read out the value pointed to at that end and move the pointer over by one. When
the two pointers are equal, we know we're done.</p>
<p>Note that the order of read and offset are reversed for <code>next</code> and <code>next_back</code>
For <code>next_back</code> the pointer is always after the element it wants to read next,
while for <code>next</code> the pointer is always at the element it wants to read next.
To see why this is, consider the case where every element but one has been
yielded.</p>
<p>The array looks like this:</p>
<pre><code class="language-text">          S  E
[X, X, X, O, X, X, X]
</code></pre>
<p>If E pointed directly at the element it wanted to yield next, it would be
indistinguishable from the case where there are no more elements to yield.</p>
<p>Although we don't actually care about it during iteration, we also need to hold
onto the Vec's allocation information in order to free it once IntoIter is
dropped.</p>
<p>So we're going to use the following struct:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub struct IntoIter&lt;T&gt; {
    buf: NonNull&lt;T&gt;,
    cap: usize,
    start: *const T,
    end: *const T,
}</code></pre>
<p>And this is what we end up with for initialization:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; IntoIterator for Vec&lt;T&gt; {
    type Item = T;
    type IntoIter = IntoIter&lt;T&gt;;
    fn into_iter(self) -&gt; IntoIter&lt;T&gt; {
        // Make sure not to drop Vec since that would free the buffer
        let vec = ManuallyDrop::new(self);

        // Can't destructure Vec since it's Drop
        let ptr = vec.ptr;
        let cap = vec.cap;
        let len = vec.len;

        IntoIter {
            buf: ptr,
            cap,
            start: ptr.as_ptr(),
            end: if cap == 0 {
                // can't offset off this pointer, it's not allocated!
                ptr.as_ptr()
            } else {
                unsafe { ptr.as_ptr().add(len) }
            },
        }
    }
}</code></pre>
<p>Here's iterating forward:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Iterator for IntoIter&lt;T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                let result = ptr::read(self.start);
                self.start = self.start.offset(1);
                Some(result)
            }
        }
    }

    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) {
        let len = (self.end as usize - self.start as usize)
                  / mem::size_of::&lt;T&gt;();
        (len, Some(len))
    }
}</code></pre>
<p>And here's iterating backwards.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; DoubleEndedIterator for IntoIter&lt;T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                self.end = self.end.offset(-1);
                Some(ptr::read(self.end))
            }
        }
    }
}</code></pre>
<p>Because IntoIter takes ownership of its allocation, it needs to implement Drop
to free it. However it also wants to implement Drop to drop any elements it
contains that weren't yielded.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Drop for IntoIter&lt;T&gt; {
    fn drop(&amp;mut self) {
        if self.cap != 0 {
            // drop any remaining elements
            for _ in &amp;mut *self {}
            let layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            unsafe {
                alloc::dealloc(self.buf.as_ptr() as *mut u8, layout);
            }
        }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rawvec"><a class="header" href="#rawvec">RawVec</a></h1>
<p>We've actually reached an interesting situation here: we've duplicated the logic
for specifying a buffer and freeing its memory in Vec and IntoIter. Now that
we've implemented it and identified <em>actual</em> logic duplication, this is a good
time to perform some logic compression.</p>
<p>We're going to abstract out the <code>(ptr, cap)</code> pair and give them the logic for
allocating, growing, and freeing:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">struct RawVec&lt;T&gt; {
    ptr: NonNull&lt;T&gt;,
    cap: usize,
}

unsafe impl&lt;T: Send&gt; Send for RawVec&lt;T&gt; {}
unsafe impl&lt;T: Sync&gt; Sync for RawVec&lt;T&gt; {}

impl&lt;T&gt; RawVec&lt;T&gt; {
    fn new() -&gt; Self {
        assert!(mem::size_of::&lt;T&gt;() != 0, "TODO: implement ZST support");
        RawVec {
            ptr: NonNull::dangling(),
            cap: 0,
        }
    }

    fn grow(&amp;mut self) {
        // This can't overflow because we ensure self.cap &lt;= isize::MAX.
        let new_cap = if self.cap == 0 { 1 } else { 2 * self.cap };

        // Layout::array checks that the number of bytes is &lt;= usize::MAX,
        // but this is redundant since old_layout.size() &lt;= isize::MAX,
        // so the `unwrap` should never fail.
        let new_layout = Layout::array::&lt;T&gt;(new_cap).unwrap();

        // Ensure that the new allocation doesn't exceed `isize::MAX` bytes.
        assert!(new_layout.size() &lt;= isize::MAX as usize, "Allocation too large");

        let new_ptr = if self.cap == 0 {
            unsafe { alloc::alloc(new_layout) }
        } else {
            let old_layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            let old_ptr = self.ptr.as_ptr() as *mut u8;
            unsafe { alloc::realloc(old_ptr, old_layout, new_layout.size()) }
        };

        // If allocation fails, `new_ptr` will be null, in which case we abort.
        self.ptr = match NonNull::new(new_ptr as *mut T) {
            Some(p) =&gt; p,
            None =&gt; alloc::handle_alloc_error(new_layout),
        };
        self.cap = new_cap;
    }
}

impl&lt;T&gt; Drop for RawVec&lt;T&gt; {
    fn drop(&amp;mut self) {
        if self.cap != 0 {
            let layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            unsafe {
                alloc::dealloc(self.ptr.as_ptr() as *mut u8, layout);
            }
        }
    }
}</code></pre>
<p>And change Vec as follows:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub struct Vec&lt;T&gt; {
    buf: RawVec&lt;T&gt;,
    len: usize,
}

impl&lt;T&gt; Vec&lt;T&gt; {
    fn ptr(&amp;self) -&gt; *mut T {
        self.buf.ptr.as_ptr()
    }

    fn cap(&amp;self) -&gt; usize {
        self.buf.cap
    }

    pub fn new() -&gt; Self {
        Vec {
            buf: RawVec::new(),
            len: 0,
        }
    }

    // push/pop/insert/remove largely unchanged:
    // * `self.ptr.as_ptr() -&gt; self.ptr()`
    // * `self.cap -&gt; self.cap()`
    // * `self.grow() -&gt; self.buf.grow()`
}

impl&lt;T&gt; Drop for Vec&lt;T&gt; {
    fn drop(&amp;mut self) {
        while let Some(_) = self.pop() {}
        // deallocation is handled by RawVec
    }
}</code></pre>
<p>And finally we can really simplify IntoIter:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub struct IntoIter&lt;T&gt; {
    _buf: RawVec&lt;T&gt;, // we don't actually care about this. Just need it to live.
    start: *const T,
    end: *const T,
}

// next and next_back literally unchanged since they never referred to the buf

impl&lt;T&gt; Drop for IntoIter&lt;T&gt; {
    fn drop(&amp;mut self) {
        // only need to ensure all our elements are read;
        // buffer will clean itself up afterwards.
        for _ in &amp;mut *self {}
    }
}

impl&lt;T&gt; IntoIterator for Vec&lt;T&gt; {
    type Item = T;
    type IntoIter = IntoIter&lt;T&gt;;
    fn into_iter(self) -&gt; IntoIter&lt;T&gt; {
        // need to use ptr::read to unsafely move the buf out since it's
        // not Copy, and Vec implements Drop (so we can't destructure it).
        let buf = unsafe { ptr::read(&amp;self.buf) };
        let len = self.len;
        mem::forget(self);

        IntoIter {
            start: buf.ptr.as_ptr(),
            end: if buf.cap == 0 {
                // can't offset off of a pointer unless it's part of an allocation
                buf.ptr.as_ptr()
            } else {
                unsafe { buf.ptr.as_ptr().add(len) }
            },
            _buf: buf,
        }
    }
}</code></pre>
<p>Much better.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="drain-1"><a class="header" href="#drain-1">Drain</a></h1>
<p>Let's move on to Drain. Drain is largely the same as IntoIter, except that
instead of consuming the Vec, it borrows the Vec and leaves its allocation
untouched. For now we'll only implement the "basic" full-range version.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::marker::PhantomData;

struct Drain&lt;'a, T: 'a&gt; {
    // Need to bound the lifetime here, so we do it with `&amp;'a mut Vec&lt;T&gt;`
    // because that's semantically what we contain. We're "just" calling
    // `pop()` and `remove(0)`.
    vec: PhantomData&lt;&amp;'a mut Vec&lt;T&gt;&gt;,
    start: *const T,
    end: *const T,
}

impl&lt;'a, T&gt; Iterator for Drain&lt;'a, T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None</code></pre>
<p>-- wait, this is seeming familiar. Let's do some more compression. Both
IntoIter and Drain have the exact same structure, let's just factor it out.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">struct RawValIter&lt;T&gt; {
    start: *const T,
    end: *const T,
}

impl&lt;T&gt; RawValIter&lt;T&gt; {
    // unsafe to construct because it has no associated lifetimes.
    // This is necessary to store a RawValIter in the same struct as
    // its actual allocation. OK since it's a private implementation
    // detail.
    unsafe fn new(slice: &amp;[T]) -&gt; Self {
        RawValIter {
            start: slice.as_ptr(),
            end: if slice.len() == 0 {
                // if `len = 0`, then this is not actually allocated memory.
                // Need to avoid offsetting because that will give wrong
                // information to LLVM via GEP.
                slice.as_ptr()
            } else {
                slice.as_ptr().add(slice.len())
            }
        }
    }
}

// Iterator and DoubleEndedIterator impls identical to IntoIter.</code></pre>
<p>And IntoIter becomes the following:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">pub struct IntoIter&lt;T&gt; {
    _buf: RawVec&lt;T&gt;, // we don't actually care about this. Just need it to live.
    iter: RawValIter&lt;T&gt;,
}

impl&lt;T&gt; Iterator for IntoIter&lt;T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; { self.iter.next() }
    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) { self.iter.size_hint() }
}

impl&lt;T&gt; DoubleEndedIterator for IntoIter&lt;T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; { self.iter.next_back() }
}

impl&lt;T&gt; Drop for IntoIter&lt;T&gt; {
    fn drop(&amp;mut self) {
        for _ in &amp;mut *self {}
    }
}

impl&lt;T&gt; IntoIterator for Vec&lt;T&gt; {
    type Item = T;
    type IntoIter = IntoIter&lt;T&gt;;
    fn into_iter(self) -&gt; IntoIter&lt;T&gt; {
        unsafe {
            let iter = RawValIter::new(&amp;self);

            let buf = ptr::read(&amp;self.buf);
            mem::forget(self);

            IntoIter {
                iter,
                _buf: buf,
            }
        }
    }
}</code></pre>
<p>Note that I've left a few quirks in this design to make upgrading Drain to work
with arbitrary subranges a bit easier. In particular we <em>could</em> have RawValIter
drain itself on drop, but that won't work right for a more complex Drain.
We also take a slice to simplify Drain initialization.</p>
<p>Alright, now Drain is really easy:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::marker::PhantomData;

pub struct Drain&lt;'a, T: 'a&gt; {
    vec: PhantomData&lt;&amp;'a mut Vec&lt;T&gt;&gt;,
    iter: RawValIter&lt;T&gt;,
}

impl&lt;'a, T&gt; Iterator for Drain&lt;'a, T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; { self.iter.next() }
    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) { self.iter.size_hint() }
}

impl&lt;'a, T&gt; DoubleEndedIterator for Drain&lt;'a, T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; { self.iter.next_back() }
}

impl&lt;'a, T&gt; Drop for Drain&lt;'a, T&gt; {
    fn drop(&amp;mut self) {
        for _ in &amp;mut *self {}
    }
}

impl&lt;T&gt; Vec&lt;T&gt; {
    pub fn drain(&amp;mut self) -&gt; Drain&lt;T&gt; {
        let iter = unsafe { RawValIter::new(&amp;self) };

        // this is a mem::forget safety thing. If Drain is forgotten, we just
        // leak the whole Vec's contents. Also we need to do this *eventually*
        // anyway, so why not do it now?
        self.len = 0;

        Drain {
            iter,
            vec: PhantomData,
        }
    }
}</code></pre>
<p>For more details on the <code>mem::forget</code> problem, see the
<a href="vec/../leaking.html">section on leaks</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-zero-sized-types"><a class="header" href="#handling-zero-sized-types">Handling Zero-Sized Types</a></h1>
<p>It's time. We're going to fight the specter that is zero-sized types. Safe Rust
<em>never</em> needs to care about this, but Vec is very intensive on raw pointers and
raw allocations, which are exactly the two things that care about
zero-sized types. We need to be careful of two things:</p>
<ul>
<li>The raw allocator API has undefined behavior if you pass in 0 for an
allocation size.</li>
<li>raw pointer offsets are no-ops for zero-sized types, which will break our
C-style pointer iterator.</li>
</ul>
<p>Thankfully we abstracted out pointer-iterators and allocating handling into
<code>RawValIter</code> and <code>RawVec</code> respectively. How mysteriously convenient.</p>
<h2 id="allocating-zero-sized-types"><a class="header" href="#allocating-zero-sized-types">Allocating Zero-Sized Types</a></h2>
<p>So if the allocator API doesn't support zero-sized allocations, what on earth
do we store as our allocation? <code>NonNull::dangling()</code> of course! Almost every operation
with a ZST is a no-op since ZSTs have exactly one value, and therefore no state needs
to be considered to store or load them. This actually extends to <code>ptr::read</code> and
<code>ptr::write</code>: they won't actually look at the pointer at all. As such we never need
to change the pointer.</p>
<p>Note however that our previous reliance on running out of memory before overflow is
no longer valid with zero-sized types. We must explicitly guard against capacity
overflow for zero-sized types.</p>
<p>Due to our current architecture, all this means is writing 3 guards, one in each
method of <code>RawVec</code>.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; RawVec&lt;T&gt; {
    fn new() -&gt; Self {
        // This branch should be stripped at compile time.
        let cap = if mem::size_of::&lt;T&gt;() == 0 { usize::MAX } else { 0 };

        // `NonNull::dangling()` doubles as "unallocated" and "zero-sized allocation"
        RawVec {
            ptr: NonNull::dangling(),
            cap,
        }
    }

    fn grow(&amp;mut self) {
        // since we set the capacity to usize::MAX when T has size 0,
        // getting to here necessarily means the Vec is overfull.
        assert!(mem::size_of::&lt;T&gt;() != 0, "capacity overflow");

        let (new_cap, new_layout) = if self.cap == 0 {
            (1, Layout::array::&lt;T&gt;(1).unwrap())
        } else {
            // This can't overflow because we ensure self.cap &lt;= isize::MAX.
            let new_cap = 2 * self.cap;

            // `Layout::array` checks that the number of bytes is &lt;= usize::MAX,
            // but this is redundant since old_layout.size() &lt;= isize::MAX,
            // so the `unwrap` should never fail.
            let new_layout = Layout::array::&lt;T&gt;(new_cap).unwrap();
            (new_cap, new_layout)
        };

        // Ensure that the new allocation doesn't exceed `isize::MAX` bytes.
        assert!(new_layout.size() &lt;= isize::MAX as usize, "Allocation too large");

        let new_ptr = if self.cap == 0 {
            unsafe { alloc::alloc(new_layout) }
        } else {
            let old_layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            let old_ptr = self.ptr.as_ptr() as *mut u8;
            unsafe { alloc::realloc(old_ptr, old_layout, new_layout.size()) }
        };

        // If allocation fails, `new_ptr` will be null, in which case we abort.
        self.ptr = match NonNull::new(new_ptr as *mut T) {
            Some(p) =&gt; p,
            None =&gt; alloc::handle_alloc_error(new_layout),
        };
        self.cap = new_cap;
    }
}

impl&lt;T&gt; Drop for RawVec&lt;T&gt; {
    fn drop(&amp;mut self) {
        let elem_size = mem::size_of::&lt;T&gt;();

        if self.cap != 0 &amp;&amp; elem_size != 0 {
            unsafe {
                alloc::dealloc(
                    self.ptr.as_ptr() as *mut u8,
                    Layout::array::&lt;T&gt;(self.cap).unwrap(),
                );
            }
        }
    }
}</code></pre>
<p>That's it. We support pushing and popping zero-sized types now. Our iterators
(that aren't provided by slice Deref) are still busted, though.</p>
<h2 id="iterating-zero-sized-types"><a class="header" href="#iterating-zero-sized-types">Iterating Zero-Sized Types</a></h2>
<p>Zero-sized offsets are no-ops. This means that our current design will always
initialize <code>start</code> and <code>end</code> as the same value, and our iterators will yield
nothing. The current solution to this is to cast the pointers to integers,
increment, and then cast them back:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; RawValIter&lt;T&gt; {
    unsafe fn new(slice: &amp;[T]) -&gt; Self {
        RawValIter {
            start: slice.as_ptr(),
            end: if mem::size_of::&lt;T&gt;() == 0 {
                ((slice.as_ptr() as usize) + slice.len()) as *const _
            } else if slice.len() == 0 {
                slice.as_ptr()
            } else {
                slice.as_ptr().add(slice.len())
            },
        }
    }
}</code></pre>
<p>Now we have a different bug. Instead of our iterators not running at all, our
iterators now run <em>forever</em>. We need to do the same trick in our iterator impls.
Also, our size_hint computation code will divide by 0 for ZSTs. Since we'll
basically be treating the two pointers as if they point to bytes, we'll just
map size 0 to divide by 1. Here's what <code>next</code> will be:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
    if self.start == self.end {
        None
    } else {
        unsafe {
            let result = ptr::read(self.start);
            self.start = if mem::size_of::&lt;T&gt;() == 0 {
                (self.start as usize + 1) as *const _
            } else {
                self.start.offset(1)
            };
            Some(result)
        }
    }
}</code></pre>
<p>Do you see the "bug"? No one else did! The original author only noticed the
problem when linking to this page years later. This code is kind of dubious
because abusing the iterator pointers to be <em>counters</em> makes them unaligned!
Our <em>one job</em> when using ZSTs is to keep pointers aligned! <em>forehead slap</em></p>
<p>Raw pointers don't need to be aligned at all times, so the basic trick of
using pointers as counters is <em>fine</em>, but they <em>should</em> definitely be aligned
when passed to <code>ptr::read</code>! This is <em>possibly</em> needless pedantry
because <code>ptr::read</code> is a noop for a ZST, but let's be a <em>little</em> more
responsible and read from <code>NonNull::dangling</code> on the ZST path.</p>
<p>(Alternatively you could call <code>read_unaligned</code> on the ZST path. Either is fine,
because either way we're making up a value from nothing and it all compiles
to doing nothing.)</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Iterator for RawValIter&lt;T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                if mem::size_of::&lt;T&gt;() == 0 {
                    self.start = (self.start as usize + 1) as *const _;
                    Some(ptr::read(NonNull::&lt;T&gt;::dangling().as_ptr()))
                } else {
                    let old_ptr = self.start;
                    self.start = self.start.offset(1);
                    Some(ptr::read(old_ptr))
                }
            }
        }
    }

    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) {
        let elem_size = mem::size_of::&lt;T&gt;();
        let len = (self.end as usize - self.start as usize)
                  / if elem_size == 0 { 1 } else { elem_size };
        (len, Some(len))
    }
}

impl&lt;T&gt; DoubleEndedIterator for RawValIter&lt;T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                if mem::size_of::&lt;T&gt;() == 0 {
                    self.end = (self.end as usize - 1) as *const _;
                    Some(ptr::read(NonNull::&lt;T&gt;::dangling().as_ptr()))
                } else {
                    self.end = self.end.offset(-1);
                    Some(ptr::read(self.end))
                }
            }
        }
    }
}</code></pre>
<p>And that's it. Iteration works!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-final-code"><a class="header" href="#the-final-code">The Final Code</a></h1>
<pre><pre class="playground"><code class="language-rust edition2021">use std::alloc::{self, Layout};
use std::marker::PhantomData;
use std::mem;
use std::ops::{Deref, DerefMut};
use std::ptr::{self, NonNull};

struct RawVec&lt;T&gt; {
    ptr: NonNull&lt;T&gt;,
    cap: usize,
}

unsafe impl&lt;T: Send&gt; Send for RawVec&lt;T&gt; {}
unsafe impl&lt;T: Sync&gt; Sync for RawVec&lt;T&gt; {}

impl&lt;T&gt; RawVec&lt;T&gt; {
    fn new() -&gt; Self {
        // !0 is usize::MAX. This branch should be stripped at compile time.
        let cap = if mem::size_of::&lt;T&gt;() == 0 { !0 } else { 0 };

        // `NonNull::dangling()` doubles as "unallocated" and "zero-sized allocation"
        RawVec {
            ptr: NonNull::dangling(),
            cap,
        }
    }

    fn grow(&amp;mut self) {
        // since we set the capacity to usize::MAX when T has size 0,
        // getting to here necessarily means the Vec is overfull.
        assert!(mem::size_of::&lt;T&gt;() != 0, "capacity overflow");

        let (new_cap, new_layout) = if self.cap == 0 {
            (1, Layout::array::&lt;T&gt;(1).unwrap())
        } else {
            // This can't overflow because we ensure self.cap &lt;= isize::MAX.
            let new_cap = 2 * self.cap;

            // `Layout::array` checks that the number of bytes is &lt;= usize::MAX,
            // but this is redundant since old_layout.size() &lt;= isize::MAX,
            // so the `unwrap` should never fail.
            let new_layout = Layout::array::&lt;T&gt;(new_cap).unwrap();
            (new_cap, new_layout)
        };

        // Ensure that the new allocation doesn't exceed `isize::MAX` bytes.
        assert!(
            new_layout.size() &lt;= isize::MAX as usize,
            "Allocation too large"
        );

        let new_ptr = if self.cap == 0 {
            unsafe { alloc::alloc(new_layout) }
        } else {
            let old_layout = Layout::array::&lt;T&gt;(self.cap).unwrap();
            let old_ptr = self.ptr.as_ptr() as *mut u8;
            unsafe { alloc::realloc(old_ptr, old_layout, new_layout.size()) }
        };

        // If allocation fails, `new_ptr` will be null, in which case we abort.
        self.ptr = match NonNull::new(new_ptr as *mut T) {
            Some(p) =&gt; p,
            None =&gt; alloc::handle_alloc_error(new_layout),
        };
        self.cap = new_cap;
    }
}

impl&lt;T&gt; Drop for RawVec&lt;T&gt; {
    fn drop(&amp;mut self) {
        let elem_size = mem::size_of::&lt;T&gt;();

        if self.cap != 0 &amp;&amp; elem_size != 0 {
            unsafe {
                alloc::dealloc(
                    self.ptr.as_ptr() as *mut u8,
                    Layout::array::&lt;T&gt;(self.cap).unwrap(),
                );
            }
        }
    }
}

pub struct Vec&lt;T&gt; {
    buf: RawVec&lt;T&gt;,
    len: usize,
}

impl&lt;T&gt; Vec&lt;T&gt; {
    fn ptr(&amp;self) -&gt; *mut T {
        self.buf.ptr.as_ptr()
    }

    fn cap(&amp;self) -&gt; usize {
        self.buf.cap
    }

    pub fn new() -&gt; Self {
        Vec {
            buf: RawVec::new(),
            len: 0,
        }
    }
    pub fn push(&amp;mut self, elem: T) {
        if self.len == self.cap() {
            self.buf.grow();
        }

        unsafe {
            ptr::write(self.ptr().add(self.len), elem);
        }

        // Can't overflow, we'll OOM first.
        self.len += 1;
    }

    pub fn pop(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.len == 0 {
            None
        } else {
            self.len -= 1;
            unsafe { Some(ptr::read(self.ptr().add(self.len))) }
        }
    }

    pub fn insert(&amp;mut self, index: usize, elem: T) {
        assert!(index &lt;= self.len, "index out of bounds");
        if self.len == self.cap() {
            self.buf.grow();
        }

        unsafe {
            ptr::copy(
                self.ptr().add(index),
                self.ptr().add(index + 1),
                self.len - index,
            );
            ptr::write(self.ptr().add(index), elem);
        }

        self.len += 1;
    }

    pub fn remove(&amp;mut self, index: usize) -&gt; T {
        assert!(index &lt; self.len, "index out of bounds");

        self.len -= 1;

        unsafe {
            let result = ptr::read(self.ptr().add(index));
            ptr::copy(
                self.ptr().add(index + 1),
                self.ptr().add(index),
                self.len - index,
            );
            result
        }
    }

    pub fn drain(&amp;mut self) -&gt; Drain&lt;T&gt; {
        let iter = unsafe { RawValIter::new(&amp;self) };

        // this is a mem::forget safety thing. If Drain is forgotten, we just
        // leak the whole Vec's contents. Also we need to do this *eventually*
        // anyway, so why not do it now?
        self.len = 0;

        Drain {
            iter,
            vec: PhantomData,
        }
    }
}

impl&lt;T&gt; Drop for Vec&lt;T&gt; {
    fn drop(&amp;mut self) {
        while let Some(_) = self.pop() {}
        // deallocation is handled by RawVec
    }
}

impl&lt;T&gt; Deref for Vec&lt;T&gt; {
    type Target = [T];
    fn deref(&amp;self) -&gt; &amp;[T] {
        unsafe { std::slice::from_raw_parts(self.ptr(), self.len) }
    }
}

impl&lt;T&gt; DerefMut for Vec&lt;T&gt; {
    fn deref_mut(&amp;mut self) -&gt; &amp;mut [T] {
        unsafe { std::slice::from_raw_parts_mut(self.ptr(), self.len) }
    }
}

impl&lt;T&gt; IntoIterator for Vec&lt;T&gt; {
    type Item = T;
    type IntoIter = IntoIter&lt;T&gt;;
    fn into_iter(self) -&gt; IntoIter&lt;T&gt; {
        let (iter, buf) = unsafe {
            (RawValIter::new(&amp;self), ptr::read(&amp;self.buf))
        };

        mem::forget(self);

        IntoIter {
            iter,
            _buf: buf,
        }
    }
}

struct RawValIter&lt;T&gt; {
    start: *const T,
    end: *const T,
}

impl&lt;T&gt; RawValIter&lt;T&gt; {
    unsafe fn new(slice: &amp;[T]) -&gt; Self {
        RawValIter {
            start: slice.as_ptr(),
            end: if mem::size_of::&lt;T&gt;() == 0 {
                ((slice.as_ptr() as usize) + slice.len()) as *const _
            } else if slice.len() == 0 {
                slice.as_ptr()
            } else {
                slice.as_ptr().add(slice.len())
            },
        }
    }
}

impl&lt;T&gt; Iterator for RawValIter&lt;T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                if mem::size_of::&lt;T&gt;() == 0 {
                    self.start = (self.start as usize + 1) as *const _;
                    Some(ptr::read(NonNull::&lt;T&gt;::dangling().as_ptr()))
                } else {
                    let old_ptr = self.start;
                    self.start = self.start.offset(1);
                    Some(ptr::read(old_ptr))
                }
            }
        }
    }

    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) {
        let elem_size = mem::size_of::&lt;T&gt;();
        let len = (self.end as usize - self.start as usize)
                  / if elem_size == 0 { 1 } else { elem_size };
        (len, Some(len))
    }
}

impl&lt;T&gt; DoubleEndedIterator for RawValIter&lt;T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; {
        if self.start == self.end {
            None
        } else {
            unsafe {
                if mem::size_of::&lt;T&gt;() == 0 {
                    self.end = (self.end as usize - 1) as *const _;
                    Some(ptr::read(NonNull::&lt;T&gt;::dangling().as_ptr()))
                } else {
                    self.end = self.end.offset(-1);
                    Some(ptr::read(self.end))
                }
            }
        }
    }
}

pub struct IntoIter&lt;T&gt; {
    _buf: RawVec&lt;T&gt;, // we don't actually care about this. Just need it to live.
    iter: RawValIter&lt;T&gt;,
}

impl&lt;T&gt; Iterator for IntoIter&lt;T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        self.iter.next()
    }
    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) {
        self.iter.size_hint()
    }
}

impl&lt;T&gt; DoubleEndedIterator for IntoIter&lt;T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; {
        self.iter.next_back()
    }
}

impl&lt;T&gt; Drop for IntoIter&lt;T&gt; {
    fn drop(&amp;mut self) {
        for _ in &amp;mut *self {}
    }
}

pub struct Drain&lt;'a, T: 'a&gt; {
    vec: PhantomData&lt;&amp;'a mut Vec&lt;T&gt;&gt;,
    iter: RawValIter&lt;T&gt;,
}

impl&lt;'a, T&gt; Iterator for Drain&lt;'a, T&gt; {
    type Item = T;
    fn next(&amp;mut self) -&gt; Option&lt;T&gt; {
        self.iter.next()
    }
    fn size_hint(&amp;self) -&gt; (usize, Option&lt;usize&gt;) {
        self.iter.size_hint()
    }
}

impl&lt;'a, T&gt; DoubleEndedIterator for Drain&lt;'a, T&gt; {
    fn next_back(&amp;mut self) -&gt; Option&lt;T&gt; {
        self.iter.next_back()
    }
}

impl&lt;'a, T&gt; Drop for Drain&lt;'a, T&gt; {
    fn drop(&amp;mut self) {
        // pre-drain the iter
        for _ in &amp;mut *self {}
    }
}
<span class="boring">
</span><span class="boring">fn main() {
</span><span class="boring">    tests::create_push_pop();
</span><span class="boring">    tests::iter_test();
</span><span class="boring">    tests::test_drain();
</span><span class="boring">    tests::test_zst();
</span><span class="boring">    println!("All tests finished OK");
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">mod tests {
</span><span class="boring">    use super::*;
</span><span class="boring">
</span><span class="boring">    pub fn create_push_pop() {
</span><span class="boring">        let mut v = Vec::new();
</span><span class="boring">        v.push(1);
</span><span class="boring">        assert_eq!(1, v.len());
</span><span class="boring">        assert_eq!(1, v[0]);
</span><span class="boring">        for i in v.iter_mut() {
</span><span class="boring">            *i += 1;
</span><span class="boring">        }
</span><span class="boring">        v.insert(0, 5);
</span><span class="boring">        let x = v.pop();
</span><span class="boring">        assert_eq!(Some(2), x);
</span><span class="boring">        assert_eq!(1, v.len());
</span><span class="boring">        v.push(10);
</span><span class="boring">        let x = v.remove(0);
</span><span class="boring">        assert_eq!(5, x);
</span><span class="boring">        assert_eq!(1, v.len());
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    pub fn iter_test() {
</span><span class="boring">        let mut v = Vec::new();
</span><span class="boring">        for i in 0..10 {
</span><span class="boring">            v.push(Box::new(i))
</span><span class="boring">        }
</span><span class="boring">        let mut iter = v.into_iter();
</span><span class="boring">        let first = iter.next().unwrap();
</span><span class="boring">        let last = iter.next_back().unwrap();
</span><span class="boring">        drop(iter);
</span><span class="boring">        assert_eq!(0, *first);
</span><span class="boring">        assert_eq!(9, *last);
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    pub fn test_drain() {
</span><span class="boring">        let mut v = Vec::new();
</span><span class="boring">        for i in 0..10 {
</span><span class="boring">            v.push(Box::new(i))
</span><span class="boring">        }
</span><span class="boring">        {
</span><span class="boring">            let mut drain = v.drain();
</span><span class="boring">            let first = drain.next().unwrap();
</span><span class="boring">            let last = drain.next_back().unwrap();
</span><span class="boring">            assert_eq!(0, *first);
</span><span class="boring">            assert_eq!(9, *last);
</span><span class="boring">        }
</span><span class="boring">        assert_eq!(0, v.len());
</span><span class="boring">        v.push(Box::new(1));
</span><span class="boring">        assert_eq!(1, *v.pop().unwrap());
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    pub fn test_zst() {
</span><span class="boring">        let mut v = Vec::new();
</span><span class="boring">        for _i in 0..10 {
</span><span class="boring">            v.push(())
</span><span class="boring">        }
</span><span class="boring">
</span><span class="boring">        let mut count = 0;
</span><span class="boring">
</span><span class="boring">        for _ in v.into_iter() {
</span><span class="boring">            count += 1
</span><span class="boring">        }
</span><span class="boring">
</span><span class="boring">        assert_eq!(10, count);
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-arc-and-mutex"><a class="header" href="#implementing-arc-and-mutex">Implementing Arc and Mutex</a></h1>
<p>Knowing the theory is all fine and good, but the <em>best</em> way to understand
something is to use it. To better understand atomics and interior mutability,
we'll be implementing versions of the standard library's <code>Arc</code> and <code>Mutex</code> types.</p>
<p>TODO: Write <code>Mutex</code> chapters.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-arc"><a class="header" href="#implementing-arc">Implementing Arc</a></h1>
<p>In this section, we'll be implementing a simpler version of <code>std::sync::Arc</code>.
Similarly to <a href="arc-mutex/../vec/vec.html">the implementation of <code>Vec</code> we made earlier</a>, we won't be
taking advantage of as many optimizations, intrinsics, or unstable code as the
standard library may.</p>
<p>This implementation is loosely based on the standard library's implementation
(technically taken from <code>alloc::sync</code> in 1.49, as that's where it's actually
implemented), but it will not support weak references at the moment as they
make the implementation slightly more complex.</p>
<p>Please note that this section is very work-in-progress at the moment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layout-1"><a class="header" href="#layout-1">Layout</a></h1>
<p>Let's start by making the layout for our implementation of <code>Arc</code>.</p>
<p>An <code>Arc&lt;T&gt;</code> provides thread-safe shared ownership of a value of type <code>T</code>,
allocated in the heap. Sharing implies immutability in Rust, so we don't need to
design anything that manages access to that value, right? Although interior
mutability types like Mutex allow Arc's users to create shared mutability, Arc
itself doesn't need to concern itself with these issues.</p>
<p>However there <em>is</em> one place where Arc needs to concern itself with mutation:
destruction. When all the owners of the Arc go away, we need to be able to
<code>drop</code> its contents and free its allocation. So we need a way for an owner to
know if it's the <em>last</em> owner, and the simplest way to do that is with a count
of the owners -- Reference Counting.</p>
<p>Unfortunately, this reference count is inherently shared mutable state, so Arc
<em>does</em> need to think about synchronization. We <em>could</em> use a Mutex for this, but
that's overkill. Instead, we'll use atomics. And since everyone already needs a
pointer to the T's allocation, we might as well put the reference count in that
same allocation.</p>
<p>Naively, it would look something like this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::atomic;

pub struct Arc&lt;T&gt; {
    ptr: *mut ArcInner&lt;T&gt;,
}

pub struct ArcInner&lt;T&gt; {
    rc: atomic::AtomicUsize,
    data: T,
}
<span class="boring">}</span></code></pre></pre>
<p>This would compile, however it would be incorrect. First of all, the compiler
will give us too strict variance. For example, an <code>Arc&lt;&amp;'static str&gt;</code> couldn't
be used where an <code>Arc&lt;&amp;'a str&gt;</code> was expected. More importantly, it will give
incorrect ownership information to the drop checker, as it will assume we don't
own any values of type <code>T</code>. As this is a structure providing shared ownership of
a value, at some point there will be an instance of this structure that entirely
owns its data. See <a href="arc-mutex/../ownership.html">the chapter on ownership and lifetimes</a> for
all the details on variance and drop check.</p>
<p>To fix the first problem, we can use <code>NonNull&lt;T&gt;</code>. Note that <code>NonNull&lt;T&gt;</code> is a
wrapper around a raw pointer that declares that:</p>
<ul>
<li>We are covariant over <code>T</code></li>
<li>Our pointer is never null</li>
</ul>
<p>To fix the second problem, we can include a <code>PhantomData</code> marker containing an
<code>ArcInner&lt;T&gt;</code>. This will tell the drop checker that we have some notion of
ownership of a value of <code>ArcInner&lt;T&gt;</code> (which itself contains some <code>T</code>).</p>
<p>With these changes we get our final structure:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::marker::PhantomData;
use std::ptr::NonNull;
use std::sync::atomic::AtomicUsize;

pub struct Arc&lt;T&gt; {
    ptr: NonNull&lt;ArcInner&lt;T&gt;&gt;,
    phantom: PhantomData&lt;ArcInner&lt;T&gt;&gt;,
}

pub struct ArcInner&lt;T&gt; {
    rc: AtomicUsize,
    data: T,
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="base-code"><a class="header" href="#base-code">Base Code</a></h1>
<p>Now that we've decided the layout for our implementation of <code>Arc</code>, let's create
some basic code.</p>
<h2 id="constructing-the-arc"><a class="header" href="#constructing-the-arc">Constructing the Arc</a></h2>
<p>We'll first need a way to construct an <code>Arc&lt;T&gt;</code>.</p>
<p>This is pretty simple, as we just need to box the <code>ArcInner&lt;T&gt;</code> and get a
<code>NonNull&lt;T&gt;</code> pointer to it.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Arc&lt;T&gt; {
    pub fn new(data: T) -&gt; Arc&lt;T&gt; {
        // We start the reference count at 1, as that first reference is the
        // current pointer.
        let boxed = Box::new(ArcInner {
            rc: AtomicUsize::new(1),
            data,
        });
        Arc {
            // It is okay to call `.unwrap()` here as we get a pointer from
            // `Box::into_raw` which is guaranteed to not be null.
            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),
            phantom: PhantomData,
        }
    }
}</code></pre>
<h2 id="send-and-sync-1"><a class="header" href="#send-and-sync-1">Send and Sync</a></h2>
<p>Since we're building a concurrency primitive, we'll need to be able to send it
across threads. Thus, we can implement the <code>Send</code> and <code>Sync</code> marker traits. For
more information on these, see <a href="arc-mutex/../send-and-sync.html">the section on <code>Send</code> and
<code>Sync</code></a>.</p>
<p>This is okay because:</p>
<ul>
<li>You can only get a mutable reference to the value inside an <code>Arc</code> if and only
if it is the only <code>Arc</code> referencing that data (which only happens in <code>Drop</code>)</li>
<li>We use atomics for the shared mutable reference counting</li>
</ul>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">unsafe impl&lt;T: Sync + Send&gt; Send for Arc&lt;T&gt; {}
unsafe impl&lt;T: Sync + Send&gt; Sync for Arc&lt;T&gt; {}</code></pre>
<p>We need to have the bound <code>T: Sync + Send</code> because if we did not provide those
bounds, it would be possible to share values that are thread-unsafe across a
thread boundary via an <code>Arc</code>, which could possibly cause data races or
unsoundness.</p>
<p>For example, if those bounds were not present, <code>Arc&lt;Rc&lt;u32&gt;&gt;</code> would be <code>Sync</code> or
<code>Send</code>, meaning that you could clone the <code>Rc</code> out of the <code>Arc</code> to send it across
a thread (without creating an entirely new <code>Rc</code>), which would create data races
as <code>Rc</code> is not thread-safe.</p>
<h2 id="getting-the-arcinner"><a class="header" href="#getting-the-arcinner">Getting the <code>ArcInner</code></a></h2>
<p>To dereference the <code>NonNull&lt;T&gt;</code> pointer into a <code>&amp;T</code>, we can call
<code>NonNull::as_ref</code>. This is unsafe, unlike the typical <code>as_ref</code> function, so we
must call it like this:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">unsafe { self.ptr.as_ref() }</code></pre>
<p>We'll be using this snippet a few times in this code (usually with an associated
<code>let</code> binding).</p>
<p>This unsafety is okay because while this <code>Arc</code> is alive, we're guaranteed that
the inner pointer is valid.</p>
<h2 id="deref-1"><a class="header" href="#deref-1">Deref</a></h2>
<p>Alright. Now we can make <code>Arc</code>s (and soon will be able to clone and destroy them correctly), but how do we get
to the data inside?</p>
<p>What we need now is an implementation of <code>Deref</code>.</p>
<p>We'll need to import the trait:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::ops::Deref;</code></pre>
<p>And here's the implementation:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Deref for Arc&lt;T&gt; {
    type Target = T;

    fn deref(&amp;self) -&gt; &amp;T {
        let inner = unsafe { self.ptr.as_ref() };
        &amp;inner.data
    }
}</code></pre>
<p>Pretty simple, eh? This simply dereferences the <code>NonNull</code> pointer to the
<code>ArcInner&lt;T&gt;</code>, then gets a reference to the data inside.</p>
<h2 id="code"><a class="header" href="#code">Code</a></h2>
<p>Here's all the code from this section:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::ops::Deref;

impl&lt;T&gt; Arc&lt;T&gt; {
    pub fn new(data: T) -&gt; Arc&lt;T&gt; {
        // We start the reference count at 1, as that first reference is the
        // current pointer.
        let boxed = Box::new(ArcInner {
            rc: AtomicUsize::new(1),
            data,
        });
        Arc {
            // It is okay to call `.unwrap()` here as we get a pointer from
            // `Box::into_raw` which is guaranteed to not be null.
            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),
            phantom: PhantomData,
        }
    }
}

unsafe impl&lt;T: Sync + Send&gt; Send for Arc&lt;T&gt; {}
unsafe impl&lt;T: Sync + Send&gt; Sync for Arc&lt;T&gt; {}


impl&lt;T&gt; Deref for Arc&lt;T&gt; {
    type Target = T;

    fn deref(&amp;self) -&gt; &amp;T {
        let inner = unsafe { self.ptr.as_ref() };
        &amp;inner.data
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cloning"><a class="header" href="#cloning">Cloning</a></h1>
<p>Now that we've got some basic code set up, we'll need a way to clone the <code>Arc</code>.</p>
<p>Basically, we need to:</p>
<ol>
<li>Increment the atomic reference count</li>
<li>Construct a new instance of the <code>Arc</code> from the inner pointer</li>
</ol>
<p>First, we need to get access to the <code>ArcInner</code>:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let inner = unsafe { self.ptr.as_ref() };</code></pre>
<p>We can update the atomic reference count as follows:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let old_rc = inner.rc.fetch_add(1, Ordering::???);</code></pre>
<p>But what ordering should we use here? We don't really have any code that will
need atomic synchronization when cloning, as we do not modify the internal value
while cloning. Thus, we can use a Relaxed ordering here, which implies no
happens-before relationship but is atomic. When <code>Drop</code>ping the Arc, however,
we'll need to atomically synchronize when decrementing the reference count. This
is described more in <a href="arc-mutex/arc-drop.html">the section on the <code>Drop</code> implementation for
<code>Arc</code></a>. For more information on atomic relationships and Relaxed
ordering, see <a href="arc-mutex/../atomics/atomics.html">the section on atomics</a>.</p>
<p>Thus, the code becomes this:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);</code></pre>
<p>We'll need to add another import to use <code>Ordering</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::atomic::Ordering;
<span class="boring">}</span></code></pre></pre>
<p>However, we have one problem with this implementation right now. What if someone
decides to <code>mem::forget</code> a bunch of Arcs? The code we have written so far (and
will write) assumes that the reference count accurately portrays how many Arcs
are in memory, but with <code>mem::forget</code> this is false. Thus, when more and more
Arcs are cloned from this one without them being <code>Drop</code>ped and the reference
count being decremented, we can overflow! This will cause use-after-free which
is <strong>INCREDIBLY BAD!</strong></p>
<p>To handle this, we need to check that the reference count does not go over some
arbitrary value (below <code>usize::MAX</code>, as we're storing the reference count as an
<code>AtomicUsize</code>), and do <em>something</em>.</p>
<p>The standard library's implementation decides to just abort the program (as it
is an incredibly unlikely case in normal code and if it happens, the program is
probably incredibly degenerate) if the reference count reaches <code>isize::MAX</code>
(about half of <code>usize::MAX</code>) on any thread, on the assumption that there are
probably not about 2 billion threads (or about <strong>9 quintillion</strong> on some 64-bit
machines) incrementing the reference count at once. This is what we'll do.</p>
<p>It's pretty simple to implement this behavior:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">if old_rc &gt;= isize::MAX as usize {
    std::process::abort();
}</code></pre>
<p>Then, we need to return a new instance of the <code>Arc</code>:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">Self {
    ptr: self.ptr,
    phantom: PhantomData
}</code></pre>
<p>Now, let's wrap this all up inside the <code>Clone</code> implementation:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">use std::sync::atomic::Ordering;

impl&lt;T&gt; Clone for Arc&lt;T&gt; {
    fn clone(&amp;self) -&gt; Arc&lt;T&gt; {
        let inner = unsafe { self.ptr.as_ref() };
        // Using a relaxed ordering is alright here as we don't need any atomic
        // synchronization here as we're not modifying or accessing the inner
        // data.
        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);

        if old_rc &gt;= isize::MAX as usize {
            std::process::abort();
        }

        Self {
            ptr: self.ptr,
            phantom: PhantomData,
        }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dropping"><a class="header" href="#dropping">Dropping</a></h1>
<p>We now need a way to decrease the reference count and drop the data once it is
low enough, otherwise the data will live forever on the heap.</p>
<p>To do this, we can implement <code>Drop</code>.</p>
<p>Basically, we need to:</p>
<ol>
<li>Decrement the reference count</li>
<li>If there is only one reference remaining to the data, then:</li>
<li>Atomically fence the data to prevent reordering of the use and deletion of
the data</li>
<li>Drop the inner data</li>
</ol>
<p>First, we'll need to get access to the <code>ArcInner</code>:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">let inner = unsafe { self.ptr.as_ref() };</code></pre>
<p>Now, we need to decrement the reference count. To streamline our code, we can
also return if the returned value from <code>fetch_sub</code> (the value of the reference
count before decrementing it) is not equal to <code>1</code> (which happens when we are not
the last reference to the data).</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">if inner.rc.fetch_sub(1, Ordering::Release) != 1 {
    return;
}</code></pre>
<p>We then need to create an atomic fence to prevent reordering of the use of the
data and deletion of the data. As described in <a href="https://github.com/rust-lang/rust/blob/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/alloc/src/sync.rs#L1440-L1467">the standard library's
implementation of <code>Arc</code></a>:</p>
<blockquote>
<p>This fence is needed to prevent reordering of use of the data and deletion of
the data. Because it is marked <code>Release</code>, the decreasing of the reference
count synchronizes with this <code>Acquire</code> fence. This means that use of the data
happens before decreasing the reference count, which happens before this
fence, which happens before the deletion of the data.</p>
<p>As explained in the <a href="https://www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html">Boost documentation</a>,</p>
<blockquote>
<p>It is important to enforce any possible access to the object in one
thread (through an existing reference) to <em>happen before</em> deleting
the object in a different thread. This is achieved by a "release"
operation after dropping a reference (any access to the object
through this reference must obviously happened before), and an
"acquire" operation before deleting the object.</p>
</blockquote>
<p>In particular, while the contents of an Arc are usually immutable, it's
possible to have interior writes to something like a Mutex<T>. Since a Mutex
is not acquired when it is deleted, we can't rely on its synchronization logic
to make writes in thread A visible to a destructor running in thread B.</p>
<p>Also note that the Acquire fence here could probably be replaced with an
Acquire load, which could improve performance in highly-contended situations.
See <a href="https://github.com/rust-lang/rust/pull/41714">2</a>.</p>
</blockquote>
<p>To do this, we do the following:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">use std::sync::atomic::Ordering;
</span>use std::sync::atomic;
atomic::fence(Ordering::Acquire);
<span class="boring">}</span></code></pre></pre>
<p>Finally, we can drop the data itself. We use <code>Box::from_raw</code> to drop the boxed
<code>ArcInner&lt;T&gt;</code> and its data. This takes a <code>*mut T</code> and not a <code>NonNull&lt;T&gt;</code>, so we
must convert using <code>NonNull::as_ptr</code>.</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">unsafe { Box::from_raw(self.ptr.as_ptr()); }</code></pre>
<p>This is safe as we know we have the last pointer to the <code>ArcInner</code> and that its
pointer is valid.</p>
<p>Now, let's wrap this all up inside the <code>Drop</code> implementation:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">impl&lt;T&gt; Drop for Arc&lt;T&gt; {
    fn drop(&amp;mut self) {
        let inner = unsafe { self.ptr.as_ref() };
        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {
            return;
        }
        // This fence is needed to prevent reordering of the use and deletion
        // of the data.
        atomic::fence(Ordering::Acquire);
        // This is safe as we know we have the last pointer to the `ArcInner`
        // and that its pointer is valid.
        unsafe { Box::from_raw(self.ptr.as_ptr()); }
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="final-code"><a class="header" href="#final-code">Final Code</a></h1>
<p>Here's the final code, with some added comments and re-ordered imports:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::marker::PhantomData;
use std::ops::Deref;
use std::ptr::NonNull;
use std::sync::atomic::{self, AtomicUsize, Ordering};

pub struct Arc&lt;T&gt; {
    ptr: NonNull&lt;ArcInner&lt;T&gt;&gt;,
    phantom: PhantomData&lt;ArcInner&lt;T&gt;&gt;,
}

pub struct ArcInner&lt;T&gt; {
    rc: AtomicUsize,
    data: T,
}

impl&lt;T&gt; Arc&lt;T&gt; {
    pub fn new(data: T) -&gt; Arc&lt;T&gt; {
        // We start the reference count at 1, as that first reference is the
        // current pointer.
        let boxed = Box::new(ArcInner {
            rc: AtomicUsize::new(1),
            data,
        });
        Arc {
            // It is okay to call `.unwrap()` here as we get a pointer from
            // `Box::into_raw` which is guaranteed to not be null.
            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),
            phantom: PhantomData,
        }
    }
}

unsafe impl&lt;T: Sync + Send&gt; Send for Arc&lt;T&gt; {}
unsafe impl&lt;T: Sync + Send&gt; Sync for Arc&lt;T&gt; {}

impl&lt;T&gt; Deref for Arc&lt;T&gt; {
    type Target = T;

    fn deref(&amp;self) -&gt; &amp;T {
        let inner = unsafe { self.ptr.as_ref() };
        &amp;inner.data
    }
}

impl&lt;T&gt; Clone for Arc&lt;T&gt; {
    fn clone(&amp;self) -&gt; Arc&lt;T&gt; {
        let inner = unsafe { self.ptr.as_ref() };
        // Using a relaxed ordering is alright here as we don't need any atomic
        // synchronization here as we're not modifying or accessing the inner
        // data.
        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);

        if old_rc &gt;= isize::MAX as usize {
            std::process::abort();
        }

        Self {
            ptr: self.ptr,
            phantom: PhantomData,
        }
    }
}

impl&lt;T&gt; Drop for Arc&lt;T&gt; {
    fn drop(&amp;mut self) {
        let inner = unsafe { self.ptr.as_ref() };
        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {
            return;
        }
        // This fence is needed to prevent reordering of the use and deletion
        // of the data.
        atomic::fence(Ordering::Acquire);
        // This is safe as we know we have the last pointer to the `ArcInner`
        // and that its pointer is valid.
        unsafe { Box::from_raw(self.ptr.as_ptr()); }
    }
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foreign-function-interface"><a class="header" href="#foreign-function-interface">Foreign Function Interface</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>This guide will use the <a href="https://github.com/google/snappy">snappy</a>
compression/decompression library as an introduction to writing bindings for
foreign code. Rust is currently unable to call directly into a C++ library, but
snappy includes a C interface (documented in
<a href="https://github.com/google/snappy/blob/master/snappy-c.h"><code>snappy-c.h</code></a>).</p>
<h2 id="a-note-about-libc"><a class="header" href="#a-note-about-libc">A note about libc</a></h2>
<p>Many of these examples use <a href="https://crates.io/crates/libc">the <code>libc</code> crate</a>, which provides various
type definitions for C types, among other things. If you’re trying these
examples yourself, you’ll need to add <code>libc</code> to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
libc = "0.2.0"
</code></pre>
<h2 id="calling-foreign-functions"><a class="header" href="#calling-foreign-functions">Calling foreign functions</a></h2>
<p>The following is a minimal example of calling a foreign function which will
compile if snappy is installed:</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">use libc::size_t;

#[link(name = "snappy")]
extern {
    fn snappy_max_compressed_length(source_length: size_t) -&gt; size_t;
}

fn main() {
    let x = unsafe { snappy_max_compressed_length(100) };
    println!("max compressed length of a 100 byte buffer: {}", x);
}</code></pre>
<p>The <code>extern</code> block is a list of function signatures in a foreign library, in
this case with the platform's C ABI. The <code>#[link(...)]</code> attribute is used to
instruct the linker to link against the snappy library so the symbols are
resolved.</p>
<p>Foreign functions are assumed to be unsafe so calls to them need to be wrapped
with <code>unsafe {}</code> as a promise to the compiler that everything contained within
truly is safe. C libraries often expose interfaces that aren't thread-safe, and
almost any function that takes a pointer argument isn't valid for all possible
inputs since the pointer could be dangling, and raw pointers fall outside of
Rust's safe memory model.</p>
<p>When declaring the argument types to a foreign function, the Rust compiler
cannot check if the declaration is correct, so specifying it correctly is part
of keeping the binding correct at runtime.</p>
<p>The <code>extern</code> block can be extended to cover the entire snappy API:</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">use libc::{c_int, size_t};

#[link(name = "snappy")]
extern {
    fn snappy_compress(input: *const u8,
                       input_length: size_t,
                       compressed: *mut u8,
                       compressed_length: *mut size_t) -&gt; c_int;
    fn snappy_uncompress(compressed: *const u8,
                         compressed_length: size_t,
                         uncompressed: *mut u8,
                         uncompressed_length: *mut size_t) -&gt; c_int;
    fn snappy_max_compressed_length(source_length: size_t) -&gt; size_t;
    fn snappy_uncompressed_length(compressed: *const u8,
                                  compressed_length: size_t,
                                  result: *mut size_t) -&gt; c_int;
    fn snappy_validate_compressed_buffer(compressed: *const u8,
                                         compressed_length: size_t) -&gt; c_int;
}
<span class="boring">fn main() {}</span></code></pre>
<h2 id="creating-a-safe-interface"><a class="header" href="#creating-a-safe-interface">Creating a safe interface</a></h2>
<p>The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts
like vectors. A library can choose to expose only the safe, high-level interface and hide the unsafe
internal details.</p>
<p>Wrapping the functions which expect buffers involves using the <code>slice::raw</code> module to manipulate Rust
vectors as pointers to memory. Rust's vectors are guaranteed to be a contiguous block of memory. The
length is the number of elements currently contained, and the capacity is the total size in elements of
the allocated memory. The length is less than or equal to the capacity.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore"><span class="boring">use libc::{c_int, size_t};
</span><span class="boring">unsafe fn snappy_validate_compressed_buffer(_: *const u8, _: size_t) -&gt; c_int { 0 }
</span><span class="boring">fn main() {}
</span>pub fn validate_compressed_buffer(src: &amp;[u8]) -&gt; bool {
    unsafe {
        snappy_validate_compressed_buffer(src.as_ptr(), src.len() as size_t) == 0
    }
}</code></pre>
<p>The <code>validate_compressed_buffer</code> wrapper above makes use of an <code>unsafe</code> block, but it makes the
guarantee that calling it is safe for all inputs by leaving off <code>unsafe</code> from the function
signature.</p>
<p>The <code>snappy_compress</code> and <code>snappy_uncompress</code> functions are more complex, since a buffer has to be
allocated to hold the output too.</p>
<p>The <code>snappy_max_compressed_length</code> function can be used to allocate a vector with the maximum
required capacity to hold the compressed output. The vector can then be passed to the
<code>snappy_compress</code> function as an output parameter. An output parameter is also passed to retrieve
the true length after compression for setting the length.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore"><span class="boring">use libc::{size_t, c_int};
</span><span class="boring">unsafe fn snappy_compress(a: *const u8, b: size_t, c: *mut u8,
</span><span class="boring">                          d: *mut size_t) -&gt; c_int { 0 }
</span><span class="boring">unsafe fn snappy_max_compressed_length(a: size_t) -&gt; size_t { a }
</span><span class="boring">fn main() {}
</span>pub fn compress(src: &amp;[u8]) -&gt; Vec&lt;u8&gt; {
    unsafe {
        let srclen = src.len() as size_t;
        let psrc = src.as_ptr();

        let mut dstlen = snappy_max_compressed_length(srclen);
        let mut dst = Vec::with_capacity(dstlen as usize);
        let pdst = dst.as_mut_ptr();

        snappy_compress(psrc, srclen, pdst, &amp;mut dstlen);
        dst.set_len(dstlen as usize);
        dst
    }
}</code></pre>
<p>Decompression is similar, because snappy stores the uncompressed size as part of the compression
format and <code>snappy_uncompressed_length</code> will retrieve the exact buffer size required.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore"><span class="boring">use libc::{size_t, c_int};
</span><span class="boring">unsafe fn snappy_uncompress(compressed: *const u8,
</span><span class="boring">                            compressed_length: size_t,
</span><span class="boring">                            uncompressed: *mut u8,
</span><span class="boring">                            uncompressed_length: *mut size_t) -&gt; c_int { 0 }
</span><span class="boring">unsafe fn snappy_uncompressed_length(compressed: *const u8,
</span><span class="boring">                                     compressed_length: size_t,
</span><span class="boring">                                     result: *mut size_t) -&gt; c_int { 0 }
</span><span class="boring">fn main() {}
</span>pub fn uncompress(src: &amp;[u8]) -&gt; Option&lt;Vec&lt;u8&gt;&gt; {
    unsafe {
        let srclen = src.len() as size_t;
        let psrc = src.as_ptr();

        let mut dstlen: size_t = 0;
        snappy_uncompressed_length(psrc, srclen, &amp;mut dstlen);

        let mut dst = Vec::with_capacity(dstlen as usize);
        let pdst = dst.as_mut_ptr();

        if snappy_uncompress(psrc, srclen, pdst, &amp;mut dstlen) == 0 {
            dst.set_len(dstlen as usize);
            Some(dst)
        } else {
            None // SNAPPY_INVALID_INPUT
        }
    }
}</code></pre>
<p>Then, we can add some tests to show how to use them.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore"><span class="boring">use libc::{c_int, size_t};
</span><span class="boring">unsafe fn snappy_compress(input: *const u8,
</span><span class="boring">                          input_length: size_t,
</span><span class="boring">                          compressed: *mut u8,
</span><span class="boring">                          compressed_length: *mut size_t)
</span><span class="boring">                          -&gt; c_int { 0 }
</span><span class="boring">unsafe fn snappy_uncompress(compressed: *const u8,
</span><span class="boring">                            compressed_length: size_t,
</span><span class="boring">                            uncompressed: *mut u8,
</span><span class="boring">                            uncompressed_length: *mut size_t)
</span><span class="boring">                            -&gt; c_int { 0 }
</span><span class="boring">unsafe fn snappy_max_compressed_length(source_length: size_t) -&gt; size_t { 0 }
</span><span class="boring">unsafe fn snappy_uncompressed_length(compressed: *const u8,
</span><span class="boring">                                     compressed_length: size_t,
</span><span class="boring">                                     result: *mut size_t)
</span><span class="boring">                                     -&gt; c_int { 0 }
</span><span class="boring">unsafe fn snappy_validate_compressed_buffer(compressed: *const u8,
</span><span class="boring">                                            compressed_length: size_t)
</span><span class="boring">                                            -&gt; c_int { 0 }
</span><span class="boring">fn main() { }
</span><span class="boring">
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn valid() {
        let d = vec![0xde, 0xad, 0xd0, 0x0d];
        let c: &amp;[u8] = &amp;compress(&amp;d);
        assert!(validate_compressed_buffer(c));
        assert!(uncompress(c) == Some(d));
    }

    #[test]
    fn invalid() {
        let d = vec![0, 0, 0, 0];
        assert!(!validate_compressed_buffer(&amp;d));
        assert!(uncompress(&amp;d).is_none());
    }

    #[test]
    fn empty() {
        let d = vec![];
        assert!(!validate_compressed_buffer(&amp;d));
        assert!(uncompress(&amp;d).is_none());
        let c = compress(&amp;d);
        assert!(validate_compressed_buffer(&amp;c));
        assert!(uncompress(&amp;c) == Some(d));
    }
}</code></pre>
<h2 id="destructors-1"><a class="header" href="#destructors-1">Destructors</a></h2>
<p>Foreign libraries often hand off ownership of resources to the calling code.
When this occurs, we must use Rust's destructors to provide safety and guarantee
the release of these resources (especially in the case of panic).</p>
<p>For more about destructors, see the <a href="../std/ops/trait.Drop.html">Drop trait</a>.</p>
<h2 id="calling-rust-code-from-c"><a class="header" href="#calling-rust-code-from-c">Calling Rust code from C</a></h2>
<p>You may wish to compile Rust code in a way so that it can be called from C.
This is fairly easy, but requires a few things.</p>
<h3 id="rust-side"><a class="header" href="#rust-side">Rust side</a></h3>
<p>First, we assume you have a lib crate named as <code>rust_from_c</code>.
<code>lib.rs</code> should have Rust code as following:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#[no_mangle]
pub extern "C" fn hello_from_rust() {
    println!("Hello from Rust!");
}
<span class="boring">fn main() {}</span></code></pre></pre>
<p>The <code>extern "C"</code> makes this function adhere to the C calling convention, as discussed below in "<a href="ffi.html#foreign-calling-conventions">Foreign Calling Conventions</a>".
The <code>no_mangle</code> attribute turns off Rust's name mangling, so that it has a well defined symbol to link to.</p>
<p>Then, to compile Rust code as a shared library that can be called from C, add the following to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[lib]
crate-type = ["cdylib"]
</code></pre>
<p>(NOTE: We could also use the <code>staticlib</code> crate type but it needs to tweak some linking flags.)</p>
<p>Run <code>cargo build</code> and you're ready to go on the Rust side.</p>
<h3 id="c-side"><a class="header" href="#c-side">C side</a></h3>
<p>We'll create a C file to call the <code>hello_from_rust</code> function and compile it by <code>gcc</code>.</p>
<p>C file should look like:</p>
<pre><code class="language-c">extern void hello_from_rust();

int main(void) {
    hello_from_rust();
    return 0;
}
</code></pre>
<p>We name the file as <code>call_rust.c</code> and place it on the crate root.
Run the following to compile:</p>
<pre><code class="language-sh">gcc call_rust.c -o call_rust -lrust_from_c -L./target/debug
</code></pre>
<p><code>-l</code> and <code>-L</code> tell gcc to find our Rust library.</p>
<p>Finally, we can call Rust code from C with <code>LD_LIBRARY_PATH</code> specified:</p>
<pre><code class="language-sh">$ LD_LIBRARY_PATH=./target/debug ./call_rust
Hello from Rust!
</code></pre>
<p>That's it!
For more realistic example, check the <a href="https://github.com/eqrion/cbindgen"><code>cbindgen</code></a>.</p>
<h2 id="callbacks-from-c-code-to-rust-functions"><a class="header" href="#callbacks-from-c-code-to-rust-functions">Callbacks from C code to Rust functions</a></h2>
<p>Some external libraries require the usage of callbacks to report back their
current state or intermediate data to the caller.
It is possible to pass functions defined in Rust to an external library.
The requirement for this is that the callback function is marked as <code>extern</code>
with the correct calling convention to make it callable from C code.</p>
<p>The callback function can then be sent through a registration call
to the C library and afterwards be invoked from there.</p>
<p>A basic example is:</p>
<p>Rust code:</p>
<pre><pre class="playground"><code class="language-rust no_run edition2021">extern fn callback(a: i32) {
    println!("I'm called from C with value {0}", a);
}

#[link(name = "extlib")]
extern {
   fn register_callback(cb: extern fn(i32)) -&gt; i32;
   fn trigger_callback();
}

fn main() {
    unsafe {
        register_callback(callback);
        trigger_callback(); // Triggers the callback.
    }
}</code></pre></pre>
<p>C code:</p>
<pre><code class="language-c">typedef void (*rust_callback)(int32_t);
rust_callback cb;

int32_t register_callback(rust_callback callback) {
    cb = callback;
    return 1;
}

void trigger_callback() {
  cb(7); // Will call callback(7) in Rust.
}
</code></pre>
<p>In this example Rust's <code>main()</code> will call <code>trigger_callback()</code> in C,
which would, in turn, call back to <code>callback()</code> in Rust.</p>
<h2 id="targeting-callbacks-to-rust-objects"><a class="header" href="#targeting-callbacks-to-rust-objects">Targeting callbacks to Rust objects</a></h2>
<p>The former example showed how a global function can be called from C code.
However it is often desired that the callback is targeted to a special
Rust object. This could be the object that represents the wrapper for the
respective C object.</p>
<p>This can be achieved by passing a raw pointer to the object down to the
C library. The C library can then include the pointer to the Rust object in
the notification. This will allow the callback to unsafely access the
referenced Rust object.</p>
<p>Rust code:</p>
<pre><pre class="playground"><code class="language-rust no_run edition2021">struct RustObject {
    a: i32,
    // Other members...
}

extern "C" fn callback(target: *mut RustObject, a: i32) {
    println!("I'm called from C with value {0}", a);
    unsafe {
        // Update the value in RustObject with the value received from the callback:
        (*target).a = a;
    }
}

#[link(name = "extlib")]
extern {
   fn register_callback(target: *mut RustObject,
                        cb: extern fn(*mut RustObject, i32)) -&gt; i32;
   fn trigger_callback();
}

fn main() {
    // Create the object that will be referenced in the callback:
    let mut rust_object = Box::new(RustObject { a: 5 });

    unsafe {
        register_callback(&amp;mut *rust_object, callback);
        trigger_callback();
    }
}</code></pre></pre>
<p>C code:</p>
<pre><code class="language-c">typedef void (*rust_callback)(void*, int32_t);
void* cb_target;
rust_callback cb;

int32_t register_callback(void* callback_target, rust_callback callback) {
    cb_target = callback_target;
    cb = callback;
    return 1;
}

void trigger_callback() {
  cb(cb_target, 7); // Will call callback(&amp;rustObject, 7) in Rust.
}
</code></pre>
<h2 id="asynchronous-callbacks"><a class="header" href="#asynchronous-callbacks">Asynchronous callbacks</a></h2>
<p>In the previously given examples the callbacks are invoked as a direct reaction
to a function call to the external C library.
The control over the current thread is switched from Rust to C to Rust for the
execution of the callback, but in the end the callback is executed on the
same thread that called the function which triggered the callback.</p>
<p>Things get more complicated when the external library spawns its own threads
and invokes callbacks from there.
In these cases access to Rust data structures inside the callbacks is
especially unsafe and proper synchronization mechanisms must be used.
Besides classical synchronization mechanisms like mutexes, one possibility in
Rust is to use channels (in <code>std::sync::mpsc</code>) to forward data from the C
thread that invoked the callback into a Rust thread.</p>
<p>If an asynchronous callback targets a special object in the Rust address space
it is also absolutely necessary that no more callbacks are performed by the
C library after the respective Rust object gets destroyed.
This can be achieved by unregistering the callback in the object's
destructor and designing the library in a way that guarantees that no
callback will be performed after deregistration.</p>
<h2 id="linking"><a class="header" href="#linking">Linking</a></h2>
<p>The <code>link</code> attribute on <code>extern</code> blocks provides the basic building block for
instructing rustc how it will link to native libraries. There are two accepted
forms of the link attribute today:</p>
<ul>
<li><code>#[link(name = "foo")]</code></li>
<li><code>#[link(name = "foo", kind = "bar")]</code></li>
</ul>
<p>In both of these cases, <code>foo</code> is the name of the native library that we're
linking to, and in the second case <code>bar</code> is the type of native library that the
compiler is linking to. There are currently three known types of native
libraries:</p>
<ul>
<li>Dynamic - <code>#[link(name = "readline")]</code></li>
<li>Static - <code>#[link(name = "my_build_dependency", kind = "static")]</code></li>
<li>Frameworks - <code>#[link(name = "CoreFoundation", kind = "framework")]</code></li>
</ul>
<p>Note that frameworks are only available on macOS targets.</p>
<p>The different <code>kind</code> values are meant to differentiate how the native library
participates in linkage. From a linkage perspective, the Rust compiler creates
two flavors of artifacts: partial (rlib/staticlib) and final (dylib/binary).
Native dynamic library and framework dependencies are propagated to the final
artifact boundary, while static library dependencies are not propagated at
all, because the static libraries are integrated directly into the subsequent
artifact.</p>
<p>A few examples of how this model can be used are:</p>
<ul>
<li>
<p>A native build dependency. Sometimes some C/C++ glue is needed when writing
some Rust code, but distribution of the C/C++ code in a library format is
a burden. In this case, the code will be archived into <code>libfoo.a</code> and then the
Rust crate would declare a dependency via <code>#[link(name = "foo", kind = "static")]</code>.</p>
<p>Regardless of the flavor of output for the crate, the native static library
will be included in the output, meaning that distribution of the native static
library is not necessary.</p>
</li>
<li>
<p>A normal dynamic dependency. Common system libraries (like <code>readline</code>) are
available on a large number of systems, and often a static copy of these
libraries cannot be found. When this dependency is included in a Rust crate,
partial targets (like rlibs) will not link to the library, but when the rlib
is included in a final target (like a binary), the native library will be
linked in.</p>
</li>
</ul>
<p>On macOS, frameworks behave with the same semantics as a dynamic library.</p>
<h2 id="unsafe-blocks"><a class="header" href="#unsafe-blocks">Unsafe blocks</a></h2>
<p>Some operations, like dereferencing raw pointers or calling functions that have been marked
unsafe are only allowed inside unsafe blocks. Unsafe blocks isolate unsafety and are a promise to
the compiler that the unsafety does not leak out of the block.</p>
<p>Unsafe functions, on the other hand, advertise it to the world. An unsafe function is written like
this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>unsafe fn kaboom(ptr: *const i32) -&gt; i32 { *ptr }
<span class="boring">}</span></code></pre></pre>
<p>This function can only be called from an <code>unsafe</code> block or another <code>unsafe</code> function.</p>
<h2 id="accessing-foreign-globals"><a class="header" href="#accessing-foreign-globals">Accessing foreign globals</a></h2>
<p>Foreign APIs often export a global variable which could do something like track
global state. In order to access these variables, you declare them in <code>extern</code>
blocks with the <code>static</code> keyword:</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">#[link(name = "readline")]
extern {
    static rl_readline_version: libc::c_int;
}

fn main() {
    println!("You have readline version {} installed.",
             unsafe { rl_readline_version as i32 });
}</code></pre>
<p>Alternatively, you may need to alter global state provided by a foreign
interface. To do this, statics can be declared with <code>mut</code> so we can mutate
them.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">use std::ffi::CString;
use std::ptr;

#[link(name = "readline")]
extern {
    static mut rl_prompt: *const libc::c_char;
}

fn main() {
    let prompt = CString::new("[my-awesome-shell] $").unwrap();
    unsafe {
        rl_prompt = prompt.as_ptr();

        println!("{:?}", rl_prompt);

        rl_prompt = ptr::null();
    }
}</code></pre>
<p>Note that all interaction with a <code>static mut</code> is unsafe, both reading and
writing. Dealing with global mutable state requires a great deal of care.</p>
<h2 id="foreign-calling-conventions"><a class="header" href="#foreign-calling-conventions">Foreign calling conventions</a></h2>
<p>Most foreign code exposes a C ABI, and Rust uses the platform's C calling convention by default when
calling foreign functions. Some foreign functions, most notably the Windows API, use other calling
conventions. Rust provides a way to tell the compiler which convention to use:</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">#[cfg(all(target_os = "win32", target_arch = "x86"))]
#[link(name = "kernel32")]
#[allow(non_snake_case)]
extern "stdcall" {
    fn SetEnvironmentVariableA(n: *const u8, v: *const u8) -&gt; libc::c_int;
}
<span class="boring">fn main() { }</span></code></pre>
<p>This applies to the entire <code>extern</code> block. The list of supported ABI constraints
are:</p>
<ul>
<li><code>stdcall</code></li>
<li><code>aapcs</code></li>
<li><code>cdecl</code></li>
<li><code>fastcall</code></li>
<li><code>thiscall</code></li>
<li><code>vectorcall</code>
This is currently hidden behind the <code>abi_vectorcall</code> gate and is subject to change.</li>
<li><code>Rust</code></li>
<li><code>rust-intrinsic</code></li>
<li><code>system</code></li>
<li><code>C</code></li>
<li><code>win64</code></li>
<li><code>sysv64</code></li>
</ul>
<p>Most of the abis in this list are self-explanatory, but the <code>system</code> abi may
seem a little odd. This constraint selects whatever the appropriate ABI is for
interoperating with the target's libraries. For example, on win32 with a x86
architecture, this means that the abi used would be <code>stdcall</code>. On x86_64,
however, windows uses the <code>C</code> calling convention, so <code>C</code> would be used. This
means that in our previous example, we could have used <code>extern "system" { ... }</code>
to define a block for all windows systems, not only x86 ones.</p>
<h2 id="interoperability-with-foreign-code"><a class="header" href="#interoperability-with-foreign-code">Interoperability with foreign code</a></h2>
<p>Rust guarantees that the layout of a <code>struct</code> is compatible with the platform's
representation in C only if the <code>#[repr(C)]</code> attribute is applied to it.
<code>#[repr(C, packed)]</code> can be used to lay out struct members without padding.
<code>#[repr(C)]</code> can also be applied to an enum.</p>
<p>Rust's owned boxes (<code>Box&lt;T&gt;</code>) use non-nullable pointers as handles which point
to the contained object. However, they should not be manually created because
they are managed by internal allocators. References can safely be assumed to be
non-nullable pointers directly to the type.  However, breaking the borrow
checking or mutability rules is not guaranteed to be safe, so prefer using raw
pointers (<code>*</code>) if that's needed because the compiler can't make as many
assumptions about them.</p>
<p>Vectors and strings share the same basic memory layout, and utilities are
available in the <code>vec</code> and <code>str</code> modules for working with C APIs. However,
strings are not terminated with <code>\0</code>. If you need a NUL-terminated string for
interoperability with C, you should use the <code>CString</code> type in the <code>std::ffi</code>
module.</p>
<p>The <a href="https://crates.io/crates/libc"><code>libc</code> crate on crates.io</a> includes type aliases and function
definitions for the C standard library in the <code>libc</code> module, and Rust links
against <code>libc</code> and <code>libm</code> by default.</p>
<h2 id="variadic-functions"><a class="header" href="#variadic-functions">Variadic functions</a></h2>
<p>In C, functions can be 'variadic', meaning they accept a variable number of arguments. This can
be achieved in Rust by specifying <code>...</code> within the argument list of a foreign function declaration:</p>
<pre><code class="language-no_run">extern {
    fn foo(x: i32, ...);
}

fn main() {
    unsafe {
        foo(10, 20, 30, 40, 50);
    }
}
</code></pre>
<p>Normal Rust functions can <em>not</em> be variadic:</p>
<pre><pre class="playground"><code class="language-rust compile_fail edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This will not compile

fn foo(x: i32, ...) {}
<span class="boring">}</span></code></pre></pre>
<h2 id="the-nullable-pointer-optimization"><a class="header" href="#the-nullable-pointer-optimization">The "nullable pointer optimization"</a></h2>
<p>Certain Rust types are defined to never be <code>null</code>. This includes references (<code>&amp;T</code>,
<code>&amp;mut T</code>), boxes (<code>Box&lt;T&gt;</code>), and function pointers (<code>extern "abi" fn()</code>). When
interfacing with C, pointers that might be <code>null</code> are often used, which would seem to
require some messy <code>transmute</code>s and/or unsafe code to handle conversions to/from Rust types.
However, trying to construct/work with these invalid values <strong>is undefined behavior</strong>,
so you should use the following workaround instead.</p>
<p>As a special case, an <code>enum</code> is eligible for the "nullable pointer optimization" if it contains
exactly two variants, one of which contains no data and the other contains a field of one of the
non-nullable types listed above.  This means no extra space is required for a discriminant; rather,
the empty variant is represented by putting a <code>null</code> value into the non-nullable field. This is
called an "optimization", but unlike other optimizations it is guaranteed to apply to eligible
types.</p>
<p>The most common type that takes advantage of the nullable pointer optimization is <code>Option&lt;T&gt;</code>,
where <code>None</code> corresponds to <code>null</code>. So <code>Option&lt;extern "C" fn(c_int) -&gt; c_int&gt;</code> is a correct way
to represent a nullable function pointer using the C ABI (corresponding to the C type
<code>int (*)(int)</code>).</p>
<p>Here is a contrived example. Let's say some C library has a facility for registering a
callback, which gets called in certain situations. The callback is passed a function pointer
and an integer and it is supposed to run the function with the integer as a parameter. So
we have function pointers flying across the FFI boundary in both directions.</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">use libc::c_int;

<span class="boring">#[cfg(hidden)]
</span>extern "C" {
    /// Registers the callback.
    fn register(cb: Option&lt;extern "C" fn(Option&lt;extern "C" fn(c_int) -&gt; c_int&gt;, c_int) -&gt; c_int&gt;);
}
<span class="boring">unsafe fn register(_: Option&lt;extern "C" fn(Option&lt;extern "C" fn(c_int) -&gt; c_int&gt;,
</span><span class="boring">                                           c_int) -&gt; c_int&gt;)
</span><span class="boring">{}
</span>
/// This fairly useless function receives a function pointer and an integer
/// from C, and returns the result of calling the function with the integer.
/// In case no function is provided, it squares the integer by default.
extern "C" fn apply(process: Option&lt;extern "C" fn(c_int) -&gt; c_int&gt;, int: c_int) -&gt; c_int {
    match process {
        Some(f) =&gt; f(int),
        None    =&gt; int * int
    }
}

fn main() {
    unsafe {
        register(Some(apply));
    }
}</code></pre>
<p>And the code on the C side looks like this:</p>
<pre><code class="language-c">void register(int (*f)(int (*)(int), int)) {
    ...
}
</code></pre>
<p>No <code>transmute</code> required!</p>
<h2 id="ffi-and-unwinding"><a class="header" href="#ffi-and-unwinding">FFI and unwinding</a></h2>
<p>It’s important to be mindful of unwinding when working with FFI. Most
ABI strings come in two variants, one with an <code>-unwind</code> suffix and one without.
The <code>Rust</code> ABI always permits unwinding, so there is no <code>Rust-unwind</code> ABI.</p>
<p>If you expect Rust <code>panic</code>s or foreign (e.g. C++) exceptions to cross an FFI
boundary, that boundary must use the appropriate <code>-unwind</code> ABI string.
Conversely, if you do not expect unwinding to cross an ABI boundary, use one of
the non-<code>unwind</code> ABI strings.</p>
<blockquote>
<p>Note: Compiling with <code>panic=abort</code> will still cause <code>panic!</code> to immediately
abort the process, regardless of which ABI is specified by the function that
<code>panic</code>s.</p>
</blockquote>
<p>If an unwinding operation does encounter an ABI boundary that is
not permitted to unwind, the behavior depends on the source of the unwinding
(Rust <code>panic</code> or a foreign exception):</p>
<ul>
<li><code>panic</code> will cause the process to safely abort.</li>
<li>A foreign exception entering Rust will cause undefined behavior.</li>
</ul>
<p>Note that the interaction of <code>catch_unwind</code> with foreign exceptions <strong>is
undefined</strong>, as is the interaction of <code>panic</code> with foreign exception-catching
mechanisms (notably C++'s <code>try</code>/<code>catch</code>).</p>
<h3 id="rust-panic-with-c-unwind"><a class="header" href="#rust-panic-with-c-unwind">Rust <code>panic</code> with <code>"C-unwind"</code></a></h3>
<!-- ignore: using unstable feature -->
<pre><code class="language-rust ignore">#[no_mangle]
extern "C-unwind" fn example() {
    panic!("Uh oh");
}</code></pre>
<p>This function (when compiled with <code>panic=unwind</code>) is permitted to unwind C++
stack frames.</p>
<pre><code class="language-text">[Rust function with `catch_unwind`, which stops the unwinding]
      |
     ...
      |
[C++ frames]
      |                           ^
      | (calls)                   | (unwinding
      v                           |  goes this
[Rust function `example`]         |  way)
      |                           |
      +--- rust function panics --+
</code></pre>
<p>If the C++ frames have objects, their destructors will be called.</p>
<h3 id="c-throw-with-c-unwind"><a class="header" href="#c-throw-with-c-unwind">C++ <code>throw</code> with <code>"C-unwind"</code></a></h3>
<!-- ignore: using unstable feature -->
<pre><code class="language-rust ignore">#[link(...)]
extern "C-unwind" {
    // A C++ function that may throw an exception
    fn may_throw();
}

#[no_mangle]
extern "C-unwind" fn rust_passthrough() {
    let b = Box::new(5);
    unsafe { may_throw(); }
    println!("{:?}", &amp;b);
}</code></pre>
<p>A C++ function with a <code>try</code> block may invoke <code>rust_passthrough</code> and <code>catch</code> an
exception thrown by <code>may_throw</code>.</p>
<pre><code class="language-text">[C++ function with `try` block that invokes `rust_passthrough`]
      |
     ...
      |
[Rust function `rust_passthrough`]
      |                            ^
      | (calls)                    | (unwinding
      v                            |  goes this
[C++ function `may_throw`]         |  way)
      |                            |
      +--- C++ function throws ----+
</code></pre>
<p>If <code>may_throw</code> does throw an exception, <code>b</code> will be dropped. Otherwise, <code>5</code>
will be printed.</p>
<h3 id="panic-can-be-stopped-at-an-abi-boundary"><a class="header" href="#panic-can-be-stopped-at-an-abi-boundary"><code>panic</code> can be stopped at an ABI boundary</a></h3>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[no_mangle]
extern "C" fn assert_nonzero(input: u32) {
    assert!(input != 0)
}
<span class="boring">}</span></code></pre></pre>
<p>If <code>assert_nonzero</code> is called with the argument <code>0</code>, the runtime is guaranteed
to (safely) abort the process, whether or not compiled with <code>panic=abort</code>.</p>
<h3 id="catching-panic-preemptively"><a class="header" href="#catching-panic-preemptively">Catching <code>panic</code> preemptively</a></h3>
<p>If you are writing Rust code that may panic, and you don't wish to abort the
process if it panics, you must use <a href="../std/panic/fn.catch_unwind.html"><code>catch_unwind</code></a>:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use std::panic::catch_unwind;

#[no_mangle]
pub extern "C" fn oh_no() -&gt; i32 {
    let result = catch_unwind(|| {
        panic!("Oops!");
    });
    match result {
        Ok(_) =&gt; 0,
        Err(_) =&gt; 1,
    }
}

fn main() {}</code></pre></pre>
<p>Please note that <a href="../std/panic/fn.catch_unwind.html"><code>catch_unwind</code></a> will only catch unwinding panics, not
those that abort the process. See the documentation of <a href="../std/panic/fn.catch_unwind.html"><code>catch_unwind</code></a>
for more information.</p>
<h2 id="representing-opaque-structs"><a class="header" href="#representing-opaque-structs">Representing opaque structs</a></h2>
<p>Sometimes, a C library wants to provide a pointer to something, but not let you know the internal details of the thing it wants.
A stable and simple way is to use a <code>void *</code> argument:</p>
<pre><code class="language-c">void foo(void *arg);
void bar(void *arg);
</code></pre>
<p>We can represent this in Rust with the <code>c_void</code> type:</p>
<!-- ignore: requires libc crate -->
<pre><code class="language-rust ignore">extern "C" {
    pub fn foo(arg: *mut libc::c_void);
    pub fn bar(arg: *mut libc::c_void);
}
<span class="boring">fn main() {}</span></code></pre>
<p>This is a perfectly valid way of handling the situation. However, we can do a bit
better. To solve this, some C libraries will instead create a <code>struct</code>, where
the details and memory layout of the struct are private. This gives some amount
of type safety. These structures are called ‘opaque’. Here’s an example, in C:</p>
<pre><code class="language-c">struct Foo; /* Foo is a structure, but its contents are not part of the public interface */
struct Bar;
void foo(struct Foo *arg);
void bar(struct Bar *arg);
</code></pre>
<p>To do this in Rust, let’s create our own opaque types:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#[repr(C)]
pub struct Foo {
    _data: [u8; 0],
    _marker:
        core::marker::PhantomData&lt;(*mut u8, core::marker::PhantomPinned)&gt;,
}
#[repr(C)]
pub struct Bar {
    _data: [u8; 0],
    _marker:
        core::marker::PhantomData&lt;(*mut u8, core::marker::PhantomPinned)&gt;,
}

extern "C" {
    pub fn foo(arg: *mut Foo);
    pub fn bar(arg: *mut Bar);
}
<span class="boring">fn main() {}</span></code></pre></pre>
<p>By including at least one private field and no constructor,
we create an opaque type that we can't instantiate outside of this module.
(A struct with no field could be instantiated by anyone.)
We also want to use this type in FFI, so we have to add <code>#[repr(C)]</code>.
The marker ensures the compiler does not mark the struct as <code>Send</code>, <code>Sync</code> and <code>Unpin</code> are
not applied to the struct. (<code>*mut u8</code> is not <code>Send</code> or <code>Sync</code>, <code>PhantomPinned</code> is not <code>Unpin</code>)</p>
<p>But because our <code>Foo</code> and <code>Bar</code> types are
different, we’ll get type safety between the two of them, so we cannot
accidentally pass a pointer to <code>Foo</code> to <code>bar()</code>.</p>
<p>Notice that it is a really bad idea to use an empty enum as FFI type.
The compiler relies on empty enums being uninhabited, so handling values of type
<code>&amp;Empty</code> is a huge footgun and can lead to buggy program behavior (by triggering
undefined behavior).</p>
<blockquote>
<p><strong>NOTE:</strong> The simplest way would use "extern types".
But it's currently (as of June 2021) unstable and has some unresolved questions, see the <a href="https://rust-lang.github.io/rfcs/1861-extern-types.html">RFC page</a> and the <a href="https://github.com/rust-lang/rust/issues/43467">tracking issue</a> for more details.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="beneath-std"><a class="header" href="#beneath-std">Beneath <code>std</code></a></h1>
<p>This section documents features that are normally provided by the <code>std</code> crate and
that <code>#![no_std]</code> developers have to deal with (i.e. provide) to build
<code>#![no_std]</code> binary crates.</p>
<h2 id="using-libc"><a class="header" href="#using-libc">Using <code>libc</code></a></h2>
<p>In order to build a <code>#[no_std]</code> executable we will need <code>libc</code> as a dependency.
We can specify this using our <code>Cargo.toml</code> file:</p>
<pre><code class="language-toml">[dependencies]
libc = { version = "0.2.146", default-features = false }
</code></pre>
<p>Note that the default features have been disabled. This is a critical step -
<strong>the default features of <code>libc</code> include the <code>std</code> crate and so must be
disabled.</strong></p>
<p>Alternatively, we can use the unstable <code>rustc_private</code> private feature together
with an <code>extern crate libc;</code> declaration as shown in the examples below.</p>
<h2 id="writing-an-executable-without-std"><a class="header" href="#writing-an-executable-without-std">Writing an executable without <code>std</code></a></h2>
<p>We will probably need a nightly version of the compiler to produce
a <code>#![no_std]</code> executable because on many platforms, we have to provide the
<code>eh_personality</code> <a href="https://doc.rust-lang.org/nightly/unstable-book/language-features/lang-items.html">lang item</a>, which is unstable.</p>
<p>Controlling the entry point is possible in two ways: the <code>#[start]</code> attribute,
or overriding the default shim for the C <code>main</code> function with your own.
Additionally, it's required to define a <a href="panic-handler.html">panic handler function</a>.</p>
<p>The function marked <code>#[start]</code> is passed the command line parameters
in the same format as C (aside from the exact integer types being used):</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(start, lang_items, core_intrinsics, rustc_private)]
#![allow(internal_features)]
#![no_std]

// Necessary for `panic = "unwind"` builds on some platforms.
#![feature(panic_unwind)]
extern crate unwind;

// Pull in the system libc library for what crt0.o likely requires.
extern crate libc;

use core::panic::PanicInfo;

// Entry point for this program.
#[start]
fn main(_argc: isize, _argv: *const *const u8) -&gt; isize {
    0
}

// These functions are used by the compiler, but not for an empty program like this.
// They are normally provided by `std`.
#[lang = "eh_personality"]
fn rust_eh_personality() {}
#[panic_handler]
fn panic_handler(_info: &amp;PanicInfo) -&gt; ! { core::intrinsics::abort() }</code></pre></pre>
<p>To override the compiler-inserted <code>main</code> shim, we have to disable it
with <code>#![no_main]</code> and then create the appropriate symbol with the
correct ABI and the correct name, which requires overriding the
compiler's name mangling too:</p>
<pre><pre class="playground"><code class="language-rust edition2021">#![feature(lang_items, core_intrinsics, rustc_private)]
#![allow(internal_features)]
#![no_std]
#![no_main]

// Necessary for `panic = "unwind"` builds on some platforms.
#![feature(panic_unwind)]
extern crate unwind;

// Pull in the system libc library for what crt0.o likely requires.
extern crate libc;

use core::ffi::{c_char, c_int};
use core::panic::PanicInfo;

// Entry point for this program.
#[no_mangle] // ensure that this symbol is included in the output as `main`
extern "C" fn main(_argc: c_int, _argv: *const *const c_char) -&gt; c_int {
    0
}

// These functions are used by the compiler, but not for an empty program like this.
// They are normally provided by `std`.
#[lang = "eh_personality"]
fn rust_eh_personality() {}
#[panic_handler]
fn panic_handler(_info: &amp;PanicInfo) -&gt; ! { core::intrinsics::abort() }</code></pre></pre>
<p>If you are working with a target that doesn't have binary releases of the
standard library available via rustup (this probably means you are building the
<code>core</code> crate yourself) and need compiler-rt intrinsics (i.e. you are probably
getting linker errors when building an executable:
<code>undefined reference to `__aeabi_memcpy'</code>), you need to manually link to the
<a href="https://crates.io/crates/compiler_builtins"><code>compiler_builtins</code> crate</a> to get those intrinsics and solve the linker errors.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="panic_handler"><a class="header" href="#panic_handler">#[panic_handler]</a></h1>
<p><code>#[panic_handler]</code> is used to define the behavior of <code>panic!</code> in <code>#![no_std]</code> applications.
The <code>#[panic_handler]</code> attribute must be applied to a function with signature <code>fn(&amp;PanicInfo) -&gt; !</code> and such function must appear <em>once</em> in the dependency graph of a binary / dylib / cdylib
crate. The API of <code>PanicInfo</code> can be found in the <a href="../core/panic/struct.PanicInfo.html">API docs</a>.</p>
<p>Given that <code>#![no_std]</code> applications have no <em>standard</em> output and that some <code>#![no_std]</code>
applications, e.g. embedded applications, need different panicking behaviors for development and for
release it can be helpful to have panic crates, crate that only contain a <code>#[panic_handler]</code>.
This way applications can easily swap the panicking behavior by simply linking to a different panic
crate.</p>
<p>Below is shown an example where an application has a different panicking behavior depending on
whether is compiled using the dev profile (<code>cargo build</code>) or using the release profile (<code>cargo build --release</code>).</p>
<p><code>panic-semihosting</code> crate -- log panic messages to the host stderr using semihosting:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">#![no_std]

use core::fmt::{Write, self};
use core::panic::PanicInfo;

struct HStderr {
    // ..
<span class="boring">    _0: (),
</span>}
<span class="boring">
</span><span class="boring">impl HStderr {
</span><span class="boring">    fn new() -&gt; HStderr { HStderr { _0: () } }
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl fmt::Write for HStderr {
</span><span class="boring">    fn write_str(&amp;mut self, _: &amp;str) -&gt; fmt::Result { Ok(()) }
</span><span class="boring">}
</span>
#[panic_handler]
fn panic(info: &amp;PanicInfo) -&gt; ! {
    let mut host_stderr = HStderr::new();

    // logs "panicked at '$reason', src/main.rs:27:4" to the host stderr
    writeln!(host_stderr, "{}", info).ok();

    loop {}
}</code></pre>
<p><code>panic-halt</code> crate -- halt the thread on panic; messages are discarded:</p>
<!-- ignore: simplified code -->
<pre><code class="language-rust ignore">#![no_std]

use core::panic::PanicInfo;

#[panic_handler]
fn panic(_info: &amp;PanicInfo) -&gt; ! {
    loop {}
}</code></pre>
<p><code>app</code> crate:</p>
<!-- ignore: requires the above crates -->
<pre><code class="language-rust ignore">#![no_std]

// dev profile
#[cfg(debug_assertions)]
extern crate panic_semihosting;

// release profile
#[cfg(not(debug_assertions))]
extern crate panic_halt;

fn main() {
    // ..
}</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
